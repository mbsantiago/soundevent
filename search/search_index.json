{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"soundevent","text":"<p>Warning This package is under active development, use with caution.</p> <p><code>soundevent</code> is an open-source Python package that aims to support the computational biocoustic community in developing well-tested, coherent, and standardized software tools for bioacoustic analysis. The main goal of the package is to provide a flexible yet consistent definition of what a sound event is in a computational sense, along with a set of tools to easily work with this definition. The package comprises three key components:</p>"},{"location":"#main_features","title":"Main features","text":""},{"location":"#1_data_schemas_for_bioacoustic_analysis","title":"1. Data Schemas for Bioacoustic Analysis","text":"<p>The <code>soundevent</code> package introduces several data schemas designed to conceptualize and standardize recurring objects in bioacoustic analysis. These data schemas establish relationships between various concepts and define the attributes each object possesses. They provide flexibility to cover a broad spectrum of use cases in bioacoustic analysis while incorporating data validation mechanisms to ensure stored information is both valid and meaningful. Notably, the package defines schemas related to sound events, including user annotations and model predictions.</p>"},{"location":"#2_serialization_storage_and_reading_functions","title":"2. Serialization, Storage, and Reading Functions","text":"<p>To promote standardized data formats for storing annotated datasets and other information about sounds in recordings, the <code>soundevent</code> package provides several functions for serialization, storage, and reading of the different data classes offered. These functions enable easy sharing of information about common objects in bioacoustic research. By employing a consistent data format, researchers can exchange data more efficiently and collaborate seamlessly.</p>"},{"location":"#3_handling_functions_for_sound_events","title":"3. Handling Functions for Sound Events","text":"<p>The <code>soundevent</code> package also includes a variety of functions that facilitate the handling of sound event objects. These functions serve multiple purposes, such as matching sound events for model prediction evaluation, transforming sound events, and managing metadata and labels. By offering a comprehensive set of handling functions, the package aims to streamline the analysis workflow for bioacoustic researchers, providing them with powerful tools to manipulate and extract insights from their data.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install <code>soundevent</code> using pip:</p> <pre><code>pip install soundevent\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed information on how to use the package, please refer to the documentation.</p>"},{"location":"#example_usage","title":"Example Usage","text":"<p>To see practical examples of how to use soundevent, you can explore the collection of examples provided in the documentation's gallery.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from the community to make <code>soundevent</code> even better. If you would like to contribute, please refer to the contribution guidelines.</p>"},{"location":"#license","title":"License","text":"<p><code>soundevent</code> is released under the MIT License.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our_pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our_standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our_responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"CONTRIBUTING/","title":"Contributing guidelines","text":"<p>We welcome any kind of contribution to <code>soundevent</code>, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation).</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"CONTRIBUTING/#you_have_a_question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"CONTRIBUTING/#you_think_you_may_have_found_a_bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:<ul> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> </ul> </li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"CONTRIBUTING/#you_want_to_make_some_kind_of_change_to_the_code_base","title":"You want to make some kind of change to the code base","text":"<ol> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest main commit. While working on your feature branch, make sure to stay up to date with the main branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>Install dependencies with <code>pip install -e .</code>;</li> <li>make sure the existing tests still work by running <code>pytest</code>. If project tests fails use <code>pytest --keep-baked-projects</code> to keep generated project in /tmp/pytest-* and investigate;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation;</li> <li>push your feature branch to (your fork of) the Python Template repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ol> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"introduction/","title":"Soundevent Data Schemas","text":""},{"location":"introduction/#introduction","title":"Introduction","text":"<p>Welcome to the world of bioacoustic analysis with the <code>soundevent</code> package! \ud83c\udfb5 This package brings you a collection of data schemas designed to be the backbone of your computational bioacoustics analysis. Whether you're a seasoned Python developer or a curious bioacoustics enthusiast, these schemas aim to make your life simpler.</p>"},{"location":"introduction/#what_is_a_schema","title":"What is a schema?","text":"<p>Let's start with the basics. A schema is like the blueprint for your data. It's a formal way of specifying how data is structured, allowing you to clearly define what data objects hold and how they store it.</p> <p>More on Schemas</p> <p>For a deeper dive into schemas, check out Understanding JSON Schema.</p>"},{"location":"introduction/#why_should_you_care","title":"Why should you care?","text":"<p>Now, let's discuss why these data schemas matter to us bioacousticians:</p> <ol> <li> <p>Teamwork: Shared schemas mean everyone's on the same page. When everyone    follows the same rules, collaboration and tool compatibility become much    smoother.</p> </li> <li> <p>Quality Check: Ever wished for an easy way to identify errors in your    data? Schemas make it happen. They enable you to check an object against the    schema, validating if the data has everything it needs and is correct.</p> </li> <li> <p>Enhanced developer experience: Python is entering the era of Type hints.    Using these hints makes your code more robust, acting like guardrails to    ensure that your data follows the rules.</p> </li> </ol> <p>What are type hints?</p> <p>For a quick introduction to what type hints are and how to use them, check out this great explanation in the FastAPI documentation.</p>"},{"location":"data_schemas/","title":"Data Schemas","text":"<p>Welcome to the data schemas tour with the <code>soundevent</code> package! In this overview, we'll break down the various data schemas provided by the package into the following sections:</p>"},{"location":"data_schemas/#describing_the_data","title":"Describing the Data","text":"<p><code>soundevent</code> provides tools to attach essential information to various objects in bioacoustic analysis:</p> <ul> <li>Users: Keeping reference of everyone's contribution.</li> <li>Terms: Standardized vocabularies ensure consistent language.</li> <li>Tags: Attaching semantic context to objects.</li> <li>Features: Numerical descriptors capturing continuously varying attributes.</li> <li>Notes: User-written free-text annotations.</li> </ul>"},{"location":"data_schemas/#audio_content","title":"Audio Content","text":"<p>At the core of acoustic analysis, we have schemas for:</p> <ul> <li>Recordings: Complete audio files.</li> <li>Dataset: A collection of recordings from a common source.</li> </ul>"},{"location":"data_schemas/#acoustic_objects","title":"Acoustic Objects","text":"<p>Identifying distinctive sound elements within audio content:</p> <ul> <li>Geometric\u00a0Objects: Defining Regions of Interest (RoI) in the temporal-frequency plane.</li> <li>Sound\u00a0Events: Individual sonic occurrences.</li> <li>Sequences: Patterns of connected sound events.</li> <li>Clips: Fragments extracted from recordings.</li> </ul>"},{"location":"data_schemas/#annotation","title":"Annotation","text":"<p><code>soundevent</code> places emphasis on human annotation processes, covering:</p> <ul> <li>Sound\u00a0Event\u00a0Annotations: Expert-created markers for relevant sound events.</li> <li>Sequence\u00a0Annotations: User provided annotations of sequences of sound events.</li> <li>Clip\u00a0Annotations: Annotations and notes at the clip level.</li> <li>Annotation\u00a0Task: Descriptions of tasks and the status of annotation.</li> <li>Annotation\u00a0Project: The collective description of tasks and annotations.</li> </ul>"},{"location":"data_schemas/#prediction","title":"Prediction","text":"<p>Automated processing methods also play a role, generating:</p> <ul> <li>Sound\u00a0Event\u00a0Predictions: Predictions made during automated processing.</li> <li>Sequence\u00a0Predictions: Predictions of sequences of sound events.</li> <li>Clip\u00a0Predictions: Collections of predictions and additional information at the clip level.</li> <li>Model\u00a0Runs: Sets of clip predictions generated in a single run by a specific model.</li> </ul>"},{"location":"data_schemas/#evaluation","title":"Evaluation","text":"<p>Assessing the accuracy of predictions is crucial, and <code>soundevent</code> provides schemas for:</p> <ul> <li>Matches: Predicted sound events overlapping with ground truth.</li> <li>Clip\u00a0Evaluation: Information about matches and performance metrics at the clip level.</li> <li>Evaluation: Comprehensive details on model performance across the entire evaluation set.</li> <li>Evaluation\u00a0Set: Human annotations serving as ground truth.</li> </ul> <p>Want to know more? Dive in for a closer look at each of these schemas.</p> <p>Unique Identifiers</p> <p>In <code>soundevent</code>, various objects feature a field called <code>uuid</code>. This field stores a Universal Unique Identifier (UUID), a 128-bit label generated automatically. When created following standard methods, UUIDs are practically unique. Information labeled with UUIDs by different parties can be combined into a unified database without fear of duplication.</p>"},{"location":"data_schemas/acoustic_objects/","title":"Acoustic Objects","text":"<p>Now, we'll explore how <code>soundevent</code> handles various \"objects\" within the acoustic content. To begin, Geometry objects offer a means to define regions of interest in the time-frequency plane. Sound events, as acoustic objects within the audio, are characterized by a geometry that delineates their location. Additionally, <code>soundevent</code> introduces a method for specifying Sequences of these Sound events. Let's delve deeper into these concepts:</p>"},{"location":"data_schemas/acoustic_objects/#geometries","title":"Geometries","text":"<p>In <code>soundevent</code>, Geometry objects are essential for defining precise regions of interest in the time-frequency plane. The package offers various geometry types, providing flexibility in delineating regions of interest:</p> <ul> <li> <p>TimeStamp: Represents a single point in time.</p> </li> <li> <p>TimeInterval: Describes a time interval,   specifying both starting and ending times.</p> </li> <li> <p>Point: Pinpoints the exact location in time and   frequency.</p> </li> <li> <p>LineString: Describes an unbroken   (potentially non-straight) line through a sequence of points.</p> </li> <li> <p>Polygon: Defines a closed shape in time and   frequency, possibly with holes.</p> </li> <li> <p>BoundingBox: Represents a rectangle in time   and frequency, offered separately due to its common use.</p> </li> <li> <p>MultiPoint: Describes a collection of   points.</p> </li> <li> <p>MultiLineString: A collection of line   strings.</p> </li> <li> <p>MultiPolygon: A collection of polygons,   useful for demarcating regions of interest that are interrupted by occluding   sounds.</p> </li> </ul> <p>Important Note on Time and Frequency Units</p> <p>Pay careful attention to the units used in <code>soundevent</code> geometries. Time is uniformly measured in seconds, and frequency is represented in hertz. It's crucial to emphasize that all time values are consistently referenced relative to the start of the recording. Adhering to these standardized units ensures smoother development of functions and interaction with geometry objects, built on reliable assumptions.</p> <p>Understanding Geometry Objects in <code>soundevent</code></p> <p><code>soundevent</code> adheres to the GeoJSON specification for structuring geometry objects. Every geometry object comprises a type field, indicating its specific geometry type, and a coordinates field defining its geometric properties. For further details and specifications, refer to the GeoJSON Specification.</p>"},{"location":"data_schemas/acoustic_objects/#sound_events","title":"Sound Events","text":"<p>Sound Events take the spotlight in this package, serving as the key players representing distinct sounds within the audio content. These \"events\" unfold within specific time intervals and frequency ranges, and the <code>soundevent</code> package ensures their precise localization using handy Geometry objects.</p> <p>Adding a layer of richness to these Sound Events is the ability to characterize them through Features. These Features provide quantitative insights into various acoustic properties, ranging from the basics like duration, bandwidth, to peak frequency. You can attach any feature you fancy, including those extracted by Deep Learning models!</p> <pre><code>erDiagram\n    SoundEvent {\n        UUID uuid\n    }\n    Geometry\n    Feature\n    SoundEvent ||--|| Geometry : geometry\n    SoundEvent ||--o{ Feature : features</code></pre>"},{"location":"data_schemas/acoustic_objects/#sequences","title":"Sequences","text":"<p>A Sequence in <code>soundevent</code> is essentially a collection of Sound Events, providing a flexible modeling tool that groups multiple Sound Events with a unifying relation. Researchers have the freedom to determine which Sound Events constitute a sequence, allowing them to customize the structure based on their specific research requirements. The Sequence object can specify a parent Sequence, supporting hierarchical arrangements, enabling the inclusion of subsequences and providing a comprehensive representation of intricate relationships within complex sequences. Similar to Sound Events, Sequences can be described using Features, offering numerical insights into their acoustic properties.</p> <pre><code>erDiagram\n    Sequence {\n        UUID uuid\n    }\n    SoundEvent\n    Feature\n    Sequence }|--|{ SoundEvent : sound_events\n    Sequence ||--o{ Feature : features\n    Sequence }|--o| Sequence : parent\n</code></pre>"},{"location":"data_schemas/acoustic_objects/#clips","title":"Clips","text":"<p>Clips in <code>soundevent</code> represent distinct fragments of a Recording, delineated by their start and end times. Serving as fundamental units for analysis and annotation tasks, Clips offer a more computationally efficient approach, particularly when working with lengthy audio files. Breaking down the Recording into manageable Clips not only enhances computational efficiency but also supports focused analysis on specific segments of interest. Standardizing Clip durations ensures consistency in annotations across diverse Recordings and facilitates easier interpretation and comparison of results in audio data. Many machine learning models process audio files in Clips, reinforcing the practical adoption of the Clip structure.</p> <p>The exploration of a Clip's content is facilitated through Features attached to the Clip, providing numerical descriptors of its acoustic content. These features can vary widely, encompassing Acoustic Indices, simple descriptors of overall acoustic information, or even abstract features derived from Deep Learning models. You have the freedom to choose features that align with the specific requirements of your work, making it a flexible and customizable aspect of your analysis.</p> <pre><code>erDiagram\n    Clip {\n        UUID uuid\n        float start_time\n        float end_time\n    }\n    Recording\n    Feature\n    Clip }|--|| Recording : recording\n    Clip }|--o{ Feature : features</code></pre> Understanding the Distinction between Clips and TimeInterval Sound Events <p>While both Clip objects and TimeInterval Sound Events share a common feature of being defined by a specific start and end time, their purpose and usage significantly differ. Generally, TimeInterval Sound Events are designed to emphasize a segment of the audio content that corresponds to a distinct and cohesive sound. In contrast, Clips have no such restriction; they represent a subset of a recording without a specific reference to a single sound event. Clips are typically considered to encapsulate the entire acoustic content, acknowledging that they may contain multiple sound events or none at all. Therefore, discussions about clips generally revolve around the entirety of the acoustic material rather than focusing on a particular sound instance.</p>"},{"location":"data_schemas/annotation/","title":"Annotation","text":"<p>Deciphering the meaning of sounds within a recording can be a nuanced task. Typically, it involves someone carefully listening to the audio and choosing specific segments to highlight or describing the soundscape in a particular manner.</p> <p>In <code>soundevent</code>, we call this process \"Annotation.\" It's about giving a human interpretation to the sounds you've identified. You can annotate various sound objects like Clips, Events, and Sequences. Annotations, in simple terms, include Tags to categorize stuff, Notes for adding details or thoughts about what the sound means, and info about who did the annotating and when.</p> <p>So, let's dive in and see how <code>soundevent</code> handles different types of annotated objects.</p>"},{"location":"data_schemas/annotation/#sound_event_annotation","title":"Sound Event Annotation","text":"<p>A SoundEventAnnotation object encompasses all details about an annotated Sound Event, including a reference to the sound event itself (capturing the specific portion of sound), attached tags (for classifying the sound event), notes, information about the user who created the annotation, and the timestamp of its creation. Through annotations, a sound event gains a distinct interpretation, providing clarity and context to the sound.</p> <pre><code>erDiagram\n    SoundEventAnnotation {\n        UUID uuid\n        datetime created_on\n    }\n    SoundEvent\n    Tag\n    Note\n    User\n    SoundEventAnnotation ||--|| SoundEvent : sound_event\n    SoundEventAnnotation }|--o{ Tag : tags\n    SoundEventAnnotation ||--o{ Note : notes\n    SoundEventAnnotation }|--o| User : created_by</code></pre>"},{"location":"data_schemas/annotation/#sequence_annotation","title":"Sequence Annotation","text":"<p>Moving beyond individual Sound Events, <code>soundevent</code> introduces Sequence Annotation, a type of annotation applied to sequences of sound events. A SequenceAnnotation object encapsulates details about the annotated sequence, referencing the particular sequence, attaching relevant tags, providing notes for additional insights or interpretations, and recording information about the annotating user and the creation timestamp. Sequence Annotation extends the interpretive layer to complex vocalization patterns, enhancing the understanding of structured sound sequences.</p> <pre><code>erDiagram\n    SequenceAnnotation {\n        UUID uuid\n        datetime created_on\n    }\n    Sequence\n    Tag\n    Note\n    User\n    SequenceAnnotation ||--|| Sequence : sequence\n    SequenceAnnotation }|--o{ Tag : tags\n    SequenceAnnotation ||--o{ Note : notes\n    SequenceAnnotation }|--o| User : created_by</code></pre>"},{"location":"data_schemas/annotation/#clip_annotations","title":"Clip Annotations","text":"<p>Expanding the scope of annotations, <code>soundevent</code> introduces Clip Annotations, providing a way to annotate entire audio clips. The ClipAnnotation object encompasses tags and notes, offering additional insights into the overall acoustic content of the clip. It retains information about the annotating user and the creation timestamp. Unlike sound event and sequence annotations, Clip Annotation goes further by storing the list of AnnotatedSoundEvents found within the clip, along with any AnnotatedSequences. This broader annotation approach allows for a comprehensive understanding of the audio content encapsulated by the clip.</p> <pre><code>erDiagram\n    ClipAnnotation {\n        UUID uuid\n        datetime created_on\n    }\n    Clip\n    Tag\n    Note\n    User\n    SoundEventAnnotation\n    SequenceAnnotation\n    ClipAnnotation ||--|| Clip : clip\n    ClipAnnotation }|--o{ Tag : tags\n    ClipAnnotation ||--o{ Note : notes\n    ClipAnnotation }|--o| User : created_by\n    ClipAnnotation ||--|{ SoundEventAnnotation : sound_events\n    ClipAnnotation ||--|{ SequenceAnnotation : sequences</code></pre>"},{"location":"data_schemas/annotation/#annotation_task","title":"Annotation Task","text":"<p>When managing an annotation project, it's important to track which segments of the entire dataset have undergone annotation. In <code>soundevent</code>, we use the Clip as the fundamental unit for annotation tasks. Typically, annotators focus on inspecting and listening to an audio clip, completing the necessary annotations as part of the task. <code>soundevent</code> provides the AnnotationTask object, containing essential details about the annotation task, including the clip being annotated, information about the status of annotation (as indicated by Status Badges), and additional information like the creation date. This data not only facilitates tracking the progress of annotations but also helps identify tasks in need of review or attention.</p> <pre><code>erDiagram\n    AnnotationTask {\n        UUID uuid\n        datetime created_on\n    }\n    StatusBadge {\n        str state\n        datetime created_on\n    }\n    Clip\n    User\n    AnnotationTask }|--|| Clip : clip\n    AnnotationTask ||--o{ StatusBadge : status_badges\n    StatusBadge }|--o{ User : owner</code></pre>"},{"location":"data_schemas/annotation/#annotation_project","title":"Annotation Project","text":"<p>An Annotation Project object in soundevent consolidates all associated Annotation Tasks and the Clip Annotations made within that project. It's essential to note that Clip Annotations may encompass Sound Event Annotations and Sequence Annotations, making the Annotation Project a comprehensive repository of all annotated data related to the designated clips. In addition to task and annotation details, the annotation project is characterized by a name, serving to identify its purpose and content, and a description, providing additional insights into the project's objectives and goals. A critical component of the annotation project is the inclusion of annotation instructions. The term \"completing an annotation task\" gains clarity and relevance through explicit annotation instructions, guiding annotators on the expectations, methods, and adherence to project standards and goals.</p> <p>In the context of an Annotation Project, it is crucial to specify a set of valid Tags that annotators can use. This practice ensures better control over the information attached to the sound objects within the project. The selection of these tags should align with the annotation goals, facilitating a more focused and purposeful annotation process.</p> <pre><code>erDiagram\n    AnnotationProject {\n        UUID uuid\n        str name\n        str description\n        str instructions\n        datetime created_on\n    }\n    Tag\n    AnnotationTask\n    ClipAnnotation\n    AnnotationProject }|--o{ Tag : annotation_tags\n    AnnotationProject }|--o{ AnnotationTask : tasks\n    AnnotationProject }|--o{ ClipAnnotation : clip_annotations</code></pre>"},{"location":"data_schemas/audio_content/","title":"Audio Content","text":""},{"location":"data_schemas/audio_content/#recordings","title":"Recordings","text":"<p>A Recording in <code>soundevent</code> is a singular audio file, typically an unaltered capture of original audio recorded by a device. Uniquely identified and linked to a file path, each Recording preserves its authenticity.</p> <p>Accompanied by metadata, Recordings reveal crucial details like duration, sample rate, and the number of audio channels, providing technical insights for accurate processing and analysis. Temporal and spatial context can be enriched with date, time, latitude, and longitude coordinates, facilitating organization and comparison based on the time of capture and geographical location.</p> <p>Optionally, Recordings can include ownership information, valuable for compiling datasets and ensuring correct attribution. A rights field specifies usage permissions. Metadata can be further enriched with additional tags, features, and notes. Tags offer categorical details like recording site or equipment used, while features provide numeric values quantifying specific characteristics. Notes contribute additional context, insights, or pertinent details, fostering a deeper understanding of the recording.</p> <pre><code>erDiagram\n    Recording {\n        UUID uuid\n        string path\n        int samplerate\n        int channels\n        float duration\n        string hash\n        date date\n        time time\n        float latitude\n        float longitude\n        string rights\n    }\n    User\n    Tag\n    Feature\n    Note\n    Recording ||--o{ Feature : features\n    Recording ||--o{ Note : notes\n    Recording }|--o{ Tag : tags\n    Recording }|--o{ User : owners</code></pre> Recording Hashes <p>Recording names can change, but their content remains the same. To ensure we can always recognize a specific recording, regardless of its name changes, we employ a helpful tool: the Hash. In <code>soundevent</code>, we use the MD5 hashing function. Imagine it as a unique fingerprint for each file, derived solely from its content. This way, even if the name switches, the essence of the recording remains unmistakably identified.</p> Audiovisual Core Metadata Schema <p>In <code>soundevent</code>, we prioritize compatibility with established standards, specifically the TDWG Audiovisual Core metadata schema. All essential fields required by the schema are integrated into the Recording object. While there may be variations, particularly in the attachment of taxonomic terms, a thoughtful selection of Tags in <code>soundevent</code> enables a smooth alignment of most Recording fields with the Audiovisual Core's standardized terms. This ensures a cohesive collaboration within the metadata realm.</p>"},{"location":"data_schemas/audio_content/#datasets","title":"Datasets","text":"<p>In <code>soundevent</code>, Datasets serve as purposeful collections of related audio Recordings, sharing a unified context, like a specific deployment or field study. Each Dataset receives a meaningful name and is accompanied by a descriptive summary, facilitating distinction among multiple datasets. The suggested descriptive information should delve into the dataset's origin, context, and relevance, enriching the understanding of its content. Although <code>soundevent</code> doesn't enforce specific criteria for defining datasets, maintaining cohesive audio collections is recommended. This practice enhances source comprehension and enables more nuanced analysis of the dataset's contents.</p> <pre><code>erDiagram\n    Dataset {\n        uuid UUID\n        string name\n        string description\n        datetime created_on\n    }\n    Recording\n    Dataset ||--|{ Recording: recordings\n</code></pre>"},{"location":"data_schemas/descriptors/","title":"Data Description","text":"<p>Let's explore users, terms, tags, features, and notes \u2013 essential tools for enriching bioacoustic research. Controlled vocabularies (terms), categorical tags, numerical features, and free-form notes provide deeper context and insights into your research objects. User information ensures proper attribution for everyone involved.</p>"},{"location":"data_schemas/descriptors/#users","title":"Users","text":"<p>Bioacoustic analysis often involves collaboration between data collectors, annotators, reviewers, administrators, developers, and researchers. To acknowledge contributions, soundevent introduces a Users data schema, storing basic information about each individual. The User object can optionally include name, email, username and institution. It's crucial to respect privacy and ensure individuals are comfortable sharing this information. If concerns remain, User objects can be omitted entirely.</p> <pre><code>erDiagram\n    User {\n        UUID uuid\n        string name\n        string email\n        string username\n        string institution\n    }</code></pre>"},{"location":"data_schemas/descriptors/#terms","title":"Terms","text":"<p>Terms ensure everyone's on the same page. Inconsistent naming like \"species\" vs. \"Species\" wastes time. Terms provide a controlled vocabulary for common properties used in annotations and descriptions.</p> <p>We've selected terms from established vocabularies like Darwin\u00a0Core and Audiovisual\u00a0Core, aligning your work with best practices. Take a look here for the terms defined in soundevent.</p>"},{"location":"data_schemas/descriptors/#tags","title":"Tags","text":"<p>Tags are informative labels within the <code>soundevent</code> package. They add meaning to recordings, clips, or sound events, helping organize and contextualize data.</p> <p>A Tag has two parts: a term and a value. The term acts as a namespace, refining the Tag's meaning and context.</p> <pre><code>erDiagram\n    Tag {\n        string value\n    }\n    Term\n    Tag ||--o| Term: term</code></pre> <p>You have the flexibility to use a term or not. We strongly recommend it, but it's not mandatory. This adaptability allows you to tailor Tags to your specific project needs.</p> What is a namespace? <p>Taken from the Wikipedia article on namespaces:</p> <p>a namespace is a set of signs (names) that are used to identify and refer to objects of various kinds. A namespace ensures that all of a given set of objects have unique names so that they can be easily identified.</p> <p>[...]</p> <p>namespaces are typically employed for the purpose of grouping symbols and identifiers around a particular functionality and to avoid name collisions between multiple identifiers that share the same name</p>"},{"location":"data_schemas/descriptors/#features","title":"Features","text":"<p>Features are numerical descriptions. They can include measurements from environmental sensors, attributes of sound-producing individuals, or even abstract features extracted by deep learning models. Features enable comparison, visualization, outlier identification, understanding characteristic distributions, and statistical analysis.</p> <p>A Feature consists of a Term and a numerical value.</p> <pre><code>erDiagram\n    Feature {\n        float value\n    }\n    Term\n    Feature ||--o| Term: term</code></pre>"},{"location":"data_schemas/descriptors/#notes","title":"Notes","text":"<p>[Notes] are free-form textual additions, facilitating communication and providing context. They can convey information, enable discussions, or flag data issues.</p> <p>Notes can have any length and include the note's creator and time of creation. Notes can also be marked as issues to highlight points needing review.</p> <pre><code>erDiagram\n    Note {\n        UUID uuid\n        string message\n        datetime created_on\n        bool is_issue\n    }\n    User\n    Note ||--o| User: created_by</code></pre>"},{"location":"data_schemas/evaluation/","title":"Evaluation","text":"<p>In computational bioacoustics, a key aspiration is to effectively replicate and automate the intricate analyses conducted by human experts. Achieving this goal would represent a big step in scalability, enabling the exploration of vast amounts of data and uncovering previously unexplored insights into animal ecology and behavior.</p> <p>To assess the success and reliability of automated methods, it is important to establish evaluation criteria. <code>soundevent</code> provides specific objects that play an important role in the evaluation process. This page delves into these objects.</p>"},{"location":"data_schemas/evaluation/#matches","title":"Matches","text":"<p>When assessing a method's capability to accurately identify sound events in audio, a common practice is to compare predicted sound events with the ground truth. Regardless of the specific matching approach, the outcome typically involves a set of matched and unmatched sound events. In response to this, the soundevent package introduces the Match object to represent these scenarios.</p> <p>A Match object comprises potentially empty source (predicted) and target (annotated) sound events, an affinity score offering a numerical measure of geometric similarity, an overall numeric score for the match, and a set of additional metrics. Notably, some predictions or ground truth sound events may remain unmatched, and this is accommodated by creating a Match object with either the source or the target (but not both) empty. The additional metrics are essentially instances of Features, representing named continuous values.</p> <pre><code>erDiagram\n    Match {\n        float affinity\n        float score\n    }\n    SoundEventPrediction\n    SoundEventAnnotation\n    Feature\n    Match }|--o| SoundEventPrediction : source\n    Match }|--o| SoundEventAnnotation : target\n    Match ||--o{ Feature : metrics</code></pre> Understanding Affinity and Score <p>Affinity serves as a measure of how well the geometries or regions of interest of two matched sound events align, disregarding any information about the semantic \"meaning\" of the sound. On the contrary, the overall score for the match incorporates this semantic information. For instance, there might be a predicted sound event whose geometry aligns with one of the ground truth events, but the assigned class is entirely incorrect. In such cases, a high affinity may be observed, but the score will be low due to the misalignment in semantic interpretation.</p>"},{"location":"data_schemas/evaluation/#clip_evaluation","title":"Clip Evaluation","text":"<p>The ClipEvaluation object encapsulates all information related to the assessment of a Clip Prediction in comparison to the ground truth Clip Annotations. It includes details about all sound event matches, whether matched or unmatched, along with an overall numeric score for the entire prediction. Additionally, a list of supplementary metrics is provided, offering insights into various aspects of the prediction's performance.</p> <pre><code>erDiagram\n    ClipEvaluation {\n        UUID uuid\n        float score\n    }\n    ClipAnnotation\n    ClipPrediction\n    Match\n    Feature\n    ClipEvaluation }|--|| ClipAnnotation : annotations\n    ClipEvaluation }|--|| ClipPrediction : predictions\n    ClipEvaluation ||--o{ Match : matches\n    ClipEvaluation ||--o{ Feature : metrics</code></pre>"},{"location":"data_schemas/evaluation/#evaluation_1","title":"Evaluation","text":"<p>The Evaluation object serves as a collection of clip evaluations, offering an overall score along with additional metrics. This object is designed to represent a model's performance across a set of Clips, providing a means to assess its correctness and reliability. As the evaluation of performance is context-dependent, the object includes an evaluation_task field, a text field providing a clear indication of the specific task attempted by the predictions. This context ensures that the scores and metrics provided have a well-defined meaning. For instance, examples of evaluation tasks include \"Clip Classification,\" where predictions aim to accurately determine the \"class\" of each processed clip. While there are no strict restrictions on this field, using standard names is recommended for easier comparison between evaluations.</p> <pre><code>erDiagram\n    Evaluation {\n        UUID uuid\n        datetime created_on\n        str evaluation_task\n        float score\n    }\n    ClipEvaluation\n    Feature\n    Evaluation ||--|{ ClipEvaluation : clip_evaluations\n    Evaluation ||--o| Feature : metrics</code></pre>"},{"location":"data_schemas/evaluation/#evaluation_set","title":"Evaluation Set","text":"<p>An Evaluation Set is a curated collection of fully annotated clips designed for reliable evaluation purposes. Serving as a comprehensive evaluation tool, it can be viewed as the equivalent of a benchmark dataset. To facilitate understanding and usage, each evaluation set is characterized by a name and a description. The name provides a clear identifier, while the description communicates essential information about the contents and intended use cases of the evaluation set. This ensures that researchers and practitioners can confidently employ the evaluation set to assess the performance and reliability of various models and algorithms in a standardized manner.</p> <pre><code>erDiagram\n    EvaluationSet {\n        UUID uuid\n        datetime created_on\n        str name\n        str description\n    }\n    ClipAnnotation\n    EvaluationSet }|--|{ ClipAnnotation : clip_annotations</code></pre>"},{"location":"data_schemas/prediction/","title":"Prediction","text":"<p>While Annotation involves expert human interpretation, often considered as ground truth, in <code>soundevent</code>, we use the term Prediction to denote interpretations derived through uncertain means, involving some level of guesswork or uncertainty. These interpretations could be the outputs of algorithms designed to automate the annotation task or even other individuals providing speculative insights into the \"truth\" of the sounds they are analyzing.</p> <p>In <code>soundevent</code>, we introduce objects designed for discussing predictions: SoundEventPrediction, SequencePrediction, ClipPrediction. These entities store predictions that aim to estimate the true state of their corresponding entities. All prediction objects share a similar structure, providing a measure of uncertainty by storing an overall confidence score. They also incorporate PredictedTags, resembling regular tags but with an additional confidence score assigned to each tag.</p>"},{"location":"data_schemas/prediction/#sound_event_predictions","title":"Sound Event Predictions","text":"<p>The SoundEventPrediction object represents a single sound event predicted through uncertain means. This object contains information about the predicted sound event, including an overall confidence score, and a list of predicted tags describing the anticipated characteristics of the sound event.</p> <pre><code>erDiagram\n    SoundEventPrediction {\n        UUID uuid\n        float score\n    }\n    SoundEvent\n    PredictedTag {\n        float score\n    }\n    Tag\n    SoundEventPrediction ||--|| SoundEvent : soundevent\n    SoundEventPrediction ||--o{ PredictedTag : tags\n    PredictedTag }|--|| Tag : tag</code></pre>"},{"location":"data_schemas/prediction/#sequence_predictions","title":"Sequence Predictions","text":"<p>The SequencePrediction object represents a predicted sequence of sound events. Much like Sound Event Predictions, it encapsulates information about the sequence, offering an overall confidence score for the prediction and a list of PredictedTags.</p> <pre><code>erDiagram\n    SequencePrediction {\n        UUID uuid\n        float score\n    }\n    Sequence\n    PredictedTag {\n        float score\n    }\n    Tag\n    SequencePrediction ||--|| Sequence : sequence\n    SequencePrediction ||--o{ PredictedTag : tags\n    PredictedTag }|--|| Tag : tag</code></pre>"},{"location":"data_schemas/prediction/#clip_predictions","title":"Clip Predictions","text":"<p>The ClipPrediction object encompasses all predictions made for the entire Clip, incorporating predictions for sound events or sequences. Like the preceding predictions, it retains fields for the overall confidence score and the predicted tags.</p> <p>Interpreting predicted tags at the clip level is straightforward\u2014they are tags that apply to the entire acoustic content, proving beneficial for tasks like sound scene classification. However, the interpretation of the score field differs from sound event or sequence predictions. Rather than providing an overall score for the confidence of sound event or sequence presence, the clip score can serve to encode the confidence of a binary classification problem. A low score indicates that the clip would not be considered a positive example in the binary classification problem. It's important to note that the utilization of scores to encode the necessary information is entirely at the discretion of the user.</p> <pre><code>erDiagram\n    ClipPrediction {\n        UUID uuid\n        float score\n    }\n    Clip\n    PredictedTag {\n        float score\n    }\n    Tag\n    SoundEventPrediction\n    SequencePrediction\n    ClipPrediction }|--|| Clip : clip\n    ClipPrediction ||--o{ PredictedTag : tags\n    ClipPrediction ||--o{ SoundEventPrediction : sound_events\n    ClipPrediction ||--o{ SequencePrediction : sequences\n    PredictedTag }|--|| Tag : tag</code></pre>"},{"location":"data_schemas/prediction/#model_runs","title":"Model Runs","text":"<p>The ModelRun object in <code>soundevent</code> serves to store collections of predictions originating from the same source. This object includes a set of predictions and provides details such as the model's name, an optional version (for precise method tracking), and a description of the method employed.</p> <pre><code>erDiagram\n    ModelRun {\n        UUID uuid\n        datetime created_on\n        str name\n        str version\n        str description\n    }\n    ClipPrediction\n    ModelRun ||--|{ ClipPrediction : clip_predictions</code></pre>"},{"location":"generated/gallery/","title":"User Guide","text":"<p>Welcome to the User Guide for the <code>soundevent</code> package! This guide is designed to help you navigate through the various topics commonly encountered in computational Bioacoustic analysis and demonstrate how the <code>soundevent</code> package can assist you in your research. Our goal is to provide you with clear and easy-to-follow instructions to make your Bioacoustic analysis journey smooth and enjoyable.</p> <p>Each page in this guide is a fully executable Python script that you can download and modify. We encourage you to experiment and integrate these examples into your own analysis workflows to suit your specific needs.</p> <p>Let's start with the basics, where we will cover essential topics such as:</p> <ul> <li>How to read and write bioacoustic analysis objects in a format compatible with the <code>soundevent</code> package.</li> <li>Loading audio data from WAV files using the tools provided by <code>soundevent</code>.</li> <li>Computing spectrograms to gain insights into the acoustic characteristics of your data.</li> </ul> Info <p>Stay tuned for future releases of the soundevent package, where we will continue to expand the examples and demonstrate additional bioacoustic analysis techniques. We are committed to providing you with the tools and resources you need for successful bioacoustic research. Enjoy exploring the user guide and happy analyzing!</p> <p> Acoustic Objects </p> <p> Saving and Loading data. </p> <p> Loading audio </p> <p> Computing spectrograms </p> <p> Geometric Operations </p> <p> Integrating Crowsetta with Soundevent </p> <p> Plotting functions </p> <p> Download all examples in Python source code: gallery_python.zip</p> <p> Download all examples in Jupyter notebooks: gallery_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/0_objects/","title":"Acoustic Objects","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/0_objects/#acoustic_objects","title":"Acoustic Objects","text":"<p>Here we showcase the different objects that are defined within <code>soundevent</code>.</p>"},{"location":"generated/gallery/0_objects/#the_data_module","title":"The data module","text":"<p>All you need to do is import the <code>data</code> module from <code>soundevent</code>.</p> <pre><code>from soundevent import data\n</code></pre>"},{"location":"generated/gallery/0_objects/#geometries","title":"Geometries","text":"<p>Here we showcase the different geometries that are defined within <code>soundevent</code>.</p> <p>Warning</p> <p>All the geometry coordinates should be provided in seconds and Hz.</p>"},{"location":"generated/gallery/0_objects/#timestamp","title":"TimeStamp","text":"<p>A <code>TimeStamp</code> is defined by a point in time relative to the start of the recording.</p> <pre><code>time_stamp = data.TimeStamp(\n    coordinates=0.1,\n)\nprint(time_stamp.model_dump_json(indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"type\": \"TimeStamp\",\n  \"coordinates\": 0.1\n}\n</code></pre>"},{"location":"generated/gallery/0_objects/#timeinterval","title":"TimeInterval","text":"<p>A <code>TimeInterval</code> consists of two points in time to mark the start and end of an interval.</p> <pre><code>time_interval = data.TimeInterval(\n    coordinates=[0.1, 0.2],\n)\nprint(time_interval.model_dump_json(indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"type\": \"TimeInterval\",\n  \"coordinates\": [\n    0.1,\n    0.2\n  ]\n}\n</code></pre>"},{"location":"generated/gallery/0_objects/#point","title":"Point","text":"<p>A <code>Point</code> is a point in time and frequency.</p> <pre><code>point = data.Point(\n    coordinates=[0.1, 2000],\n)\nprint(point.model_dump_json(indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"type\": \"Point\",\n  \"coordinates\": [\n    0.1,\n    2000.0\n  ]\n}\n</code></pre>"},{"location":"generated/gallery/0_objects/#boundingbox","title":"BoundingBox","text":"<pre><code>box = data.BoundingBox(\n    coordinates=[0.1, 2000, 0.2, 3000],\n)\nprint(box.model_dump_json(indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"type\": \"BoundingBox\",\n  \"coordinates\": [\n    0.1,\n    2000.0,\n    0.2,\n    3000.0\n  ]\n}\n</code></pre> <p>A <code>LineString</code> is a sequence of points that are connected by a line.</p> <pre><code>line = data.LineString(\n    coordinates=[[0.1, 2000], [0.2, 4000]],\n)\nprint(line.model_dump_json(indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"type\": \"LineString\",\n  \"coordinates\": [\n    [\n      0.1,\n      2000.0\n    ],\n    [\n      0.2,\n      4000.0\n    ]\n  ]\n}\n</code></pre> <p>A <code>Polygon</code></p> <pre><code>polygon = data.Polygon(\n    coordinates=[\n        [\n            [0.1, 2000],\n            [0.2, 3000],\n            [0.3, 2000],\n            [0.2, 1000],\n            [0.1, 2000],\n        ],\n        [\n            [0.15, 2000],\n            [0.25, 2000],\n            [0.2, 1500],\n            [0.15, 2000],\n        ],\n    ],\n)\nprint(polygon.model_dump_json(indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"type\": \"Polygon\",\n  \"coordinates\": [\n    [\n      [\n        0.1,\n        2000.0\n      ],\n      [\n        0.2,\n        3000.0\n      ],\n      [\n        0.3,\n        2000.0\n      ],\n      [\n        0.2,\n        1000.0\n      ],\n      [\n        0.1,\n        2000.0\n      ]\n    ],\n    [\n      [\n        0.15,\n        2000.0\n      ],\n      [\n        0.25,\n        2000.0\n      ],\n      [\n        0.2,\n        1500.0\n      ],\n      [\n        0.15,\n        2000.0\n      ]\n    ]\n  ]\n}\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.702 seconds) Estimated memory usage: 16 MB</p> <p> Download Python source code: 0_objects.py</p> <p> Download Jupyter notebook: 0_objects.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/1_saving_and_loading/","title":"Saving and Loading data.","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/1_saving_and_loading/#saving_and_loading_data","title":"Saving and Loading data.","text":"<p>In <code>soundevent</code>, we use the Acoustic Objects Exchange Format (AOEF) for storing and exchanging audio objects. AOEF is a JSON-based format specifically designed to standardize the representation of computational bioacoustic data, enabling effective sharing and collaboration among researchers.</p> <p>Why JSON?</p> <p>JSON or JavaScript Object Notation, is a lightweight data-interchange format that is widely supported across various platforms and programming languages. It provides human-readable syntax and is commonly used in web applications, making it an ideal choice for data exchange.</p> <p>We use AOEF to share common collections of audio objects, such as datasets, annotation projects, evaluation sets, model runs and performance evaluations.</p> <p>To demonstrate how to save and load data in AOEF format, we provide examples below:</p>"},{"location":"generated/gallery/1_saving_and_loading/#datasets","title":"Datasets","text":"<p>Suppose we have an example dataset stored in the AOEF format. The dataset is stored as a text file following the JSON structure. To view the contents of the file, you can use the following code.</p> <pre><code>import json\nfrom pathlib import Path\n\ndataset_path = Path(\"example_dataset.json\")\nwith open(dataset_path) as file:\n    dataset_contents = json.load(file)\n\nprint(json.dumps(dataset_contents, indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"version\": \"1.1.0\",\n  \"created_on\": \"2025-08-10T23:37:26.992647\",\n  \"data\": {\n    \"uuid\": \"b1096756-eea2-4489-9e6a-b98b559647bb\",\n    \"collection_type\": \"dataset\",\n    \"created_on\": \"2023-11-21T13:43:14.742002\",\n    \"recordings\": [\n      {\n        \"uuid\": \"89957d47-f67d-4bfe-8352-bf0fe5a8ce3e\",\n        \"path\": \"recording1.wav\",\n        \"duration\": 10.0,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"time_expansion\": 10.0,\n        \"hash\": \"1234567890abcdef\",\n        \"date\": \"2021-01-01\",\n        \"time\": \"21:34:56\",\n        \"latitude\": 12.345,\n        \"longitude\": 34.567,\n        \"tags\": [\n          0,\n          1,\n          2\n        ],\n        \"features\": {\n          \"SNR\": 10.0,\n          \"ACI\": 0.5\n        },\n        \"notes\": [\n          {\n            \"uuid\": \"2931b864-43e4-4fb1-aae1-a214dccca6e3\",\n            \"message\": \"This is a note.\",\n            \"created_by\": \"04ef3927-3a3d-40df-9d6e-2cc5e21482a0\",\n            \"is_issue\": false,\n            \"created_on\": \"2023-11-21T13:43:14.742073\"\n          }\n        ],\n        \"owners\": [\n          \"d6eb0862-a619-4919-992c-eb3625692c13\"\n        ]\n      },\n      {\n        \"uuid\": \"bd30f886-3abb-475b-aacb-c7148a4d4420\",\n        \"path\": \"recording2.wav\",\n        \"duration\": 8.0,\n        \"channels\": 1,\n        \"samplerate\": 441000,\n        \"time_expansion\": 10.0,\n        \"hash\": \"234567890abcdef1\",\n        \"date\": \"2021-01-02\",\n        \"time\": \"19:34:56\",\n        \"latitude\": 13.345,\n        \"longitude\": 32.567,\n        \"tags\": [\n          3,\n          4,\n          5\n        ],\n        \"features\": {\n          \"SNR\": 7.0,\n          \"ACI\": 0.3\n        },\n        \"notes\": [\n          {\n            \"uuid\": \"713b6c15-0e3d-4cc5-acc6-3f1093209a40\",\n            \"message\": \"Unsure about the species.\",\n            \"created_by\": \"04ef3927-3a3d-40df-9d6e-2cc5e21482a0\",\n            \"is_issue\": false,\n            \"created_on\": \"2023-11-21T13:43:14.742147\"\n          }\n        ],\n        \"owners\": [\n          \"d6eb0862-a619-4919-992c-eb3625692c13\"\n        ]\n      }\n    ],\n    \"tags\": [\n      {\n        \"id\": 0,\n        \"key\": \"species\",\n        \"value\": \"Myotis myotis\"\n      },\n      {\n        \"id\": 1,\n        \"key\": \"sex\",\n        \"value\": \"female\"\n      },\n      {\n        \"id\": 2,\n        \"key\": \"behaviour\",\n        \"value\": \"foraging\"\n      },\n      {\n        \"id\": 3,\n        \"key\": \"species\",\n        \"value\": \"Eptesicus serotinus\"\n      },\n      {\n        \"id\": 4,\n        \"key\": \"sex\",\n        \"value\": \"male\"\n      },\n      {\n        \"id\": 5,\n        \"key\": \"behaviour\",\n        \"value\": \"social calls\"\n      }\n    ],\n    \"users\": [\n      {\n        \"uuid\": \"04ef3927-3a3d-40df-9d6e-2cc5e21482a0\",\n        \"name\": \"John Doe\"\n      },\n      {\n        \"uuid\": \"d6eb0862-a619-4919-992c-eb3625692c13\",\n        \"email\": \"data.collector@soundevent.org\",\n        \"name\": \"Data Collector\"\n      }\n    ],\n    \"name\": \"test_dataset\",\n    \"description\": \"A test dataset\"\n  }\n}\n</code></pre>"},{"location":"generated/gallery/1_saving_and_loading/#loading_datasets","title":"Loading Datasets","text":"<p>By using the loading functions provided by the <code>soundevent</code> package, you can directly load the data into Python and obtain a <code>Dataset</code> object.</p> <pre><code>from soundevent import io\n\ndataset = io.load(dataset_path)\nprint(repr(dataset))\n</code></pre> <p>Out:</p> <pre><code>Dataset(uuid=UUID('b1096756-eea2-4489-9e6a-b98b559647bb'), created_on=datetime.datetime(2023, 11, 21, 13, 43, 14, 742002), name='test_dataset', description='A test dataset')\n</code></pre> <p>The <code>load</code> function allows you to access and analyze the dataset, which contains recordings and related objects, all structured in a standardized and manageable way.</p> <pre><code>recording = dataset.recordings[0]\nprint(f\"First recording: {recording!r}\")\nprint(f\"Recording tags: {recording.tags}\")\n</code></pre> <p>Out:</p> <pre><code>First recording: Recording(path=PosixPath('recording1.wav'))\nRecording tags: [Tag(term=Term(label='Scientific Taxon Name'), value='Myotis myotis'), Tag(term=Term(label='sex'), value='female'), Tag(term=Term(label='behaviour'), value='foraging')]\n</code></pre>"},{"location":"generated/gallery/1_saving_and_loading/#saving_datasets","title":"Saving Datasets","text":"<p>If you have your own dataset, you can save it to a file using the <code>save</code> function. This function stores the dataset in AOEF format, ensuring compatibility and easy sharing with other researchers.</p> <pre><code>io.save(dataset, dataset_path)\n</code></pre>"},{"location":"generated/gallery/1_saving_and_loading/#annotation_projects","title":"Annotation Projects","text":"<p>Similar to loading datasets, you can also use the <code>load</code> function to load annotations stored in the AOEF format.</p> <p>Here we have transformed 10 random annotated recordings from the NIPS4BPlus dataset into the AOEF format and stored it in the <code>nips4b_plus_aoef.json</code> file. You can use the provided code to view the annotations.</p> <pre><code>annotation_path = Path(\"nips4b_plus_sample.json\")\nwith open(annotation_path) as file:\n    annotation_contents = json.load(file)\n\nprint(json.dumps(annotation_contents, indent=2))\n</code></pre> <p>Out:</p> <pre><code>{\n  \"version\": \"1.1.0\",\n  \"created_on\": \"2025-08-10T23:37:27.738390\",\n  \"data\": {\n    \"uuid\": \"c18624a1-8145-4657-a3f1-b3512134ecf6\",\n    \"collection_type\": \"annotation_set\",\n    \"recordings\": [\n      {\n        \"uuid\": \"8392b0ff-293f-4d5b-bc1b-d40d2a0eb0dc\",\n        \"path\": \"train/nips4b_birds_trainfile079.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"35e120d0-1633-4864-88c9-063aca992747\",\n        \"path\": \"train/nips4b_birds_trainfile237.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"7ee23c44-1bc6-4833-8d66-14aa4a8e8634\",\n        \"path\": \"train/nips4b_birds_trainfile587.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"24eceb91-535e-42f0-80e4-8c670465bac8\",\n        \"path\": \"train/nips4b_birds_trainfile106.wav\",\n        \"duration\": 4.069297052154195,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"24dd23da-ca07-40eb-80e6-af8b6b2a75ee\",\n        \"path\": \"train/nips4b_birds_trainfile430.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"8954beed-21d0-4f20-98bd-58c14264d853\",\n        \"path\": \"train/nips4b_birds_trainfile661.wav\",\n        \"duration\": 1.3873922902494331,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"69f7bf42-087a-4d88-b312-f26688597974\",\n        \"path\": \"train/nips4b_birds_trainfile429.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"b98b5a59-3518-45e5-9306-67c522540ae1\",\n        \"path\": \"train/nips4b_birds_trainfile633.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"52e329a3-afbb-475e-938c-f70c88580723\",\n        \"path\": \"train/nips4b_birds_trainfile200.wav\",\n        \"duration\": 5.00390022675737,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      },\n      {\n        \"uuid\": \"6eeca672-f3ec-41ca-a551-54ddec0dd1a7\",\n        \"path\": \"train/nips4b_birds_trainfile545.wav\",\n        \"duration\": 2.7921995464852607,\n        \"channels\": 1,\n        \"samplerate\": 44100,\n        \"owners\": []\n      }\n    ],\n    \"clips\": [\n      {\n        \"uuid\": \"b283a71f-4aa8-4ee2-ac83-7c7d5d0af2bc\",\n        \"recording\": \"8392b0ff-293f-4d5b-bc1b-d40d2a0eb0dc\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"e2ec7571-bfe1-4682-8d4d-15e3e34edfc9\",\n        \"recording\": \"35e120d0-1633-4864-88c9-063aca992747\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"ab5bcc8f-078e-4194-97a4-763998d289fa\",\n        \"recording\": \"7ee23c44-1bc6-4833-8d66-14aa4a8e8634\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"dd9ad126-8b15-4121-84f5-f59dc3f85802\",\n        \"recording\": \"24eceb91-535e-42f0-80e4-8c670465bac8\",\n        \"start_time\": 0.0,\n        \"end_time\": 4.069297052154195\n      },\n      {\n        \"uuid\": \"0a53e14f-f51c-4ade-9567-87c2b02bd197\",\n        \"recording\": \"24dd23da-ca07-40eb-80e6-af8b6b2a75ee\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"ae455d97-d1fa-43a1-b179-907bee8471e6\",\n        \"recording\": \"8954beed-21d0-4f20-98bd-58c14264d853\",\n        \"start_time\": 0.0,\n        \"end_time\": 1.3873922902494331\n      },\n      {\n        \"uuid\": \"0b42c9e1-0289-4f03-b3cb-10859a80103e\",\n        \"recording\": \"69f7bf42-087a-4d88-b312-f26688597974\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"56329c3c-5beb-48ed-8a79-be4e9bd2d9ed\",\n        \"recording\": \"b98b5a59-3518-45e5-9306-67c522540ae1\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"57e0b23a-8663-49f3-8455-997e0f4d2b49\",\n        \"recording\": \"52e329a3-afbb-475e-938c-f70c88580723\",\n        \"start_time\": 0.0,\n        \"end_time\": 5.00390022675737\n      },\n      {\n        \"uuid\": \"c7b6f927-fd3b-40ac-9b25-d682eec2d3ac\",\n        \"recording\": \"6eeca672-f3ec-41ca-a551-54ddec0dd1a7\",\n        \"start_time\": 0.0,\n        \"end_time\": 2.7921995464852607\n      }\n    ],\n    \"clip_annotations\": [\n      {\n        \"uuid\": \"0e7a786e-48e4-4424-a8da-bf080bdefd9e\",\n        \"clip\": \"b283a71f-4aa8-4ee2-ac83-7c7d5d0af2bc\",\n        \"created_on\": \"2023-11-23T20:44:32.913233\"\n      },\n      {\n        \"uuid\": \"af9713a7-2bf4-45df-bdff-63d47de34d71\",\n        \"clip\": \"e2ec7571-bfe1-4682-8d4d-15e3e34edfc9\",\n        \"created_on\": \"2023-11-23T20:44:32.913255\"\n      },\n      {\n        \"uuid\": \"70738dff-3c98-4838-979c-0ed073edd0ea\",\n        \"clip\": \"ab5bcc8f-078e-4194-97a4-763998d289fa\",\n        \"created_on\": \"2023-11-23T20:44:32.913264\"\n      },\n      {\n        \"uuid\": \"fa0fc36e-0c1a-450f-900b-5a32df17a159\",\n        \"clip\": \"dd9ad126-8b15-4121-84f5-f59dc3f85802\",\n        \"created_on\": \"2023-11-23T20:44:32.913377\"\n      },\n      {\n        \"uuid\": \"178bf156-2bab-4970-8a76-4abfdc4b31b7\",\n        \"clip\": \"0a53e14f-f51c-4ade-9567-87c2b02bd197\",\n        \"created_on\": \"2023-11-23T20:44:32.913386\"\n      },\n      {\n        \"uuid\": \"dc71d06a-54d6-4fad-ac6a-205bbee7ec96\",\n        \"clip\": \"ae455d97-d1fa-43a1-b179-907bee8471e6\",\n        \"created_on\": \"2023-11-23T20:44:32.913392\"\n      },\n      {\n        \"uuid\": \"b418abd6-52cc-4ef4-9725-33d3d38b7878\",\n        \"clip\": \"0b42c9e1-0289-4f03-b3cb-10859a80103e\",\n        \"created_on\": \"2023-11-23T20:44:32.913400\"\n      },\n      {\n        \"uuid\": \"d18ca56f-2f13-44dd-be43-f995e4d2edb6\",\n        \"clip\": \"56329c3c-5beb-48ed-8a79-be4e9bd2d9ed\",\n        \"created_on\": \"2023-11-23T20:44:32.913415\"\n      },\n      {\n        \"uuid\": \"8dfa7d26-5f8b-4f37-bf0a-c031373b22f7\",\n        \"clip\": \"57e0b23a-8663-49f3-8455-997e0f4d2b49\",\n        \"created_on\": \"2023-11-23T20:44:32.913430\"\n      },\n      {\n        \"uuid\": \"ebc5e8d0-bd3e-4d01-95e0-d9a0ce337ac9\",\n        \"clip\": \"c7b6f927-fd3b-40ac-9b25-d682eec2d3ac\",\n        \"created_on\": \"2023-11-23T20:44:32.913437\"\n      }\n    ],\n    \"created_on\": \"2023-11-23T20:44:32.913488\"\n  }\n}\n</code></pre>"},{"location":"generated/gallery/1_saving_and_loading/#loading_annotation_projects","title":"Loading Annotation Projects","text":"<p>The <code>load</code> function can be used to load the annotations into Python and obtain an <code>AnnotationProject</code> object directly.</p> <pre><code>nips4b_sample = io.load(annotation_path, type=\"annotation_set\")\nprint(repr(nips4b_sample))\n</code></pre> <p>Out:</p> <pre><code>AnnotationSet(created_on=datetime.datetime(2023, 11, 23, 20, 44, 32, 913488), name=None, description=None)\n</code></pre> <p>This object allows you to access and analyze the annotations, along with their associated objects.</p> <pre><code>for clip_annotation in nips4b_sample.clip_annotations:\n    clip = clip_annotation.clip\n    recording = clip.recording\n    print(\n        f\"* Recording {recording.path} [from \"\n        f\"{clip.start_time:.3f}s to {clip.end_time:.3f}s]\"\n    )\n    print(\n        f\"\\t{len(clip_annotation.sound_events)} sound event annotations found\"\n    )\n    for annotation in clip_annotation.sound_events:\n        sound_event = annotation.sound_event\n        start_time, end_time = sound_event.geometry.coordinates\n        print(f\"\\t+ Sound event from {start_time:.3f}s to {end_time:.3f}s\")\n        for tag in annotation.tags:\n            print(f\"\\t\\t- {tag}\")\n    print(\"\")\n</code></pre> <p>Out:</p> <pre><code>* Recording train/nips4b_birds_trainfile079.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile237.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile587.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile106.wav [from 0.000s to 4.069s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile430.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile661.wav [from 0.000s to 1.387s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile429.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile633.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile200.wav [from 0.000s to 5.004s]\n        0 sound event annotations found\n\n* Recording train/nips4b_birds_trainfile545.wav [from 0.000s to 2.792s]\n        0 sound event annotations found\n</code></pre>"},{"location":"generated/gallery/1_saving_and_loading/#saving_annotation_projects","title":"Saving Annotation Projects","text":"<p>Saving the annotation project is just as straightforward using the <code>save</code> function:</p> <pre><code>io.save(nips4b_sample, \"nips4b_plus_sample.json\")\n</code></pre>"},{"location":"generated/gallery/1_saving_and_loading/#model_runs","title":"Model Runs","text":"<p>Finally, the outputs of a model run can also be stored in the AOEF format. You can save and load model runs using the <code>save</code> and <code>load</code> functions, respectively. The loading function reads the AOEF file and returns a <code>ModelRun</code> object that can be used for further analysis.</p> <p>By utilizing the saving and loading functions provided by soundevent, you can easily manage and exchange acoustic data objects in AOEF format, promoting collaboration and advancing your bioacoustic research endeavors.</p> <p>Total running time of the script: ( 0 minutes  0.706 seconds) Estimated memory usage: 32 MB</p> <p> Download Python source code: 1_saving_and_loading.py</p> <p> Download Jupyter notebook: 1_saving_and_loading.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/2_loading_audio/","title":"Loading audio","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/2_loading_audio/#loading_audio","title":"Loading audio","text":"<p>One of the fundamental operations in computational bioacoustics is reading audio files into a workable format. In <code>soundevent</code>, we use <code>xarray.DataArray</code> objects to hold loaded audio data. <code>xarray.DataArray</code> objects are an extension of <code>numpy</code> arrays, so there's no need to learn new concepts if you are already familiar with <code>numpy</code> arrays.</p> <p>Why use <code>xarray.DataArray</code> objects?</p> <p><code>xarray.DataArray</code> objects offer two key benefits: coordinates for easier referencing of time-related locations in the array, and the ability to store additional metadata such as <code>samplerate</code>, <code>time_expansion</code>, and specify that the temporal units are seconds. To learn more about <code>xarray.DataArray</code> objects, see the xarray documentation.</p> <p>Supported audio formats</p> <p><code>soundevent</code> supports most of the audio file formats supported by the <code>soundfile</code> library. Some formats were excluded because they do not support seeking and are not suitable for random access. This still includes most of the common audio file formats, such as WAV, FLAC, AIFF, and MP3. For a full list of supported formats, see the audio.is_audio_file documentation.</p>"},{"location":"generated/gallery/2_loading_audio/#getting_a_recording_object","title":"Getting a Recording object","text":"<p>To create a <code>data.Recording</code> object from an audio file, you can use the <code>from_file</code> method. This method extracts the metadata from the file and populates the <code>Recording</code> object with the relevant information.</p> <pre><code>from soundevent import data\n\nrecording = data.Recording.from_file(\"sample_audio.wav\")\nprint(repr(recording))\n</code></pre> <p>Out:</p> <pre><code>Recording(path=PosixPath('sample_audio.wav'))\n</code></pre>"},{"location":"generated/gallery/2_loading_audio/#loading_the_audio","title":"Loading the audio","text":"<p>Once you have a <code>data.Recording</code> object, you can load the audio data using the <code>audio.load_recording</code> function:</p> <pre><code>from soundevent import audio\n\nwav = audio.load_recording(recording)\nprint(wav)\n</code></pre> <p>Out:</p> <pre><code>&lt;xarray.DataArray (time: 66150, channel: 1)&gt; Size: 529kB\narray([[ 9.15527344e-05],\n       [ 2.13623047e-04],\n       [ 0.00000000e+00],\n       ...,\n       [-3.66210938e-04],\n       [-1.30310059e-02],\n       [-6.92749023e-03]])\nCoordinates:\n  * time     (time) float64 529kB 0.0 4.535e-05 9.07e-05 ... 3.0 3.0 3.0\n  * channel  (channel) int64 8B 0\nAttributes:\n    recording_id:   9568a146-1f68-4af5-b2e1-52229ca096ba\n    path:           sample_audio.wav\n    units:          V\n    standard_name:  amplitude\n    long_name:      Amplitude\n</code></pre> <p>Note that the returned object is an <code>xarray.DataArray</code> object with two dimensions: time and channel. The time coordinate represents the array of times in seconds corresponding to the samples in the xarray.DataArray object.</p>"},{"location":"generated/gallery/2_loading_audio/#selecting_clips_from_a_recording","title":"Selecting clips from a recording","text":"<p>You can use the <code>sel</code> method of xarray.DataArray to select a clip from the recording. This is useful when you have the full file loaded into memory and want to extract a specific clip:</p> <pre><code># You can select a clip by specifying the start and end times in seconds.\nsubwav = wav.sel(time=slice(0, 1))\nprint(repr(subwav))\n</code></pre> <p>Out:</p> <pre><code>&lt;xarray.DataArray (time: 22051, channel: 1)&gt; Size: 176kB\narray([[ 9.15527344e-05],\n       [ 2.13623047e-04],\n       [ 0.00000000e+00],\n       ...,\n       [-9.88769531e-03],\n       [-6.65283203e-03],\n       [-1.06811523e-03]])\nCoordinates:\n  * time     (time) float64 176kB 0.0 4.535e-05 9.07e-05 ... 0.9999 1.0 1.0\n  * channel  (channel) int64 8B 0\nAttributes:\n    recording_id:   9568a146-1f68-4af5-b2e1-52229ca096ba\n    path:           sample_audio.wav\n    units:          V\n    standard_name:  amplitude\n    long_name:      Amplitude\n</code></pre> <p>Alternatively, if you only need to load a clip from the file without loading the entire file into memory, you can use the <code>audio.load_clip</code> function:</p> <pre><code>clip = data.Clip(\n    recording=recording,\n    start_time=0,\n    end_time=1,\n)\nsubwav2 = audio.load_clip(clip)\nprint(repr(subwav2))\n</code></pre> <p>Out:</p> <pre><code>&lt;xarray.DataArray (time: 22050, channel: 1)&gt; Size: 176kB\narray([[ 9.15527344e-05],\n       [ 2.13623047e-04],\n       [ 0.00000000e+00],\n       ...,\n       [-1.11694336e-02],\n       [-9.88769531e-03],\n       [-6.65283203e-03]])\nCoordinates:\n  * time     (time) float64 176kB 0.0 4.535e-05 9.07e-05 ... 0.9999 0.9999 1.0\n  * channel  (channel) int64 8B 0\nAttributes:\n    recording_id:   9568a146-1f68-4af5-b2e1-52229ca096ba\n    clip_id:        fa74e966-4fc6-4378-bcbb-6d2cf6657610\n    path:           sample_audio.wav\n    units:          V\n    standard_name:  amplitude\n    long_name:      Amplitude\n</code></pre> <p>In most cases, the results from <code>wav.sel</code> and <code>audio.load_clip</code> will be the same, except for the last sample. However, the difference is negligible, and the <code>audio.load_clip</code> function is generally preferred for efficiency.</p> <p>You can verify the similarity of the clips:</p> <pre><code>import numpy as np\n\nprint(np.allclose(subwav[:-1], subwav2))\n</code></pre> <p>Out:</p> <pre><code>True\n</code></pre> <p>Total running time of the script: ( 0 minutes  2.369 seconds) Estimated memory usage: 88 MB</p> <p> Download Python source code: 2_loading_audio.py</p> <p> Download Jupyter notebook: 2_loading_audio.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/3_computing_spectrograms/","title":"Computing spectrograms","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/3_computing_spectrograms/#computing_spectrograms","title":"Computing spectrograms","text":"<p>After audio loading one of the most common operations within audio analysis is to compute spectral representations of the audio signal. These attempt to describe the signal in terms of its frequency content over time.</p> <p>The most common spectral representation is the spectrogram. This is a two-dimensional representation of the signal that shows the frequency content of the signal over time. The spectrogram is computed by taking the short-time Fourier transform (STFT) of the signal.</p> <p>Here we will show how to compute spectrograms using the <code>audio.compute_spectrogram</code> function.</p> <p>First, we will load a recording. We will use the the example recording from the (audio loading tutorial)[audio_loading].</p> <pre><code>from soundevent import arrays, audio, data\n\nrecording = data.Recording.from_file(\"sample_audio.wav\")\nwave = audio.load_recording(recording)\nprint(wave)\n</code></pre> <p>Out:</p> <pre><code>&lt;xarray.DataArray (time: 66150, channel: 1)&gt; Size: 529kB\narray([[ 9.15527344e-05],\n       [ 2.13623047e-04],\n       [ 0.00000000e+00],\n       ...,\n       [-3.66210938e-04],\n       [-1.30310059e-02],\n       [-6.92749023e-03]])\nCoordinates:\n  * time     (time) float64 529kB 0.0 4.535e-05 9.07e-05 ... 3.0 3.0 3.0\n  * channel  (channel) int64 8B 0\nAttributes:\n    recording_id:   0b8cbf03-3594-4290-96eb-41210823b501\n    path:           sample_audio.wav\n    units:          V\n    standard_name:  amplitude\n    long_name:      Amplitude\n</code></pre> <p>Next, we will compute the spectrogram. Notice the spectrogram parameters are specified in terms of time, not samples. This is to facilitate working with audio signals that have different sample rates.</p> <pre><code>spectrogram = audio.compute_spectrogram(\n    wave,\n    window_size=0.064,\n    hop_size=0.032,\n)\nprint(spectrogram)\n</code></pre> <p>Out:</p> <pre><code>&lt;xarray.DataArray (frequency: 706, time: 95, channel: 1)&gt; Size: 537kB\narray([[[9.48147039e-10],\n        [1.91579591e-09],\n        [3.98807627e-09],\n        ...,\n        [1.07020113e-08],\n        [1.55005539e-09],\n        [3.26203298e-08]],\n\n       [[9.54001588e-10],\n        [2.12661567e-09],\n        [9.61547511e-10],\n        ...,\n        [2.32123311e-09],\n        [1.87590952e-08],\n        [5.31787765e-08]],\n\n       [[7.86433801e-10],\n        [8.36052541e-10],\n        [2.13328850e-09],\n        ...,\n...\n        ...,\n        [4.53488538e-09],\n        [3.50412152e-09],\n        [6.53502087e-10]],\n\n       [[1.30371431e-14],\n        [1.07942541e-13],\n        [3.98618216e-14],\n        ...,\n        [1.62310750e-08],\n        [3.94325918e-09],\n        [5.51758315e-10]],\n\n       [[1.23269432e-15],\n        [1.13442323e-14],\n        [2.60915676e-14],\n        ...,\n        [6.38338455e-09],\n        [7.43449520e-10],\n        [4.53509492e-10]]])\nCoordinates:\n  * frequency  (frequency) float64 6kB 0.0 15.63 31.25 ... 1.1e+04 1.102e+04\n  * time       (time) float64 760B 0.0 0.03202 0.06404 ... 2.946 2.978 3.01\n  * channel    (channel) int64 8B 0\nAttributes:\n    recording_id:   0b8cbf03-3594-4290-96eb-41210823b501\n    path:           sample_audio.wav\n    units:          V**2/Hz\n    standard_name:  spectrogram\n    long_name:      Power Spectral Density Spectrogram\n    window_size:    0.064\n    hop_size:       0.032\n    window_type:    hann\n</code></pre> <p>Notice that the spectrogram is a three-dimensional <code>xarray.DataArray</code>. The first dimension is frequency, the second is time, and the third is channel. The spectrogram is computed separately for each channel of the audio signal.</p> <p>One of the nice things about using xarray is that it allows us to easily plot the spectrogram using the built-in plotting functions.</p> <pre><code>spectrogram.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.collections.QuadMesh object at 0x7fd344af8040&gt;\n</code></pre> <p>The initial plot is hard to interpret due to the linear scale. Decibels (dB)  are more perceptually relevant for sound. Let's convert it to decibels using the <code>arrays.to_db</code> function.</p> <pre><code>spectrogram_db = arrays.to_db(spectrogram)\nspectrogram_db.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.collections.QuadMesh object at 0x7fd371bca6a0&gt;\n</code></pre> <p>This is much better! We can now clearly see the frequency content evolving over time.</p> <p>To make subtle details even more apparent, we can apply a de-noising technique like PCEN (Per-Channel Energy Normalization). PCEN helps reduce background noise and enhance the target sounds. We can apply PCEN using the <code>audio.pcen</code> function.</p> <pre><code>pcen = audio.pcen(spectrogram)\npcen_db = arrays.to_db(pcen)\npcen_db.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.collections.QuadMesh object at 0x7fd371667d00&gt;\n</code></pre> <p>In this case the PCEN transformation has not made a huge difference, but it can be very useful in other cases.</p> <p>Total running time of the script: ( 0 minutes  1.507 seconds) Estimated memory usage: 43 MB</p> <p> Download Python source code: 3_computing_spectrograms.py</p> <p> Download Jupyter notebook: 3_computing_spectrograms.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/4_geometric_operations/","title":"Geometric Operations","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/4_geometric_operations/#geometric_operations","title":"Geometric Operations","text":"<p>Sound events are naturally spatial object defined by their location in both time and frequency. The soundevent's geometry module provides tools to manipulate the geometry of the sound events, facilitating the analysis of acoustic data.</p> Usage details <p>To use the <code>soundevent.geometry</code> module you need to install some additional dependencies. You can do this by running the following command:</p> <pre><code>pip install soundevent[geometry]\n</code></pre>"},{"location":"generated/gallery/4_geometric_operations/#understanding_geometric_operations","title":"Understanding Geometric Operations","text":"<p>Geometric operations provide essential tools to conduct sound event analysis:</p> <ol> <li>Buffering: Expand sound event boundaries, accommodating uncertainties inherent in annotations.</li> <li>Clipping: Remove unwanted sections.</li> <li>Spatial Relationships: Determine overlaps, containment, and distances between sound events, aiding in comparative analysis.</li> </ol> <p>Here we will explore the geometric operations that can be applied to sound events and how they can be used to analyze and compare sound events.</p> <pre><code>from soundevent import data, geometry\n\ngeom = data.Polygon(coordinates=[[[1, 2], [4, 3], [5, 6], [1, 2]]])\ngeom\n</code></pre> 1.005.0026POLYGON ((0.25 0.5, 1 0.75, 1.25 1.5, 0.25 0.5)) <pre><code>buffered = geometry.buffer_geometry(geom, time_buffer=1, freq_buffer=0.5)\nbuffered\n</code></pre> 0.006.7918POLYGON ((0 0.1449628471538281, 0 0.2880963635219793, 0.9273687952358558 1.144962847153828, 1 1.1340513875370795, 0.7235737572896033 0.3678177689649299, 0 0.1449628471538281))"},{"location":"generated/gallery/4_geometric_operations/#datasets","title":"Datasets","text":"<p>Total running time of the script: ( 0 minutes  0.374 seconds) Estimated memory usage: 30 MB</p> <p> Download Python source code: 4_geometric_operations.py</p> <p> Download Jupyter notebook: 4_geometric_operations.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/5_integration_with_crowsetta/","title":"Integrating Crowsetta with Soundevent","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/5_integration_with_crowsetta/#integrating_crowsetta_with_soundevent","title":"Integrating Crowsetta with Soundevent","text":"<p>Crowsetta is a versatile Python tool designed for handling annotations of animal vocalizations and bioacoustics data. If you're working with diverse annotation formats Crowsetta has you covered. Soundevent complements this functionality with its <code>soundevent.io.crowsetta</code> module, offering a convenient solution for converting between Crowsetta and Soundevent formats.</p> <p>In this guide, we'll walk through the process of using Crowsetta to load annotations and then converting them to Soundevent format using the <code>soundevent.io.crowsetta</code> module.</p> Usage details <p>To use the <code>soundevent.io.crowsetta</code> module you need to install some additional dependencies. You can do this by running the following command:</p> <pre><code>pip install soundevent[crowsetta]\n</code></pre>"},{"location":"generated/gallery/5_integration_with_crowsetta/#loading_annotations_with_crowsetta","title":"Loading annotations with <code>crowsetta</code>","text":"<p>To begin our journey, let's delve into loading annotations using Crowsetta.</p>"},{"location":"generated/gallery/5_integration_with_crowsetta/#crowsetta_supported_formats","title":"Crowsetta supported formats","text":"<p>Crowsetta offers support for various annotation formats. Let's explore the available formats:</p> <pre><code>import crowsetta\n\nprint(crowsetta.data.available_formats())\n</code></pre> <p>Out:</p> <pre><code>/home/runner/work/soundevent/soundevent/.venv/lib/python3.9/site-packages/crowsetta/_vendor/textgrid/textgrid.py:562: DeprecationWarning: invalid escape sequence \\w\n  m = re.match('File type = \"([\\w ]+)\"', header)\n['aud-txt', 'birdsong-recognition-dataset', 'generic-seq', 'notmat', 'raven', 'simple-seq', 'textgrid', 'timit']\n</code></pre>"},{"location":"generated/gallery/5_integration_with_crowsetta/#loading_example_raven_annotations","title":"Loading Example Raven Annotations","text":"<p>Let's walk through the process of loading example Raven annotations using Crowsetta.</p> <pre><code>import os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    # Extract the example data files to a temporary directory\n    data_dir = os.path.join(tmpdirname, \"crowsetta_data\")\n    crowsetta.data.extract_data_files(user_data_dir=data_dir)\n\n    # Select a Raven example file\n    example_file = crowsetta.data.get(\"raven\", user_data_dir=data_dir)\n\n    # Create a Raven transcriber\n    transcriber = crowsetta.Transcriber(\"raven\")\n\n    # Load the Raven annotations\n    # For this example, we assume the annotations correspond to a test audio\n    # file.\n    raven_annotations = transcriber.from_file(\n        example_file.annot_path,\n        annot_col=\"Species\",\n        audio_path=\"sample_audio.wav\",\n    )\n\n    print(raven_annotations)\n\n    # Convert the annotations to the standard crowsetta format\n    annotations = raven_annotations.to_annot()\n\nprint(f\"Citation: {example_file.citation}\")\nprint(f\"Loaded {len(annotations.bboxes)} bounding box annotations\")\nprint(\"Notated file: \", annotations.notated_path)\n</code></pre> <p>Out:</p> <pre><code>Raven(df=   Selection           View  Channel  ...  low_freq_hz  high_freq_hz  annotation\n0          1  Spectrogram 1        1  ...       2878.2        4049.0        EATO\n1          2  Spectrogram 1        1  ...       2731.9        3902.7        EATO\n2          3  Spectrogram 1        1  ...       2878.2        3975.8        EATO\n3          4  Spectrogram 1        1  ...       2756.2        3951.4        EATO\n4          5  Spectrogram 1        1  ...       2707.5        3975.8        EATO\n5          6  Spectrogram 1        1  ...       2951.4        3975.8        EATO\n\n[6 rows x 8 columns], annot_path=PosixPath('/tmp/tmppj1lwz94/crowsetta_data/raven/Recording_1_Segment_02.Table.1.selections.txt'), annot_col='Species', audio_path=PosixPath('sample_audio.wav'))\nCitation: Chronister, L. M., Rhinehart, T. A., Place, A., &amp; Kitzes, J. (2021). An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information.https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z\nLoaded 6 bounding box annotations\nNotated file:  sample_audio.wav\n</code></pre>"},{"location":"generated/gallery/5_integration_with_crowsetta/#converting_to_soundevent_format","title":"Converting to Soundevent format","text":"<p>Having successfully loaded the annotations using Crowsetta, we're now ready to convert them to Soundevent format.</p> <pre><code>import soundevent.io.crowsetta as cr\n\n# Convert Crowsetta Annotations to Soundevent ClipAnnotation\nclip_annotation = cr.annotation_to_clip_annotation(annotations)\n\n# Print JSON representation of the ClipAnnotation object\nprint(\n    clip_annotation.model_dump_json(\n        indent=2,\n        # Avoid printing irrelevant information\n        exclude_none=True,\n        exclude_defaults=True,\n        exclude_unset=True,\n    )\n)\n</code></pre> <p>Out:</p> <pre><code>{\n  \"clip\": {\n    \"recording\": {\n      \"path\": \"sample_audio.wav\",\n      \"duration\": 3.0,\n      \"channels\": 1,\n      \"samplerate\": 22050,\n      \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n    },\n    \"start_time\": 0.0,\n    \"end_time\": 3.0\n  },\n  \"sound_events\": [\n    {\n      \"sound_event\": {\n        \"geometry\": {\n          \"coordinates\": [\n            154.387792767,\n            2878.2,\n            154.911598217,\n            4049.0\n          ]\n        },\n        \"recording\": {\n          \"path\": \"sample_audio.wav\",\n          \"duration\": 3.0,\n          \"channels\": 1,\n          \"samplerate\": 22050,\n          \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n        }\n      },\n      \"tags\": [\n        {\n          \"term\": {\n            \"label\": \"crowsetta\",\n            \"definition\": \"Unknown\",\n            \"name\": \"crowsetta\"\n          },\n          \"value\": \"EATO\"\n        }\n      ]\n    },\n    {\n      \"sound_event\": {\n        \"geometry\": {\n          \"coordinates\": [\n            167.526598245,\n            2731.9,\n            168.17302044,\n            3902.7\n          ]\n        },\n        \"recording\": {\n          \"path\": \"sample_audio.wav\",\n          \"duration\": 3.0,\n          \"channels\": 1,\n          \"samplerate\": 22050,\n          \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n        }\n      },\n      \"tags\": [\n        {\n          \"term\": {\n            \"label\": \"crowsetta\",\n            \"definition\": \"Unknown\",\n            \"name\": \"crowsetta\"\n          },\n          \"value\": \"EATO\"\n        }\n      ]\n    },\n    {\n      \"sound_event\": {\n        \"geometry\": {\n          \"coordinates\": [\n            183.609636834,\n            2878.2,\n            184.097751553,\n            3975.8\n          ]\n        },\n        \"recording\": {\n          \"path\": \"sample_audio.wav\",\n          \"duration\": 3.0,\n          \"channels\": 1,\n          \"samplerate\": 22050,\n          \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n        }\n      },\n      \"tags\": [\n        {\n          \"term\": {\n            \"label\": \"crowsetta\",\n            \"definition\": \"Unknown\",\n            \"name\": \"crowsetta\"\n          },\n          \"value\": \"EATO\"\n        }\n      ]\n    },\n    {\n      \"sound_event\": {\n        \"geometry\": {\n          \"coordinates\": [\n            250.527480604,\n            2756.2,\n            251.160710509,\n            3951.4\n          ]\n        },\n        \"recording\": {\n          \"path\": \"sample_audio.wav\",\n          \"duration\": 3.0,\n          \"channels\": 1,\n          \"samplerate\": 22050,\n          \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n        }\n      },\n      \"tags\": [\n        {\n          \"term\": {\n            \"label\": \"crowsetta\",\n            \"definition\": \"Unknown\",\n            \"name\": \"crowsetta\"\n          },\n          \"value\": \"EATO\"\n        }\n      ]\n    },\n    {\n      \"sound_event\": {\n        \"geometry\": {\n          \"coordinates\": [\n            277.88724277,\n            2707.5,\n            278.480895806,\n            3975.8\n          ]\n        },\n        \"recording\": {\n          \"path\": \"sample_audio.wav\",\n          \"duration\": 3.0,\n          \"channels\": 1,\n          \"samplerate\": 22050,\n          \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n        }\n      },\n      \"tags\": [\n        {\n          \"term\": {\n            \"label\": \"crowsetta\",\n            \"definition\": \"Unknown\",\n            \"name\": \"crowsetta\"\n          },\n          \"value\": \"EATO\"\n        }\n      ]\n    },\n    {\n      \"sound_event\": {\n        \"geometry\": {\n          \"coordinates\": [\n            295.52970757,\n            2951.4,\n            296.110168316,\n            3975.8\n          ]\n        },\n        \"recording\": {\n          \"path\": \"sample_audio.wav\",\n          \"duration\": 3.0,\n          \"channels\": 1,\n          \"samplerate\": 22050,\n          \"hash\": \"7df7fabc84c9fa3d235db620c38ef288\"\n        }\n      },\n      \"tags\": [\n        {\n          \"term\": {\n            \"label\": \"crowsetta\",\n            \"definition\": \"Unknown\",\n            \"name\": \"crowsetta\"\n          },\n          \"value\": \"EATO\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>And that's it! We have successfully loaded annotations using <code>crowsetta</code> and converted them to <code>soundevent</code> format.</p>"},{"location":"generated/gallery/5_integration_with_crowsetta/#converting_back_to_crowsetta_format","title":"Converting back to <code>crowsetta</code> format","text":"<p>Now, let's explore the process of converting Soundevent annotations back to Crowsetta objects.</p> <pre><code>from soundevent.data import (\n    BoundingBox,\n    Clip,\n    ClipAnnotation,\n    Recording,\n    SoundEvent,\n    SoundEventAnnotation,\n    Tag,\n)\n\n# First, let's create some annotations for the example audio file\nrecording = Recording.from_file(\"sample_audio.wav\")\n\nclip_annotation = ClipAnnotation(\n    clip=Clip(\n        recording=recording,\n        start_time=0,\n        end_time=1,\n    ),\n    sound_events=[\n        SoundEventAnnotation(\n            tags=[\n                Tag(key=\"species\", value=\"bird\"),\n                Tag(key=\"color\", value=\"red\"),\n            ],\n            sound_event=SoundEvent(\n                recording=recording,\n                geometry=BoundingBox(coordinates=[0.1, 2000, 0.2, 3000]),\n            ),\n        ),\n        SoundEventAnnotation(\n            tags=[Tag(key=\"species\", value=\"frog\")],\n            sound_event=SoundEvent(\n                recording=recording,\n                geometry=BoundingBox(coordinates=[0.3, 1000, 0.6, 1500]),\n            ),\n        ),\n    ],\n)\n</code></pre> <p>Now, let's convert the ClipAnnotation object to Crowsetta format</p> <pre><code>annotations = cr.annotation_from_clip_annotation(\n    clip_annotation,\n    \"random_file_path.txt\",\n    annotation_fmt=\"bbox\",\n)\n\nprint(annotations)\n</code></pre> <p>Out:</p> <pre><code>Annotation(annot_path=PosixPath('random_file_path.txt'), notated_path=PosixPath('sample_audio.wav'), bboxes=[BBox(onset=0.1, offset=0.2, low_freq=2000.0, high_freq=3000.0, label='dwc:scientificName:bird,color:red'), BBox(onset=0.3, offset=0.6, low_freq=1000.0, high_freq=1500.0, label='dwc:scientificName:frog')])\n</code></pre> <p>Note</p> <p>While working with <code>crowsetta</code>, annotation objects are typically loaded from a file. In this demonstration, we're using a random file name to instantiate the annotations, even though the file doesn't exist. It's important to note that <code>crowsetta</code> requires a file path to create annotations, even if they are not actually written to the file. Therefore, using a random filepath is a safe practice.</p>"},{"location":"generated/gallery/5_integration_with_crowsetta/#finer_control","title":"Finer Control","text":"<p>When converting between crowsetta and soundevent formats, you have a multitude of options at your disposal. Soundevent objects can contain a wealth of information beyond what crowsetta objects offer, including multiple tags, notes, various geometry types, and more. Consequently, the conversion process isn't always straightforward. Particularly when converting from soundevent to crowsetta format, you'll need to make decisions regarding how to handle the additional information.</p>"},{"location":"generated/gallery/5_integration_with_crowsetta/#tags_and_labels","title":"Tags and Labels","text":"<p>One of the primary distinctions between crowsetta and soundevent lies in their handling of labels/tags. While crowsetta employs a single textual label for each annotation, soundevent utilizes a list of key-value tags. This difference complicates the conversion process.</p> <p>By default, when converting to crowsetta format, the label field of the crowsetta annotation gets converted to a single tag with the key <code>crowsetta</code> and the value of the label field. However, numerous customization options are available to tailor this behavior. Refer to the documentation for more information.</p> <pre><code>label = \"bird\"\ntags = cr.label_to_tags(label)\nprint(tags)\n</code></pre> <p>Out:</p> <pre><code>[Tag(term=Term(label='crowsetta'), value='bird')]\n</code></pre> <p>In the reverse direction, the default behavior amalgamates all tags into a single label. For example:</p> <pre><code>tags = [\n    Tag(key=\"species\", value=\"bird\"),\n    Tag(key=\"color\", value=\"red\"),\n]\nlabel = cr.label_from_tags(tags)\nprint(label)\n</code></pre> <p>Out:</p> <pre><code>dwc:scientificName:bird,color:red\n</code></pre> <p>Once again, you have the option to customize this behavior. Refer to the documentation for more information.</p> <p>Total running time of the script: ( 0 minutes  1.312 seconds) Estimated memory usage: 20 MB</p> <p> Download Python source code: 5_integration_with_crowsetta.py</p> <p> Download Jupyter notebook: 5_integration_with_crowsetta.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/6_plotting/","title":"Plotting functions","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/6_plotting/#plotting_functions","title":"Plotting functions","text":"<p>In this tutorial, we will demonstrate how to plot audio data and spectrograms using the built-in plotting functions in <code>soundevent</code>.</p>"},{"location":"generated/gallery/6_plotting/#audio_arrays","title":"Audio Arrays","text":"<p>First lets compute the spectrogram of an audio file.</p> <pre><code>from soundevent import arrays, audio, data\n\nrecording = data.Recording.from_file(\"sample_audio.wav\")\nwave = audio.load_recording(recording)\nspectrogram = arrays.to_db(\n    audio.compute_spectrogram(\n        wave,\n        window_size=0.064,\n        hop_size=0.032,\n    )\n)\n</code></pre> <p>Both the wave and spectrogram are xarray.DataArray objects. Hence we can use the built-in plotting functions to visualize them.</p> <pre><code>wave.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>[&lt;matplotlib.lines.Line2D object at 0x7fd340a8d7f0&gt;]\n</code></pre> <p>And the spectrogram.</p> <pre><code>spectrogram.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.collections.QuadMesh object at 0x7fd37159f7f0&gt;\n</code></pre> <p>Note</p> <p>xarray plotting functions are quite flexible and allow you to customize the plot. To see how to use the xarray plotting functions, see the xarray plotting documentation.</p>"},{"location":"generated/gallery/6_plotting/#geometries","title":"Geometries","text":"<p>To plot geometries, we can use the <code>plot_geometry</code> function in the <code>soundevent.plot</code> module.</p> <pre><code>from soundevent import plot\n</code></pre> <p>We can plot the different geometries that are defined within <code>soundevent</code>.</p> <pre><code>time_stamp = data.TimeStamp(coordinates=0.1)\ntime_interval = data.TimeInterval(coordinates=[0.2, 0.3])\npoint = data.Point(coordinates=[0.4, 6000])\nbox = data.BoundingBox(coordinates=[0.5, 3000, 0.6, 5000])\nline = data.LineString(coordinates=[[0.7, 2000], [0.8, 3000], [0.9, 8000]])\npolygon = data.Polygon(\n    coordinates=[\n        [\n            [1.0, 4000],\n            [1.1, 5000],\n            [1.2, 4000],\n            [1.1, 3000],\n            [1.0, 4000],\n        ]\n    ]\n)\n\nax = plot.plot_geometry(time_stamp)\nplot.plot_geometry(time_interval, ax=ax, color=\"red\")\nplot.plot_geometry(point, ax=ax, color=\"green\")\nplot.plot_geometry(box, ax=ax, color=\"blue\")\nplot.plot_geometry(line, ax=ax, color=\"purple\", linestyle=\"--\")\nplot.plot_geometry(polygon, ax=ax, color=\"orange\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Axes: &gt;\n</code></pre>"},{"location":"generated/gallery/6_plotting/#sound_event_annotations","title":"Sound Event Annotations","text":"<p>To plot sound event annotations, we can use the <code>plot_annotation</code> function in the <code>soundevent.plot</code> module.</p> <pre><code>sound_event_annotations = [\n    data.SoundEventAnnotation(\n        tags=[\n            data.Tag(key=\"animal\", value=\"dog\"),\n            data.Tag(key=\"loudness\", value=\"loud\"),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[0.58, 100, 0.78, 2000]),\n        ),\n    ),\n    data.SoundEventAnnotation(\n        tags=[\n            data.Tag(key=\"animal\", value=\"dog\"),\n            data.Tag(key=\"loudness\", value=\"loud\"),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[1.08, 60, 1.34, 1600]),\n        ),\n    ),\n    data.SoundEventAnnotation(\n        tags=[\n            data.Tag(key=\"animal\", value=\"dog\"),\n            data.Tag(key=\"loudness\", value=\"medium\"),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[1.52, 30, 1.69, 1400]),\n        ),\n    ),\n    data.SoundEventAnnotation(\n        tags=[\n            data.Tag(key=\"animal\", value=\"dog\"),\n            data.Tag(key=\"loudness\", value=\"soft\"),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[1.98, 30, 2.1, 800]),\n        ),\n    ),\n    data.SoundEventAnnotation(\n        tags=[\n            data.Tag(key=\"animal\", value=\"dog\"),\n            data.Tag(key=\"loudness\", value=\"soft\"),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[2.45, 30, 2.70, 1300]),\n        ),\n    ),\n]\n\nax = plot.create_axes()\nspectrogram.plot(ax=ax, cmap=\"gray\")\n\nplot.plot_annotations(\n    sound_event_annotations,\n    ax=ax,\n    color=\"red\",\n    add_points=False,\n    time_offset=0.03,\n    freq_offset=300,\n)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Axes: title={'center': 'channel = 0'}, xlabel='Time since start of recording\\n[s]', ylabel='Frequency [Hz]'&gt;\n</code></pre>"},{"location":"generated/gallery/6_plotting/#sound_event_predictions","title":"Sound Event Predictions","text":"<pre><code>sound_event_predictions = [\n    data.SoundEventPrediction(\n        score=0.9,\n        tags=[\n            data.PredictedTag(\n                score=0.95, tag=data.Tag(key=\"animal\", value=\"dog\")\n            ),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[0.58, 100, 0.78, 2000]),\n        ),\n    ),\n    data.SoundEventPrediction(\n        score=0.8,\n        tags=[\n            data.PredictedTag(\n                score=0.9, tag=data.Tag(key=\"animal\", value=\"dog\")\n            ),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[1.08, 60, 1.34, 1600]),\n        ),\n    ),\n    data.SoundEventPrediction(\n        score=0.4,\n        tags=[\n            data.PredictedTag(\n                score=0.7, tag=data.Tag(key=\"animal\", value=\"dog\")\n            ),\n            data.PredictedTag(\n                score=0.3, tag=data.Tag(key=\"animal\", value=\"cat\")\n            ),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[1.52, 30, 1.69, 1400]),\n        ),\n    ),\n    data.SoundEventPrediction(\n        score=0.3,\n        tags=[\n            data.PredictedTag(\n                score=0.5, tag=data.Tag(key=\"animal\", value=\"dog\")\n            ),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[1.98, 30, 2.1, 800]),\n        ),\n    ),\n    data.SoundEventPrediction(\n        score=0.8,\n        tags=[\n            data.PredictedTag(\n                score=0.4, tag=data.Tag(key=\"animal\", value=\"dog\")\n            ),\n        ],\n        sound_event=data.SoundEvent(\n            recording=recording,\n            geometry=data.BoundingBox(coordinates=[2.45, 30, 2.70, 1300]),\n        ),\n    ),\n]\n\nax = plot.create_axes()\nspectrogram.plot(ax=ax, cmap=\"gray\")\nplot.plot_predictions(\n    sound_event_predictions,\n    ax=ax,\n    color=\"red\",\n    add_points=False,\n    time_offset=0.03,\n    freq_offset=300,\n)\n</code></pre> <p>Out:</p> <pre><code>&lt;Axes: title={'center': 'channel = 0'}, xlabel='Time since start of recording\\n[s]', ylabel='Frequency [Hz]'&gt;\n</code></pre> <p>Total running time of the script: ( 0 minutes  2.528 seconds) Estimated memory usage: 76 MB</p> <p> Download Python source code: 6_plotting.py</p> <p> Download Jupyter notebook: 6_plotting.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/mg_execution_times/","title":"Computation times","text":"<p>00:09.498 total execution time for generated_gallery files:</p> <p>+---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 6_plotting (docs/user_guide/6_plotting.py)                                                       | 00:02.528 | 76.4 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 2_loading_audio (docs/user_guide/2_loading_audio.py)                                        | 00:02.369 | 88.4 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 3_computing_spectrograms (docs/user_guide/3_computing_spectrograms.py)             | 00:01.507 | 42.7 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 5_integration_with_crowsetta (docs/user_guide/5_integration_with_crowsetta.py) | 00:01.312 | 20.5 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 1_saving_and_loading (docs/user_guide/1_saving_and_loading.py)                         | 00:00.706 | 31.6 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 0_objects (docs/user_guide/0_objects.py)                                                          | 00:00.702 | 16.2 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+ | 4_geometric_operations (docs/user_guide/4_geometric_operations.py)                   | 00:00.374 | 29.6 MB | +---------------------------------------------------------------------------------------------------------------------+-----------+---------+</p>"},{"location":"reference/arrays/","title":"Arrays Module","text":"Additional dependencies <p>To use the <code>soundevent.arrays</code> module you need to install some additional dependencies. Make sure you have them installed by running the following command:</p> <pre><code>pip install soundevent[audio]\n</code></pre>"},{"location":"reference/arrays/#soundevent.arrays","title":"<code>soundevent.arrays</code>","text":"<p>Module for manipulation of xarray.DataArray objects.</p> <p>This module provides functions for manipulating xarray.DataArray objects, including creating range dimensions, setting dimension attributes, cropping and extending axes, getting dimension ranges and widths, and setting values at specific positions.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions","title":"<code>soundevent.arrays.dimensions</code>","text":"<p>Creating and manipulating DataArray dimensions in computational acoustics.</p> <p>This module provides functions to:</p> <ul> <li> <p>Define standard dimensions:  Quickly create common dimensions in computational acoustics like 'time', 'frequency', 'channel', 'category', and 'feature' using the <code>Dimensions</code> enumeration.</p> </li> <li> <p>Build flexible data structures:  Construct range-based dimensions (e.g., for time or frequency) with desired start, stop, and step values using <code>create_range_dim</code>.</p> </li> <li> <p>Work with time series:  Generate time dimensions from arrays or given parameters with <code>create_time_range</code> and <code>create_time_dim_from_array</code>.</p> </li> <li> <p>Handle frequency representations:  Create frequency dimensions from arrays or specified ranges with  <code>create_frequency_range</code> and <code>create_frequency_dim_from_array</code>.</p> </li> <li> <p>Modify and extract metadata:  Set dimension attributes (<code>set_dim_attrs</code>), retrieve dimension ranges (<code>get_dim_range</code>), calculate dimension width (<code>get_dim_width</code>), and estimate dimension step size (<code>get_dim_step</code>).</p> </li> </ul> <p>Classes:</p> Name Description <code>Dimensions</code> <p>Defines standard dimension names for computational acoustics arrays.</p> <p>Functions:</p> Name Description <code>create_frequency_dim_from_array</code> <p>Create a frequency dimension from an array of frequency values.</p> <code>create_frequency_range</code> <p>Generate an xarray Variable representing a frequency range dimension.</p> <code>create_range_dim</code> <p>Create a range dimension.</p> <code>create_time_dim_from_array</code> <p>Create a time dimension from an array of time values.</p> <code>create_time_range</code> <p>Generate an xarray Variable representing a time range dimension.</p> <code>estimate_dim_step</code> <p>Estimate the step size of a numerical array.</p> <code>get_coord_index</code> <p>Get the index of a value along a dimension in a DataArray.</p> <code>get_dim_range</code> <p>Get the range of a dimension in a data array.</p> <code>get_dim_step</code> <p>Calculate the step size between values along a dimension in a DataArray.</p> <code>get_dim_width</code> <p>Get the width of a dimension in a data array.</p> <code>set_dim_attrs</code> <p>Set the range of a dimension in a data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions-attributes","title":"Attributes","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.FREQUENCY_LONG_NAME","title":"<code>soundevent.arrays.dimensions.FREQUENCY_LONG_NAME = 'Frequency'</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.FREQUENCY_STANDARD_NAME","title":"<code>soundevent.arrays.dimensions.FREQUENCY_STANDARD_NAME = 'frequency'</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.FREQUENCY_UNITS","title":"<code>soundevent.arrays.dimensions.FREQUENCY_UNITS = 'Hz'</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.TIME_LONG_NAME","title":"<code>soundevent.arrays.dimensions.TIME_LONG_NAME = 'Time since start of recording'</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.TIME_STANDARD_NAME","title":"<code>soundevent.arrays.dimensions.TIME_STANDARD_NAME = 'time'</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.TIME_UNITS","title":"<code>soundevent.arrays.dimensions.TIME_UNITS = 's'</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions-classes","title":"Classes","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions","title":"<code>soundevent.arrays.dimensions.Dimensions</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Defines standard dimension names for computational acoustics arrays.</p> <p>This enumeration provides convenient and descriptive names for dimensions essential to representing acoustic data</p> Notes <p>Use these dimension names to ensure consistency and clarity in your code.</p> <p>Attributes:</p> Name Type Description <code>category</code> <p>Name for the category dimension of an array.</p> <code>channel</code> <p>Name for the channel dimension of an array.</p> <code>feature</code> <p>Name for the feature dimension of an array.</p> <code>frequency</code> <p>Name for the frequency dimension of an array.</p> <code>time</code> <p>Name for the time dimension of an array.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions-attributes","title":"Attributes","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions.category","title":"<code>category = 'category'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name for the category dimension of an array.</p> <p>This dimension represents a categorical variable or label for each element in the array. If the original data is not categorical, it's converted to categorical data. Each value should be a string or integer label corresponding to a category or class.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions.channel","title":"<code>channel = 'channel'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name for the channel dimension of an array.</p> <p>This dimension represents the channel number of a multi-channel array, typically used in multi-channel audio recordings or spectrograms. Each channel corresponds to a distinct audio source or microphone in the recording.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions.feature","title":"<code>feature = 'feature'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name for the feature dimension of an array.</p> <p>This dimension represents a feature or numerical descriptor of the data. It's not limited to feature extraction results but can also include hand-measured or derived features. If an array contains multiple features, each feature should be stored along this dimension, with the name of the feature stored as a coordinate variable. If the array has time and frequency dimensions, the feature dimension then represents the feature values at each time-frequency point.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions.frequency","title":"<code>frequency = 'frequency'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name for the frequency dimension of an array.</p> <p>This dimension represents frequency in Hz and should monotonically increase from the start to the end of the array. Generally regularly spaced, it may contain irregular spacing, such as with a logarithmic frequency scale or custom frequency bins.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.Dimensions.time","title":"<code>time = 'time'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name for the time dimension of an array.</p> <p>This dimension represents time in seconds and should monotonically increase from the start to the end of the array. While generally regularly spaced, it may contain missing values or irregular spacing in special cases.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions-functions","title":"Functions","text":""},{"location":"reference/arrays/#soundevent.arrays.dimensions.create_frequency_dim_from_array","title":"<code>soundevent.arrays.dimensions.create_frequency_dim_from_array(coods, name=Dimensions.frequency.value, step=None, estimate_step=False, dtype=None, **kwargs)</code>","text":"<p>Create a frequency dimension from an array of frequency values.</p> <p>Parameters:</p> Name Type Description Default <code>coods</code> <code>ndarray</code> <p>The frequency values.</p> required <code>name</code> <code>str</code> <p>The name of the frequency dimension.</p> <code>frequency.value</code> <code>dtype</code> <code>Optional[DTypeLike]</code> <p>The data type of the frequency values. If None, the data type is inferred from the input array.</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to store on the frequency dimension.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Variable</code> <p>The frequency dimension variable.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.create_frequency_range","title":"<code>soundevent.arrays.dimensions.create_frequency_range(low_freq, high_freq, step, name=Dimensions.frequency.value, dtype=np.float64, **attrs)</code>","text":"<p>Generate an xarray Variable representing a frequency range dimension.</p> <p>Creates a frequency range with a specified start (in Hz), end (in Hz),  and step size (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>low_freq</code> <code>float</code> <p>Start of the frequency range (in Hz).</p> required <code>high_freq</code> <code>float</code> <p>End of the frequency range (in Hz).</p> required <code>step</code> <code>float</code> <p>Step size between frequency values (in Hz).</p> required <code>name</code> <code>str</code> <p>Name of the frequency dimension. Defaults to 'frequency'.</p> <code>frequency.value</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type of the frequency values. Defaults to np.float64.</p> <code>float64</code> <code>**attrs</code> <p>Additional attributes for the xarray Variable.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Variable</code> <p>Variable containing the frequency range values.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.create_range_dim","title":"<code>soundevent.arrays.dimensions.create_range_dim(name, start, stop, step=None, size=None, dtype=np.float64, **attrs)</code>","text":"<p>Create a range dimension.</p> <p>Most coordinates used in computational bioacoustics are regularly spaced ranges. This function creates a range dimension with a specified start, stop, and step size. It stores the start, end, and step values as attributes on the coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the range dimension.</p> required <code>start</code> <code>float</code> <p>The start value of the range.</p> required <code>stop</code> <code>float</code> <p>The stop value of the range.</p> required <code>step</code> <code>float</code> <p>The step size between values in the range.</p> <code>None</code> <code>dtype</code> <code>dtype or str</code> <p>The data type of the values in the range. Defaults to np.float32.</p> <code>float64</code> <code>**attrs</code> <p>Additional attributes to store on the range dimension.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Variable</code> <p>A variable representing the range dimension.</p> Notes <ul> <li>The range is created using np.arange(start, stop, step, dtype).</li> <li>The variable has attributes 'start', 'end', and 'step' representing the     range parameters.</li> </ul>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.create_time_dim_from_array","title":"<code>soundevent.arrays.dimensions.create_time_dim_from_array(coods, name=Dimensions.time.value, dtype=None, step=None, samplerate=None, estimate_step=False, **kwargs)</code>","text":"<p>Create a time dimension from an array of time values.</p> <p>Parameters:</p> Name Type Description Default <code>coods</code> <code>ndarray</code> <p>The time values.</p> required <code>name</code> <code>str</code> <p>The name of the time dimension.</p> <code>time.value</code> <code>dtype</code> <code>Optional[DTypeLike]</code> <p>The data type of the time values. If None, the data type is inferred from the input array.</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to store on the time dimension.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Variable</code> <p>The time dimension variable.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.create_time_range","title":"<code>soundevent.arrays.dimensions.create_time_range(start_time, end_time, step=None, samplerate=None, name=Dimensions.time.value, dtype=np.float64, **attrs)</code>","text":"<p>Generate an xarray Variable representing a time range dimension.</p> <p>Creates a time range with specified start (in seconds), end (in seconds), and the desired time step between values.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Start of the time range (in seconds).</p> required <code>end_time</code> <code>float</code> <p>End of the time range (in seconds).</p> required <code>step</code> <code>Optional[float]</code> <p>Step size between time values (in seconds). If not provided,  calculated as 1 / samplerate.</p> <code>None</code> <code>samplerate</code> <code>Optional[float]</code> <p>Sampling rate (in Hz). Used to calculate step if step is not given.  If both step and samplerate are provided, step takes precedence.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the time dimension. Defaults to 'time'.</p> <code>time.value</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type of the time values. Defaults to np.float64.</p> <code>float64</code> <code>**attrs</code> <p>Additional attributes for the xarray Variable.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Variable</code> <p>Variable containing the time range values.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.estimate_dim_step","title":"<code>soundevent.arrays.dimensions.estimate_dim_step(data, rtol=1e-05, atol=1e-08, check_tolerance=True)</code>","text":"<p>Estimate the step size of a numerical array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The numerical array.</p> required <code>rtol</code> <code>float</code> <p>The relative tolerance used when checking if all values are within a specified range of the mean step size. Defaults to 1e-5.</p> <code>1e-05</code> <code>atol</code> <code>float</code> <p>The absolute tolerance used when checking if all values are within a specified range of the mean step size. Defaults to 1e-8.</p> <code>1e-08</code> <code>check_tolerance</code> <code>bool</code> <p>A flag indicating whether to perform a tolerance check on the differences between consecutive values. If True (default), raises a ValueError if the differences exceed the specified tolerances.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The estimated step size of the array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>check_tolerance</code> is True and the differences between consecutive values exceed the specified tolerances (indicating an irregular step size).</p> Notes <p>This function calculates the mean of the differences between consecutive values in the array. If <code>check_tolerance</code> is True, it verifies if all differences are within a specified tolerance (defined by <code>rtol</code> and <code>atol</code>) of the calculated mean step size. If not, it raises a <code>ValueError</code> indicating an irregular step size.</p> <p>This function assumes the array values are numerical and equidistant (constant step size) unless the tolerance check fails.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.get_coord_index","title":"<code>soundevent.arrays.dimensions.get_coord_index(arr, dim, value, raise_error=True)</code>","text":"<p>Get the index of a value along a dimension in a DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input DataArray.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension.</p> required <code>value</code> <code>float</code> <p>The value to find along the dimension.</p> required <code>raise_error</code> <code>bool</code> <p>A flag indicating whether to raise an error if the value is outside the range of the dimension. If True (default), raises a KeyError. If False, returns the index of the closest value within the range.</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>The index of the value along the specified dimension.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not found within the range of the dimension or if the dimension is not found in the DataArray.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.get_dim_range","title":"<code>soundevent.arrays.dimensions.get_dim_range(array, dim)</code>","text":"<p>Get the range of a dimension in a data array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The data array from which to extract the dimension range.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension.</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[float], Optional[float]]</code> <p>A tuple containing the start and end values of the dimension range.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the dimension is not found in the data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.get_dim_step","title":"<code>soundevent.arrays.dimensions.get_dim_step(arr, dim, rtol=1e-05, atol=1e-08, check_tolerance=True, estimate_step=True)</code>","text":"<p>Calculate the step size between values along a dimension in a DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input DataArray.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension for which to calculate the step size.</p> required <code>rtol</code> <code>float</code> <p>The relative tolerance used when checking if all coordinate differences are within a specified range of the mean step size. Defaults to 1e-5.</p> <code>1e-05</code> <code>atol</code> <code>float</code> <p>The absolute tolerance used when checking if all coordinate differences are within a specified range of the mean step size. Defaults to 1e-8.</p> <code>1e-08</code> <code>check_tolerance</code> <code>bool</code> <p>A flag indicating whether to perform a tolerance check on the coordinate differences. If True (default), raises a ValueError if the differences exceed the specified tolerances.</p> <code>True</code> <code>estimate_step</code> <code>bool</code> <p>A flag indicating whether to estimate the step size if not present in the dimension attributes. If True (default), calculates the mean step size from the coordinate values. Otherwise, raises a ValueError if the step size is not found in the dimension attributes.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The calculated step size (spacing) between consecutive values along the specified dimension.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>check_tolerance</code> is True and the coordinate differences exceed the specified tolerances (indicating an irregular step size).</p> Notes <p>This function first attempts to retrieve the step size from the dimension's attributes using the standard attribute name <code>'step'</code> defined in the <code>RangeAttrs</code> enumeration. If the attribute is not present, it calculates the step size by taking the mean of the differences between consecutive coordinate values.</p> <p>If <code>check_tolerance</code> is True, the function verifies if all coordinate differences are within a specified tolerance (defined by <code>rtol</code> and <code>atol</code>) of the calculated mean step size. If not, it raises a <code>ValueError</code> indicating an irregular step size.</p> <p>This function assumes the DataArray coordinates are numerical and equidistant (constant step size) unless a valid step size attribute is present or the tolerance check fails.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.get_dim_width","title":"<code>soundevent.arrays.dimensions.get_dim_width(arr, dim)</code>","text":"<p>Get the width of a dimension in a data array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The data array containing the dimension.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The width of the dimension.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the dimension is not found in the data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.dimensions.set_dim_attrs","title":"<code>soundevent.arrays.dimensions.set_dim_attrs(array, dim, **attrs)</code>","text":"<p>Set the range of a dimension in a data array.</p> <p>Use this function to set the precise start and end values of a dimension in a data array. This is useful when the coordinates represent the start of a range, but you want to specify the end of the range as well.</p> <p>The start and end values are stored as attributes on the coordinates.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes","title":"<code>soundevent.arrays.attributes</code>","text":"<p>Standard attributes for acoustic data arrays.</p> <p>This module provides enumerations for commonly used attributes to describe dimensions and arrays of numerical data in computational acoustics tasks. These attributes are based on a subset of the Climate and Forecast (CF) conventions documentation, a widely used standard for describing scientific data.</p> <p>The module includes enums for:</p> <ul> <li>Dimension attributes</li> <li>Range attributes</li> <li>Array attributes</li> </ul> <p>By using these standard attributes, you can ensure interoperability and consistency in your acoustic data representation.</p> <p>For a complete list of CF conventions attributes, refer to the Attribute Conventions.</p> <p>Classes:</p> Name Description <code>ArrayAttrs</code> <p>Standard attribute names for acoustic data arrays.</p> <code>DimAttrs</code> <p>Standard attribute names for acoustic data dimensions.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes-classes","title":"Classes","text":""},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs","title":"<code>soundevent.arrays.attributes.ArrayAttrs</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standard attribute names for acoustic data arrays.</p> <p>This enumeration defines standard attribute names used to describe properties of acoustic data arrays. These attributes follow the CF conventions and provide a consistent way to represent information about the data.</p> <p>Attributes:</p> Name Type Description <code>comment</code> <p>Attribute name for any additional comments or explanations about the</p> <code>long_name</code> <p>Attribute name for a human-readable description of the array variable.</p> <code>references</code> <p>Attribute name for references to external documentation or resources</p> <code>standard_name</code> <p>Attribute name for the standard name of an array variable.</p> <code>units</code> <p>Attribute name for the units of an array variable.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs-attributes","title":"Attributes","text":""},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs.comment","title":"<code>comment = 'comment'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for any additional comments or explanations about the array variable.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs.long_name","title":"<code>long_name = 'long_name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for a human-readable description of the array variable.</p> <p>This can be more detailed than the standard name and provide additional context for users.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs.references","title":"<code>references = 'references'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for references to external documentation or resources that provide more information about the array variable.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs.standard_name","title":"<code>standard_name = 'standard_name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for the standard name of an array variable.</p> <p>As defined by the CF conventions. This provides a consistent way to identify arrays across different datasets.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.ArrayAttrs.units","title":"<code>units = 'units'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for the units of an array variable.</p> <p>This specifies the physical quantity represented by the array, such as 'dB' for sound pressure level or 'meters' for distance. It follows the UDUNITS standard for unit symbols.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.DimAttrs","title":"<code>soundevent.arrays.attributes.DimAttrs</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standard attribute names for acoustic data dimensions.</p> <p>This enumeration defines standard attribute names used to describe dimensions of acoustic data arrays. These attributes follow the CF conventions and provide a consistent way to represent information about dimensions, such as units, standard names, and long names.</p> <p>Attributes:</p> Name Type Description <code>long_name</code> <p>Attribute name for a human-readable description of the dimension.</p> <code>standard_name</code> <p>Attribute name for the standard name of a dimension.</p> <code>step</code> <p>Attribute name for the step size of a range dimension. </p> <code>units</code> <p>Attribute name for the units of a dimension. </p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.DimAttrs-attributes","title":"Attributes","text":""},{"location":"reference/arrays/#soundevent.arrays.attributes.DimAttrs.long_name","title":"<code>long_name = 'long_name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for a human-readable description of the dimension.</p> <p>This can be more detailed than the standard name and provide additional context for users.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.DimAttrs.standard_name","title":"<code>standard_name = 'standard_name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for the standard name of a dimension.</p> <p>As defined by the CF conventions. This provides a consistent way to identify dimensions across different datasets.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.DimAttrs.step","title":"<code>step = 'step'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for the step size of a range dimension. </p> <p>Specifies the distance between consecutive values in the range, which might not be explicitly stored as a coordinate value. If not present, the dimension is assumed to be irregularly spaced. Not a standard CF attribute.</p>"},{"location":"reference/arrays/#soundevent.arrays.attributes.DimAttrs.units","title":"<code>units = 'units'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Attribute name for the units of a dimension. </p> <p>This specifies the physical quantity represented by the dimension, such as 'seconds' for time or 'meters' for distance. It follows the UDUNITS standard for unit symbols.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations","title":"<code>soundevent.arrays.operations</code>","text":"<p>Module for manipulation of xarray.DataArray objects.</p> <p>Functions:</p> Name Description <code>adjust_dim_range</code> <p>Adjust the range of a specified dimension in an xarray DataArray.</p> <code>adjust_dim_width</code> <p>Adjust the width of an xarray DataArray along a specified dimension.</p> <code>center</code> <p>Center the values of a data array around zero.</p> <code>crop_dim</code> <p>Crop a dimension of a data array to a specified range.</p> <code>crop_dim_width</code> <p>Crops an xarray DataArray along a specified dimension to a given width.</p> <code>extend_dim</code> <p>Extend a dimension of a data array to a specified range.</p> <code>extend_dim_width</code> <p>Extend an xarray DataArray along a specified dimension to a given width.</p> <code>normalize</code> <p>Normalize the values of a data array to the range [0, 1].</p> <code>offset</code> <p>Offset the values of a data array by a constant value.</p> <code>resize</code> <p>Resize a data array to the specified dimensions.</p> <code>scale</code> <p>Scale the values of a data array by a constant value.</p> <code>set_value_at_pos</code> <p>Set a value at a specific position in a data array.</p> <code>to_db</code> <p>Compute the decibel values of a data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations-attributes","title":"Attributes","text":""},{"location":"reference/arrays/#soundevent.arrays.operations.Position","title":"<code>soundevent.arrays.operations.Position = Literal['start', 'center', 'end']</code>  <code>module-attribute</code>","text":""},{"location":"reference/arrays/#soundevent.arrays.operations-functions","title":"Functions","text":""},{"location":"reference/arrays/#soundevent.arrays.operations.adjust_dim_range","title":"<code>soundevent.arrays.operations.adjust_dim_range(array, dim, start=None, stop=None, fill_value=0)</code>","text":"<p>Adjust the range of a specified dimension in an xarray DataArray.</p> <p>This function modifies the range of a given dimension (<code>dim</code>) in an xarray DataArray to match a desired range defined by <code>start</code> and <code>stop</code>. It ensures that the adjusted dimension aligns with these bounds while considering the original step size of the dimension.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The input xarray DataArray to be adjusted.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension to be adjusted.</p> required <code>start</code> <code>float</code> <p>The desired starting value for the dimension. If None (default), the starting value is not adjusted.</p> <code>None</code> <code>stop</code> <code>float</code> <p>The desired stopping value for the dimension. If None (default), the stopping value is not adjusted.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>The value to fill for missing data in the extended range. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The adjusted xarray DataArray with the modified dimension range.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If both <code>start</code> and <code>stop</code> are None.</li> <li>If <code>start</code> is greater than or equal to <code>stop</code>.</li> </ul> Notes <p>The function utilizes <code>crop_dim</code> and <code>extend_dim</code> to modify the specified dimension. It calculates adjusted bounds to ensure the modified dimension aligns with the desired range while preserving the original step size as much as possible.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.adjust_dim_width","title":"<code>soundevent.arrays.operations.adjust_dim_width(array, dim, width, fill_value=0, position='start')</code>","text":"<p>Adjust the width of an xarray DataArray along a specified dimension.</p> <p>This function effectively places the original array within a larger or smaller array of the desired width (<code>width</code>), cropping or extending it as needed. The <code>position</code> parameter controls where the original array is placed within this other array:</p> <ul> <li>'start': Places the input array at the start, cropping or extending   towards the end.</li> <li>'end': Places the input array at the end, cropping or extending   towards the start.</li> <li>'center': Places the input array at the center, cropping or extending   at both ends.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The DataArray to be adjusted.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension to adjust.</p> required <code>width</code> <code>int</code> <p>The desired width of the dimension.</p> required <code>fill_value</code> <code>float</code> <p>The value to fill the extended region with, if extending.</p> <code>0</code> <code>position</code> <code>(start, end, center)</code> <p>The position from which to crop or at which to extend. - 'start': Crop/extend at the beginning of the dimension. - 'end': Crop/extend at the end of the dimension. - 'center': Crop/extend at the center of the dimension.</p> <code>'start'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The adjusted DataArray.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the width is less than 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array = xr.DataArray(\n...     np.arange(10),\n...     dims=\"x\",\n...     coords={\"x\": np.arange(10)},\n... )\n&gt;&gt;&gt; adjust_dim_width(array, \"x\", 5, position=\"start\")\n&lt;xarray.DataArray (x: 5)&gt; Size: ...\narray([0, 1, 2, 3, 4])\nCoordinates:\n  * x        (x) ... 0 1 2 3 4\n&gt;&gt;&gt; adjust_dim_width(array, \"x\", 12, position=\"end\")\n&lt;xarray.DataArray (x: 12)&gt; Size: ...\narray([0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nCoordinates:\n  * x        (x) ... -2 -1 0 1 2 3 4 5 6 7 8 9\n</code></pre>"},{"location":"reference/arrays/#soundevent.arrays.operations.center","title":"<code>soundevent.arrays.operations.center(arr)</code>","text":"<p>Center the values of a data array around zero.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to center.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>The centered data array.</p> Notes <p>This function stores the offset used for centering as an attribute in the output data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.crop_dim","title":"<code>soundevent.arrays.operations.crop_dim(arr, dim, start=None, stop=None, right_closed=False, left_closed=True, eps=1e-05)</code>","text":"<p>Crop a dimension of a data array to a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to crop.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension to crop.</p> required <code>start</code> <code>Optional[float]</code> <p>The start value of the cropped range. If None, the current start value of the axis is used. Defaults to None.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>The stop value of the cropped range. If None, the current stop value of the axis is used. Defaults to None.</p> <code>None</code> <code>right_closed</code> <code>bool</code> <p>Whether the right boundary of the cropped range is closed. Defaults to False.</p> <code>False</code> <code>left_closed</code> <code>bool</code> <p>Whether the left boundary of the cropped range is closed. Defaults to True.</p> <code>True</code> <code>eps</code> <code>float</code> <p>A small value added to start and subtracted from stop to ensure open intervals. Defaults to 10e-6.</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The cropped data array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the coordinate for the specified dimension does not have 'start' and 'stop' attributes, or if the specified range is outside the current axis range.</p> Notes <p>The function crops the specified dimension of the data array to the range     [start, stop). The <code>right_closed</code> and <code>left_closed</code> parameters control whether the     boundaries of the cropped range are closed or open. A small value <code>eps</code> is added to start and subtracted from stop to ensure     open intervals if <code>right_closed</code> or <code>left_closed</code> is False. The 'start' and 'stop' attributes of the cropped dimension coordinate are     updated accordingly.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.crop_dim_width","title":"<code>soundevent.arrays.operations.crop_dim_width(array, dim, width, position='start')</code>","text":"<p>Crops an xarray DataArray along a specified dimension to a given width.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The DataArray to be cropped.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension to crop.</p> required <code>width</code> <code>int</code> <p>The desired width of the dimension after cropping.</p> required <code>position</code> <code>(start, end, center)</code> <p>The position from which to crop. - 'start': Crop from the beginning of the dimension. - 'end': Crop from the end of the dimension. - 'center': Crop from the center of the dimension.</p> <code>'start'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The cropped DataArray.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the new width is greater than or equal to the current width, or if the position is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array = xr.DataArray(\n...     np.arange(10), dims=\"x\", coords={\"x\": np.arange(10)}\n... )\n&gt;&gt;&gt; crop_dim_width(array, \"x\", 5, position=\"start\")\n&lt;xarray.DataArray (x: 5)&gt; Size: ...\narray([0, 1, 2, 3, 4])\nCoordinates:\n  * x        (x) ... 0 1 2 3 4\n&gt;&gt;&gt; crop_dim_width(array, \"x\", 3, position=\"end\")\n&lt;xarray.DataArray (x: 3)&gt; Size: ...\narray([7, 8, 9])\nCoordinates:\n  * x        (x) ... 7 8 9\n&gt;&gt;&gt; crop_dim_width(array, \"x\", 4, position=\"center\")\n&lt;xarray.DataArray (x: 4)&gt; Size: ...\narray([3, 4, 5, 6])\nCoordinates:\n  * x        (x) ... 3 4 5 6\n</code></pre>"},{"location":"reference/arrays/#soundevent.arrays.operations.extend_dim","title":"<code>soundevent.arrays.operations.extend_dim(arr, dim, start=None, stop=None, fill_value=0, eps=1e-05, left_closed=True, right_closed=False)</code>","text":"<p>Extend a dimension of a data array to a specified range.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to extend.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension to extend.</p> required <code>start</code> <code>Optional[float]</code> <p>The start value of the extended range.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>The stop value of the extended range.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>The value to fill for missing data in the extended range. Defaults to 0.</p> <code>0</code> <code>eps</code> <code>float</code> <p>A small value added to start and subtracted from stop to ensure open intervals. Defaults to 10e-6.</p> <code>1e-05</code> <code>left_closed</code> <code>bool</code> <p>Whether the left boundary of the extended range is closed. Defaults to True.</p> <code>True</code> <code>right_closed</code> <code>bool</code> <p>Whether the right boundary of the extended range is closed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The extended data array.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the dimension is not found in the data array.</p> Notes <p>The function extends the specified dimension of the data array to the     range [start, stop). If the specified range extends beyond the current axis range, the     function adds values to the beginning or end of the coordinate     array. The 'start' and 'stop' attributes of the extended dimension coordinate     are updated accordingly.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.extend_dim_width","title":"<code>soundevent.arrays.operations.extend_dim_width(array, dim, width, fill_value=0, position='start')</code>","text":"<p>Extend an xarray DataArray along a specified dimension to a given width.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The DataArray to be extended.</p> required <code>dim</code> <code>str</code> <p>The name of the dimension to extend.</p> required <code>width</code> <code>int</code> <p>The desired width of the dimension after extension.</p> required <code>fill_value</code> <code>float</code> <p>The value to fill the extended region with.</p> <code>0</code> <code>position</code> <code>(start, end, center)</code> <p>The position at which to extend. Imagine placing the original array within a larger array of the desired width. This parameter controls where the original array is placed within this larger array:</p> <ul> <li>'start': Keep the start position of the original array, and extend   towards the end of the larger array.</li> <li>'end': Keep the end position of the original array, and extend   towards the start of the larger array.</li> <li>'center': Extend at both ends of the original array, keeping it   centered within the larger array.</li> </ul> <code>'start'</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The extended DataArray.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the new width is less than or equal to the current width, or if the position is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; array = xr.DataArray(\n...     np.arange(5), dims=\"x\", coords={\"x\": np.arange(5)}\n... )\n&gt;&gt;&gt; extend_dim_width(array, dim=\"x\", width=8, position=\"start\")\n&lt;xarray.DataArray (x: 8)&gt; Size: ...\narray([0, 1, 2, 3, 4, 0, 0, 0])\nCoordinates:\n  * x        (x) ... 0 1 2 3 4 5 6 7\n&gt;&gt;&gt; extend_dim_width(array, dim=\"x\", width=7, position=\"end\")\n&lt;xarray.DataArray (x: 7)&gt; Size: ...\narray([0, 0, 0, 1, 2, 3, 4])\nCoordinates:\n  * x        (x) ... -2 -1 0 1 2 3 4\n&gt;&gt;&gt; extend_dim_width(array, dim=\"x\", width=9, position=\"center\")\n&lt;xarray.DataArray (x: 9)&gt; Size: ...\narray([0, 0, 0, 1, 2, 3, 4, 0, 0])\nCoordinates:\n  * x        (x) ... -2 -1 0 1 2 3 4 5 6\n</code></pre>"},{"location":"reference/arrays/#soundevent.arrays.operations.normalize","title":"<code>soundevent.arrays.operations.normalize(arr)</code>","text":"<p>Normalize the values of a data array to the range [0, 1].</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to normalize.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>The normalized data array.</p> Notes <p>This function stores the offset and scale factor used for normalization as attributes in the output data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.offset","title":"<code>soundevent.arrays.operations.offset(arr, val)</code>","text":"<p>Offset the values of a data array by a constant value.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to offset.</p> required <code>val</code> <code>float</code> <p>The value to add to the data array.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>The offset data array.</p> Notes <p>This function stores the offset used for the offsetting as an attribute in the output data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.resize","title":"<code>soundevent.arrays.operations.resize(arr, method='linear', dtype=np.float64, **dims)</code>","text":"<p>Resize a data array to the specified dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to resize.</p> required <code>method</code> <code>InterpOptions</code> <p>The interpolation method to use. Defaults to 'linear'.</p> <code>'linear'</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type of the resized data array. Defaults to np.float64.</p> <code>float64</code> <code>**dims</code> <code>int</code> <p>The new dimensions for each axis of the data array.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The resized data array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the new dimensions do not match the current dimensions of the data array.</p> Notes <p>This function resizes the data array to the specified dimensions. The function does not modify the data array in place.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.scale","title":"<code>soundevent.arrays.operations.scale(arr, val)</code>","text":"<p>Scale the values of a data array by a constant value.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array to scale.</p> required <code>val</code> <code>float</code> <p>The value to multiply the data array by.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>The scaled data array.</p> Notes <p>This function stores the scale factor used for scaling as an attribute in the output data array.</p>"},{"location":"reference/arrays/#soundevent.arrays.operations.set_value_at_pos","title":"<code>soundevent.arrays.operations.set_value_at_pos(array, value, **query)</code>","text":"<p>Set a value at a specific position in a data array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The input data array.</p> required <code>value</code> <code>Any</code> <p>The value to set at the specified position.</p> required <code>**query</code> <code>dict</code> <p>Keyword arguments specifying the position in each dimension where the value should be set. Keys are dimension names, values are the positions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The modified data array with the value set at the specified position.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a dimension specified in the query is not found in the data array.</p> <code>KeyError</code> <p>If the position specified in the query is outside the range of the corresponding dimension.</p> Notes <p>Modifies the input data array in-place.</p> <p>When specifying approximate positions (e.g., <code>x=1.5</code>) the value will be set at the closest coordinate to the left of the specified value. This aligns with how coordinates are often interpreted as the boundaries of intervals.</p> <p>If <code>value</code> is a tuple or list, its dimensions must match the queried dimensions of the array, and the value will be set at the specified position along each dimension.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.zeros((3, 3))\n&gt;&gt;&gt; coords = {\"x\": np.arange(3), \"y\": np.arange(3)}\n&gt;&gt;&gt; array = xr.DataArray(data, coords=coords, dims=(\"x\", \"y\"))\n</code></pre> <p>Setting a single value:</p> <pre><code>&gt;&gt;&gt; array = set_value_at_pos(array, 1, x=1, y=1)\n&gt;&gt;&gt; print(array)\n&lt;xarray.DataArray (x: 3, y: 3)&gt; Size: ...\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\nCoordinates:\n  * x        (x) ... 0 1 2\n  * y        (y) ... 0 1 2\n</code></pre> <p>Setting a value at an approximate position:</p> <pre><code>&gt;&gt;&gt; array = xr.DataArray(data, coords=coords, dims=(\"x\", \"y\"))\n&gt;&gt;&gt; array = set_value_at_pos(array, 1, x=1.5, y=1.5)\n&gt;&gt;&gt; print(array)\n&lt;xarray.DataArray (x: 3, y: 3)&gt; Size: ...\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\nCoordinates:\n  * x        (x) ... 0 1 2\n  * y        (y) ... 0 1 2\n</code></pre> <p>Setting a multi-dimensional value:</p> <pre><code>&gt;&gt;&gt; array = xr.DataArray(data, coords=coords, dims=(\"x\", \"y\"))\n&gt;&gt;&gt; value = np.array([1, 2, 3])\n&gt;&gt;&gt; array = set_value_at_pos(array, value, x=1)\n&gt;&gt;&gt; print(array)\n&lt;xarray.DataArray (x: 3, y: 3)&gt; Size: ...\narray([[0., 0., 0.],\n       [1., 2., 3.],\n       [0., 0., 0.]])\nCoordinates:\n  * x        (x) ... 0 1 2\n  * y        (y) ... 0 1 2\n</code></pre>"},{"location":"reference/arrays/#soundevent.arrays.operations.to_db","title":"<code>soundevent.arrays.operations.to_db(arr, ref=1.0, amin=1e-10, min_db=-80.0, max_db=None, power=1)</code>","text":"<p>Compute the decibel values of a data array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>DataArray</code> <p>The input data array.</p> required <code>ref</code> <code>Union[float, Callable[[DataArray], float]]</code> <p>The reference value for the decibel computation. Defaults to 1.0.</p> <code>1.0</code> <code>amin</code> <code>float</code> <p>Minimum threshold for the input data array. Defaults to 1e-10. All values below this threshold are replaced with this value before computing the decibel values.</p> <code>1e-10</code> <code>min_db</code> <code>Optional[float]</code> <p>The minimum decibel value for the output data array. Defaults to 80.0. All values below this threshold are replaced with this value. If None, no minimum threshold is applied.</p> <code>-80.0</code> <code>max_db</code> <code>Optional[float]</code> <p>The maximum decibel value for the output data array. Defaults to None. All values above this threshold are replaced with this value. If None, no maximum threshold is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The data array with decibel values computed.</p> Notes <p>The function computes the decibel values of the input data array using the formula 10 * log10(arr / ref).</p> <p>This function is heavily inspired by and includes modifications of code originally found in librosa, available at https://github.com/librosa/librosa/. The original code is licensed under the ISC license.</p> <p>Original copyright notice: Copyright (c) 2013--2023, librosa development team.</p> <p>Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.</p>"},{"location":"reference/audio/","title":"Audio Module","text":"Additional dependencies <p>To use the <code>soundevent.audio</code> module you need to install some additional dependencies. Make sure you have them installed by running the following command:</p> <pre><code>pip install soundevent[audio]\n</code></pre>"},{"location":"reference/audio/#soundevent.audio","title":"<code>soundevent.audio</code>","text":"<p>Soundevent functions for handling audio files and arrays.</p> <p>Modules:</p> Name Description <code>attributes</code> <p>Definition of common attributes for audio objects.</p> <code>files</code> <p>Functions for recognizing audio files.</p> <code>io</code> <p>Input and output functions for soundevent.</p> <code>media_info</code> <p>Functions for getting media information from WAV files.</p> <code>operations</code> <code>spectrograms</code> <p>Functions to compute several spectral representations of sound signals.</p> <p>Classes:</p> Name Description <code>MediaInfo</code> <p>MediaInfo Class.</p> <p>Functions:</p> Name Description <code>compute_md5_checksum</code> <p>Compute the MD5 checksum of a file.</p> <code>compute_spectrogram</code> <p>Compute the spectrogram of a signal.</p> <code>filter</code> <p>Filter audio data.</p> <code>get_audio_files</code> <p>Return a generator of audio files in a directory.</p> <code>get_media_info</code> <p>Return the media information from the WAV file.</p> <code>is_audio_file</code> <p>Return whether the file is an audio file.</p> <code>load_audio</code> <p>Load an audio file.</p> <code>load_clip</code> <p>Load a clip from a file.</p> <code>load_recording</code> <p>Load a recording from a file.</p> <code>pcen</code> <p>Apply PCEN to spectrogram.</p> <code>resample</code> <p>Resample array data to a target sample rate along a given dimension.</p>"},{"location":"reference/audio/#soundevent.audio-classes","title":"Classes","text":""},{"location":"reference/audio/#soundevent.audio.MediaInfo","title":"<code>MediaInfo(samplerate_hz, duration_s, samples, channels, format, subtype)</code>  <code>dataclass</code>","text":"<p>MediaInfo Class.</p> <p>Encapsulates essential metadata about audio data for processing and analysis. The information stored in this dataclass can typically be automatically extracted from the audio file itself.</p> <p>Attributes:</p> Name Type Description <code>channels</code> <code>int</code> <p>The number of audio channels present in the data.</p> <code>duration_s</code> <code>float</code> <p>The total duration of the audio, measured in seconds (s).</p> <code>format</code> <code>str</code> <p>A code representing the audio file format. </p> <code>samplerate_hz</code> <code>int</code> <p>The sampling rate of the audio, measured in Hertz (Hz). </p> <code>samples</code> <code>int</code> <p>The total number of samples in the audio data.</p> <code>subtype</code> <code>str</code> <p>A more specific subtype of the audio format. </p>"},{"location":"reference/audio/#soundevent.audio.MediaInfo-attributes","title":"Attributes","text":""},{"location":"reference/audio/#soundevent.audio.MediaInfo.channels","title":"<code>channels</code>  <code>instance-attribute</code>","text":"<p>The number of audio channels present in the data.</p> <p>For example 1 for mono, 2 for stereo.</p>"},{"location":"reference/audio/#soundevent.audio.MediaInfo.duration_s","title":"<code>duration_s</code>  <code>instance-attribute</code>","text":"<p>The total duration of the audio, measured in seconds (s).</p> <p>This represents the length of time the audio recording spans.</p>"},{"location":"reference/audio/#soundevent.audio.MediaInfo.format","title":"<code>format</code>  <code>instance-attribute</code>","text":"<p>A code representing the audio file format. </p> <p>For example \"WAV\", \"MP3\".</p>"},{"location":"reference/audio/#soundevent.audio.MediaInfo.samplerate_hz","title":"<code>samplerate_hz</code>  <code>instance-attribute</code>","text":"<p>The sampling rate of the audio, measured in Hertz (Hz). </p> <p>This indicates the number of samples taken per second to represent the analog audio signal.</p>"},{"location":"reference/audio/#soundevent.audio.MediaInfo.samples","title":"<code>samples</code>  <code>instance-attribute</code>","text":"<p>The total number of samples in the audio data.</p>"},{"location":"reference/audio/#soundevent.audio.MediaInfo.subtype","title":"<code>subtype</code>  <code>instance-attribute</code>","text":"<p>A more specific subtype of the audio format. </p> <p>For example \"PCM_16\", \"A_LAW\". These subtypes provide additional information about the audio data, such as the bit depth for PCM encoded audio, or encoding algorithm for compressed audio formats.</p>"},{"location":"reference/audio/#soundevent.audio-functions","title":"Functions","text":""},{"location":"reference/audio/#soundevent.audio.compute_md5_checksum","title":"<code>compute_md5_checksum(path)</code>","text":"<p>Compute the MD5 checksum of a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>MD5 checksum of the file.</p>"},{"location":"reference/audio/#soundevent.audio.compute_spectrogram","title":"<code>compute_spectrogram(audio, window_size, hop_size, window_type='hann', detrend=False, padded=True, boundary='zeros', scale='psd', sort_dims=DEFAULT_DIM_ORDER)</code>","text":"<p>Compute the spectrogram of a signal.</p> <p>This function calculates the short-time Fourier transform (STFT), which decomposes a signal into overlapping windows and computes the Fourier transform of each window.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>DataArray</code> <p>The audio signal.</p> required <code>window_size</code> <code>float</code> <p>The duration of the STFT window in seconds.</p> required <code>hop_size</code> <code>float</code> <p>The duration of the STFT hop (in seconds). This determines the time step between consecutive STFT frames.</p> required <code>window_type</code> <code>str</code> <p>The type of window to use. Refer to <code>scipy.signal.get_window</code> for supported types.</p> <code>'hann'</code> <code>detrend</code> <code>Union[str, Callable, Literal[False]]</code> <p>Specifies how to detrend each STFT window. See  <code>scipy.signal.stft</code>  for options. Default is False (no detrending).</p> <code>False</code> <code>padded</code> <code>bool</code> <p>Indicates whether the input signal is zero-padded at the beginning and end before performing the STFT. See <code>scipy.signal.stft</code>. Default is True.</p> <code>True</code> <code>boundary</code> <code>Optional[Literal['zeros', 'odd', 'even', 'constant']]</code> <p>Specifies the boundary extension mode for padding the signal to perform the STFT. See <code>scipy.signal.stft</code>. Default is \"zeros\".</p> <code>'zeros'</code> <code>scale</code> <code>Literal['amplitude', 'power', 'psd']</code> <p>Specifies the scaling of the returned spectrogram values. Default is \"psd\". - \"amplitude\": Returns the magnitude of the STFT components.   Units are typically the same as the input signal (e.g., V). - \"power\": Returns the squared magnitude of the STFT components.   Units are the square of the input signal's units (e.g., V2).   This corresponds to the energy in each time-frequency bin,   normalized for windowing effects but not frequency bin width. - \"psd\": Returns the Power Spectral Density. Units include per   Hertz (e.g., V2/Hz). This scaling accounts for windowing   effects and frequency bin width, representing power density.</p> <code>'psd'</code> <code>sort_dims</code> <code>Union[tuple[str, ...], bool]</code> <p>Controls the final dimension order of the output DataArray. - If <code>True</code>, transpose to <code>DEFAULT_DIM_ORDER</code>   (currently (\"frequency\", \"time\", \"channel\")). - If a tuple of dimension names (e.g., (\"channel\", \"frequency\", \"time\")),   transpose to that specific order. <code>missing_dims=\"ignore\"</code> is used. - If <code>False</code>, do not transpose; return the array with the dimension   order directly resulting from the STFT calculation (see Notes). Default is <code>DEFAULT_DIM_ORDER</code>.</p> <code>DEFAULT_DIM_ORDER</code> <p>Returns:</p> Name Type Description <code>spectrogram</code> <code>DataArray</code> <p>The spectrogram of the audio signal. This is a three-dimensional xarray data array with the dimensions frequency, time, and channel.</p> Notes <p>Time Bin Calculation: *  The time axis of the spectrogram represents the center of each STFT window. *  The first time bin is centered at time t=hop_size / 2. *  Subsequent time bins are spaced by hop_size.</p> <p>Scaling Implementation: * The different <code>scale</code> options leverage <code>scipy.signal.stft</code>'s <code>scaling</code>     parameter and post-processing:     - <code>scale=\"amplitude\"</code> uses <code>stft(scaling='spectrum')</code> and computes <code>np.abs()</code>.     - <code>scale=\"power\"</code> uses <code>stft(scaling='spectrum')</code> and computes <code>np.abs()**2</code>.     - <code>scale=\"psd\"</code> uses <code>stft(scaling='psd')</code> and computes <code>np.abs()**2</code>. * Units mentioned in the <code>scale</code> parameter description assume the input     signal <code>audio</code> has units of Volts (V) for illustrative purposes.</p> <p>Dimension Ordering: * The intermediate dimension order, before applying <code>sort_dims</code> (i.e.,     the order if <code>sort_dims=False</code>), follows the structure where the     original time dimension (<code>axis</code>) is replaced by the frequency dimension,     and the new time segment dimension is appended last. For example, an     input with dims <code>(\"height\", \"time\", \"channel\")</code> results in an     intermediate order of <code>(\"height\", \"frequency\", \"channel\", \"time\")</code>. * By default (<code>sort_dims=True</code>), the output is transposed to match     <code>DEFAULT_DIM_ORDER</code>, which is <code>(\"frequency\", \"time\", \"channel\")</code>.</p>"},{"location":"reference/audio/#soundevent.audio.filter","title":"<code>filter(audio, low_freq=None, high_freq=None, order=5, dim=Dimensions.time.value)</code>","text":"<p>Filter audio data.</p> <p>This function assumes that the input audio object is a :class:<code>xarray.DataArray</code> with a \"samplerate\" attribute and a \"time\" dimension.</p> <p>The filtering is done using a Butterworth filter or the specified order. The type of filter (lowpass/highpass/bandpass filter) is determined by the specified cutoff frequencies. If only one cutoff frequency is specified, a low pass or high pass filter is used. If both cutoff frequencies are specified, a band pass filter is used.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>DataArray</code> <p>The audio data to filter with a \"samplerate\" attribute and a \"time\" dimension.</p> required <code>low_freq</code> <code>float</code> <p>The low cutoff frequency in Hz.</p> <code>None</code> <code>high_freq</code> <code>float</code> <p>The high cutoff frequency in Hz.</p> <code>None</code> <code>order</code> <code>int</code> <p>The order of the filter. By default, 5.</p> <code>5</code> <code>dim</code> <code>str</code> <p>The dimension along which to filter the audio data. By default, \"time\".</p> <code>time.value</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The filtered audio data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither low_freq nor high_freq is specified, or if both are specified and low_freq &gt; high_freq.</p>"},{"location":"reference/audio/#soundevent.audio.get_audio_files","title":"<code>get_audio_files(path, strict=False, recursive=True, follow_symlinks=False)</code>","text":"<p>Return a generator of audio files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the directory.</p> required <code>strict</code> <code>bool</code> <p>Whether to check the file contents to ensure it is an audio file. Will take a bit longer to run, by default False.</p> <code>False</code> <code>recursive</code> <code>bool</code> <p>Whether to search the directory recursively, by default True. This means that all audio files in subdirectories will be included. If False, only the audio files at the top level of the directory will be included.</p> <code>True</code> <code>follow_symlinks</code> <code>bool</code> <p>Whether to follow symbolic links, by default False. Care should be taken when following symbolic links to avoid infinite loops.</p> <code>False</code> <p>Yields:</p> Type Description <code>Path</code> <p>Path to the audio file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path is not a directory.</p> Notes <p>This function uses the <code>is_audio_file</code> function to check if a file is an audio file. See the documentation for <code>is_audio_file</code> for more information on which audio file formats are supported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from soundevent.audio.files import get_audio_files\n</code></pre> <p>Get all audio files in a directory recursively:</p> <pre><code>&gt;&gt;&gt; for file in get_audio_files(\"path/to/directory\"):\n...     print(file)\n</code></pre> <p>Get all audio files in a directory without recursion:</p> <pre><code>&gt;&gt;&gt; for file in get_audio_files(\"path/to/directory\", recursive=False):\n...     print(file)\n</code></pre>"},{"location":"reference/audio/#soundevent.audio.get_media_info","title":"<code>get_media_info(path)</code>","text":"<p>Return the media information from the WAV file.</p> <p>The information extracted from the WAV file is the audio format, the bit depth, the sample rate, the duration, the number of samples, and the number of channels.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the WAV file.</p> required <p>Returns:</p> Name Type Description <code>media_info</code> <code>MediaInfo</code> <p>Information about the WAV file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the WAV file is not PCM encoded.</p>"},{"location":"reference/audio/#soundevent.audio.is_audio_file","title":"<code>is_audio_file(path, strict=False)</code>","text":"<p>Return whether the file is an audio file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the file.</p> required <code>strict</code> <code>bool</code> <p>Whether to check the file contents to ensure it is an audio file. Will take a bit longer to run, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the file is an audio file.</p> Notes <p>The list of supported audio file extensions contains most of the audio files formats supported by the <code>libsndfile</code> library. See: https://libsndfile.github.io/libsndfile/</p> <p>Some formats were excluded as they do not support seeking and thus are not suitable for random access.</p> <p>Supported formats:</p> <ul> <li>aiff</li> <li>au</li> <li>avr</li> <li>caf</li> <li>flac</li> <li>htk</li> <li>ircam</li> <li>mat4</li> <li>mat5</li> <li>mp3</li> <li>mpc2k</li> <li>nist</li> <li>ogg</li> <li>paf</li> <li>pvf</li> <li>rf64</li> <li>sds</li> <li>svx</li> <li>voc</li> <li>w64</li> <li>wav</li> <li>wavex</li> <li>wve</li> </ul>"},{"location":"reference/audio/#soundevent.audio.load_audio","title":"<code>load_audio(path, offset=0, samples=None)</code>","text":"<p>Load an audio file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the audio file.</p> required <code>offset</code> <code>int</code> <p>The offset in samples from the start of the audio file.</p> <code>0</code> <code>samples</code> <code>Optional[int]</code> <p>The number of samples to load. If None, load the entire file.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>The audio data.</p> <code>samplerate</code> <code>int</code> <p>The sample rate of the audio file in Hz.</p>"},{"location":"reference/audio/#soundevent.audio.load_clip","title":"<code>load_clip(clip, audio_dir=None)</code>","text":"<p>Load a clip from a file.</p> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>Clip</code> <p>The clip to load.</p> required <code>audio_dir</code> <code>Optional[PathLike]</code> <p>The directory containing the audio file. If None, the recording path is assumed to be relative to the current working directory or an absolute path.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>audio</code> <code>DataArray</code> <p>The loaded clip. The returned clip stores the samplerate and time expansion of the recording from which it was extracted.</p>"},{"location":"reference/audio/#soundevent.audio.load_recording","title":"<code>load_recording(recording, audio_dir=None)</code>","text":"<p>Load a recording from a file.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>Recording</code> <p>The recording to load.</p> required <code>audio_dir</code> <code>Optional[PathLike]</code> <p>The directory containing the audio file. If None, the recording path is assumed to be relative to the current working directory or an absolute path.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>audio</code> <code>DataArray</code> <p>The loaded recording.</p>"},{"location":"reference/audio/#soundevent.audio.pcen","title":"<code>pcen(array, smooth=0.025, gain=0.98, bias=2, power=0.5, eps=1e-06, dim=Dimensions.time.value)</code>","text":"<p>Apply PCEN to spectrogram.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The spectrogram to which to apply PCEN.</p> required <code>smooth</code> <code>float</code> <p>The time constant for smoothing the input spectrogram. By default, 0.025.</p> <code>0.025</code> <code>gain</code> <code>float</code> <p>The gain factor for the PCEN transform. By default, 0.98.</p> <code>0.98</code> <code>bias</code> <code>float</code> <p>The bias factor for the PCEN transform. By default, 2.</p> <code>2</code> <code>power</code> <code>float</code> <p>The power factor for the PCEN transform. By default, 0.5.</p> <code>0.5</code> <code>eps</code> <code>float</code> <p>An epsilon value to prevent division by zero. By default, 1e-6.</p> <code>1e-06</code> <code>dim</code> <code>str</code> <p>The dimension along which to apply PCEN.</p> <code>time.value</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>Spectrogram with PCEN applied.</p> Notes <p>This function applies the Per-Channel Energy Normalization (PCEN) transform to a spectrogram, as described in [1].</p> <p>The PCEN transform is defined as:</p> \\[ PCEN(X) = \\left(\\frac{X}{(\\epsilon + S)^{\\alpha}} + \\delta\\right)^r - \\delta^r \\] <p>where \\(X\\) is the input spectrogram, \\(S\\) is the smoothed version of the input spectrogram, \\(\\alpha\\) is the power factor, \\(\\delta\\) is the bias factor, and \\(r\\) is the gain factor.</p> <p>The smoothed version of the input spectrogram is computed using a first-order IIR filter:</p> \\[ S_t = (1 - \\beta) S_{t-1} + \\beta X_t \\] <p>where \\(\\beta\\) is the smoothing factor.</p> <p>The default values for the parameters are taken from the PCEN paper [1].</p> References <p>[1] Wang, Y., Getreuer, P., Hughes, T., Lyon, R. F., &amp; Saurous, R. A. (2017, March). Trainable frontend for robust and far-field keyword spotting. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5670-5674). IEEE.</p>"},{"location":"reference/audio/#soundevent.audio.resample","title":"<code>resample(array, target_samplerate, window=None, dim=Dimensions.time.value)</code>","text":"<p>Resample array data to a target sample rate along a given dimension.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>DataArray</code> <p>The data array to resample.</p> required <code>target_samplerate</code> <code>int</code> <p>The target sample rate of the resampled data in Hz.</p> required <code>window</code> <code>Optional[str]</code> <p>The window to use for resampling. See scipy.signal.resample for details.</p> <code>None</code> <code>dim</code> <code>str</code> <p>The dimension along which to resample the audio data. By default, \"time\".</p> <code>time.value</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>The resampled audio data.</p> Notes <p>This function uses scipy.signal.resample to resample the input data array to the target sample rate. This function uses the Fourier method to resample the data, which is suitable for resampling audio data. For other resampling methods, consider using the xarray.DataArray.interp method.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input audio object is not a :class:<code>xarray.DataArray</code>, or if it does not have a \"samplerate\" attribute, or if it does not have a \"time\" dimension.</p>"},{"location":"reference/data/","title":"Data Module","text":""},{"location":"reference/data/#basic_models","title":"Basic Models","text":""},{"location":"reference/data/#soundevent.data","title":"<code>soundevent.data</code>","text":"<p>Data Module.</p> <p>Classes:</p> Name Description <code>User</code> <p>Information about a user.</p> <code>Term</code> <p>A term class for a standardised term.</p> <code>Tag</code> <p>Tag Class.</p> <code>Feature</code> <p>Feature Class.</p> <code>Note</code> <p>Note Class.</p> <code>Recording</code> <p>Represents an audio recording in bioacoustic research.</p> <code>RecordingSet</code> <code>Dataset</code> <code>SoundEvent</code> <p>Sound Event Class.</p> <code>Sequence</code> <p>Sequence Class.</p> <code>Clip</code> <p>Clip Class.</p> <code>SoundEventAnnotation</code> <p>Annotation Class.</p> <code>SequenceAnnotation</code> <p>A class representing the annotations of a sequence.</p> <code>ClipAnnotation</code> <p>Clip Annotations Class.</p> <code>AnnotationState</code> <p>Annotation State Enumeration.</p> <code>StatusBadge</code> <p>Annotation Status Badge Class.</p> <code>AnnotationTask</code> <code>AnnotationSet</code> <p>A collection of annotations for multiple audio clips.</p> <code>AnnotationProject</code> <p>Represents an annotation project.</p> <code>PredictedTag</code> <p>Predicted Tag Class.</p> <code>SoundEventPrediction</code> <p>Predicted Sound Event Class.</p> <code>SequencePrediction</code> <p>A class representing a sequence prediction.</p> <code>ClipPrediction</code> <p>Clip Prediction.</p> <code>PredictionSet</code> <p>A collection of predictions for multiple audio clips.</p> <code>ModelRun</code> <p>Represents the set of predictions generated by a model run.</p> <code>EvaluationSet</code> <p>Evaluation Set Class.</p> <code>Match</code> <p>Match Class.</p> <code>ClipEvaluation</code> <p>Evaluated example model.</p> <code>Evaluation</code> <p>Evaluation Class.</p> <p>Functions:</p> Name Description <code>find_tag</code> <p>Find the first matching tag based on provided criteria.</p> <code>find_tag_value</code> <p>Find the value of the first matching tag.</p> <code>find_feature</code> <p>Find the first matching feature based on a single criterion.</p> <code>find_feature_value</code> <p>Find the value of the first matching feature.</p>"},{"location":"reference/data/#soundevent.data-classes","title":"Classes","text":""},{"location":"reference/data/#soundevent.data.User","title":"<code>User</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about a user.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>Optional[EmailStr]</code> <code>institution</code> <code>Optional[str]</code> <code>model_config</code> <code>name</code> <code>Optional[str]</code> <code>username</code> <code>Optional[str]</code> <code>uuid</code> <code>UUID</code>"},{"location":"reference/data/#soundevent.data.User-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.User.email","title":"<code>email = Field(default=None, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.User.institution","title":"<code>institution = Field(default=None, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.User.model_config","title":"<code>model_config = ConfigDict(from_attributes=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.User.name","title":"<code>name = Field(default=None, repr=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.User.username","title":"<code>username = Field(default=None, repr=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.User.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term","title":"<code>Term</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A term class for a standardised term.</p> <p>Attributes:</p> Name Type Description <code>comment</code> <code>Optional[str]</code> <code>definition</code> <code>str</code> <code>description</code> <code>Optional[str]</code> <code>domain</code> <code>Optional[str]</code> <code>domain_includes</code> <code>Optional[str]</code> <code>equivalent_property</code> <code>Optional[str]</code> <code>instance_of</code> <code>Optional[str]</code> <code>label</code> <code>str</code> <code>member_of</code> <code>Optional[str]</code> <code>model_config</code> <code>name</code> <code>str</code> <code>range_includes</code> <code>Optional[str]</code> <code>scope_note</code> <code>Optional[str]</code> <code>see</code> <code>Optional[str]</code> <code>subclass_of</code> <code>Optional[str]</code> <code>subproperty_of</code> <code>Optional[str]</code> <code>term_range</code> <code>Optional[str]</code> <code>type_of_term</code> <code>str</code> <code>uri</code> <code>Optional[str]</code>"},{"location":"reference/data/#soundevent.data.Term-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Term.comment","title":"<code>comment = Field(serialization_alias='rdfs:comment', default=None, title='Comment', description='Additional information about the term or its application.', repr=False, json_schema_extra={'$id': 'https://www.w3.org/TR/rdf-schema/#ch_comment'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.definition","title":"<code>definition = Field(serialization_alias='skos:definition', title='Definition', description='The type of term: property, class, datatype, or vocabulary encoding scheme.', repr=False, json_schema_extra={'$id': 'http://www.w3.org/2004/02/skos/core#definition'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.description","title":"<code>description = Field(default=None, serialization_alias='dcterms:description', title='Description', description='An account of the resource.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/terms/description'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.domain","title":"<code>domain = Field(default=None, serialization_alias='rdfs:domain', title='Domain', description='A class of which a resource described by the term is an instance.', repr=False, json_schema_extra={'$id': 'https://www.w3.org/TR/rdf-schema/#ch_domain'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.domain_includes","title":"<code>domain_includes = Field(default=None, serialization_alias='dcam:domainIncludes', title='Domain Includes', description='A suggested class for subjects of this property.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/dcam/domainIncludes'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.equivalent_property","title":"<code>equivalent_property = Field(default=None, serialization_alias='owl:equivalentProperty', title='Equivalent Property', description='A property to which the described term is equivalent.', repr=False, json_schema_extra={'$id': 'https://www.w3.org/TR/owl-ref/#equivalentProperty-def'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.instance_of","title":"<code>instance_of = Field(default=None, serialization_alias='instanceOf', title='Instance Of', description='A class of which the described term is an instance.', repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.label","title":"<code>label = Field(serialization_alias='rdfs:label', title='Label', description='The human-readable label assigned to the term.', repr=True, json_schema_extra={'$id': 'https://www.w3.org/TR/rdf-schema/#ch_label'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.member_of","title":"<code>member_of = Field(default=None, serialization_alias='dcam:memberOf', title='Member Of', description='A collection of which the described term is a member.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/dcam/memberOf'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.model_config","title":"<code>model_config = ConfigDict(frozen=True, extra='allow')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.name","title":"<code>name = Field(title='Name', description='A token appended to the URI of a DCMI namespace to create the URI of the term.', repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.range_includes","title":"<code>range_includes = Field(default=None, serialization_alias='dcam:rangeIncludes', title='Range Includes', description='A suggested class for values of this property.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/dcam/rangeIncludes'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.scope_note","title":"<code>scope_note = Field(default=None, serialization_alias='skos:scopeNote', title='Scope Note', description='A note that helps to clarify the meaning and/or the use of a concept.', repr=False, json_schema_extra={'$id': 'https://www.w3.org/2012/09/odrl/semantic/draft/doco/skos_scopeNote'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.see","title":"<code>see = Field(default=None, serialization_alias='rdsf:seeAlso', title='See', description='Authoritative documentation related to the term.', repr=False, json_schema_extra={'$id': 'https://www.dublincore.org/specifications/dublin-core/dcmi-terms/dublin_core_terms.ttl'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.subclass_of","title":"<code>subclass_of = Field(default=None, serialization_alias='rdfs:subclassOf', title='Subclass Of', description='A class of which the described term is a sub-class.', repr=False, json_schema_extra={'$id': 'https://www.w3.org/TR/rdf-schema/#ch_subclassof'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.subproperty_of","title":"<code>subproperty_of = Field(default=None, serialization_alias='rdfs:subpropertyOf', title='Subproperty Of', description='A property of which the described term is a sub-property.', repr=False, json_schema_extra={'$id': 'https://www.w3.org/TR/rdf-schema/#ch_subpropertyof'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.term_range","title":"<code>term_range = Field(default=None, serialization_alias='rdfs:range', alias='range', title='Range', description='A class of which a value described by the term is an instance.', json_schema_extra={'$id': 'https://www.w3.org/TR/rdf-schema/#ch_range'}, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.type_of_term","title":"<code>type_of_term = Field(serialization_alias='dcterms:type', default='property', alias='type', title='Type', description='The nature or genre of the resource.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/terms/type'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Term.uri","title":"<code>uri = Field(serialization_alias='dcterms:URI', default=None, title='URI', repr=False, description='The Uniform Resource Identifier used to uniquely identify a term.', json_schema_extra={'$id': 'http://purl.org/dc/terms/URI'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Tag","title":"<code>Tag(**data)</code>","text":"<pre><code>Tag(*, term: Term, value: str)\n</code></pre><pre><code>Tag(*, key: str, value: str)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Tag Class.</p> <p>Tags annotate and categorize bioacoustic research components. Each tag is a <code>term-value</code> pair. The <code>term</code> provides context and enables categorization.</p> <p>Attributes:</p> Name Type Description <code>term</code> <code>Term</code> <p>The standardized term associated with the tag, providing context and meaning.</p> <code>value</code> <code>str</code> <p>The value associated with the tag, offering specific information.</p> <p>Methods:</p> Name Description <code>handle_key</code>"},{"location":"reference/data/#soundevent.data.Tag-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Tag.key","title":"<code>key</code>  <code>property</code>","text":"<p>Return the key of the tag.</p>"},{"location":"reference/data/#soundevent.data.Tag.term","title":"<code>term = Field(title='Term', description='The standardised term associated with the tag.', repr=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Tag.value","title":"<code>value = Field(title='Value', description='The value associated with the tag.', repr=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Tag-functions","title":"Functions","text":""},{"location":"reference/data/#soundevent.data.Tag.handle_key","title":"<code>handle_key(values)</code>  <code>classmethod</code>","text":""},{"location":"reference/data/#soundevent.data.Feature","title":"<code>Feature(**data)</code>","text":"<pre><code>Feature(*, term: Term, value: float)\n</code></pre><pre><code>Feature(*, name: str, value: float)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Feature Class.</p> <p>Features are numerical values associated with sound events, clips, and recordings, providing additional metadata. These numerical descriptors enable searching, organizing, and analyzing bioacoustic data.</p> <p>Attributes:</p> Name Type Description <code>term</code> <code>Term</code> <p>The standardized term associated with the feature, providing context and meaning.</p> <code>value</code> <code>float</code> <p>The numeric value quantifying the feature, enabling precise comparison and analysis.</p> <p>Methods:</p> Name Description <code>handle_name</code>"},{"location":"reference/data/#soundevent.data.Feature-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Feature.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the name of the feature.</p>"},{"location":"reference/data/#soundevent.data.Feature.term","title":"<code>term</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Feature.value","title":"<code>value</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Feature-functions","title":"Functions","text":""},{"location":"reference/data/#soundevent.data.Feature.handle_name","title":"<code>handle_name(values)</code>  <code>classmethod</code>","text":""},{"location":"reference/data/#soundevent.data.Note","title":"<code>Note</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Note Class.</p> <p>Notes play a pivotal role in the annotation process, providing essential textual context and enabling effective communication among users. Attached to recordings, clips, or sound events, notes serve various purposes, from offering contextual information and explanations to flagging issues and initiating discussions. This collaborative tool enhances the accuracy and depth of annotations while promoting a richer understanding of the audio data.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the note, automatically generated upon creation. This ID distinguishes each note, ensuring a clear reference for annotators and researchers.</p> <code>message</code> <code>str</code> <p>The content of the note, which can include contextual information, explanations, issues, alternative interpretations, or any other relevant details. The message provides valuable insights and explanations related to the annotated material.</p> <code>created_by</code> <code>Optional[User]</code> <p>The identifier of the user who created the note. While optional, including this information enables users to understand the source of the note, fostering transparency and accountability within the annotation process.</p> <code>is_issue</code> <code>bool</code> <p>A flag indicating whether the note highlights an issue or concern. When set to True, the note signals incomplete or incorrect annotations, guiding annotators' attention to specific areas that require further review and refinement.</p> <code>created_on</code> <code>datetime</code> <p>The date and time when the note was created. This timestamp provides a historical record of the note's origin, allowing users to track the timeline of annotations and discussions.</p>"},{"location":"reference/data/#soundevent.data.Note-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Note.created_by","title":"<code>created_by = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Note.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Note.is_issue","title":"<code>is_issue = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Note.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Note.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Recording","title":"<code>Recording</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an audio recording in bioacoustic research.</p> <p>This class models raw, unaltered audio files as captured by recording devices. Recordings are fundamental to bioacoustic research and are identified by various metadata, including:</p> <ul> <li>Unique identifier</li> <li>File path</li> <li>Duration</li> <li>Number of channels</li> <li>Sample rate</li> <li>Time expansion factor</li> <li>Hash</li> <li>Geographic coordinates</li> <li>Tags, features, and notes</li> </ul> Notes <p>For time-expanded audio (slowed down or sped up), the <code>duration</code> and <code>sample_rate</code> reflect the original recording values, not the modified values in the file. This ensures accuracy in representing the audio's true capture conditions.</p> <p>Methods:</p> Name Description <code>from_file</code> <p>Create a recording object from a file.</p> <p>Attributes:</p> Name Type Description <code>channels</code> <code>int</code> <p>The number of channels in the audio file.</p> <code>date</code> <code>Optional[date]</code> <p>The date on which the recording was made. </p> <code>duration</code> <code>float</code> <p>The duration of the audio file in seconds. </p> <code>features</code> <code>List[Feature]</code> <p>A list of features associated with the recording.</p> <code>hash</code> <code>Optional[str]</code> <p>The md5 hash of the audio file. </p> <code>latitude</code> <code>Optional[float]</code> <p>The latitude coordinate of the site of recording. </p> <code>license</code> <code>Optional[str]</code> <code>longitude</code> <code>Optional[float]</code> <code>model_config</code> <code>notes</code> <code>List[Note]</code> <p>A list of notes associated with the recording.</p> <code>owners</code> <code>List[User]</code> <code>path</code> <code>Path</code> <p>The file path to the audio recording.</p> <code>rights</code> <code>Optional[str]</code> <code>samplerate</code> <code>int</code> <p>The sample rate of the audio file in Hz. </p> <code>tags</code> <code>List[Tag]</code> <p>A list of tags associated with the recording.</p> <code>time</code> <code>Optional[time]</code> <p>The time at which the recording was made. </p> <code>time_expansion</code> <code>float</code> <p>The time expansion factor of the audio file.</p> <code>uuid</code> <code>UUID</code> <p>A unique identifier for the recording.</p>"},{"location":"reference/data/#soundevent.data.Recording-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Recording.channels","title":"<code>channels = Field(title='Channels', serialization_alias='mo:channels', description='The number of channels in the audio file.', repr=False, json_schema_extra={'$id': 'http://purl.org/ontology/mo/channels'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The number of channels in the audio file.</p>"},{"location":"reference/data/#soundevent.data.Recording.date","title":"<code>date = Field(default=None, repr=False, deprecated=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The date on which the recording was made. </p> <p>Default is None.</p>"},{"location":"reference/data/#soundevent.data.Recording.duration","title":"<code>duration = Field(title='Duration', serialization_alias='ac:mediaDuration', description='The duration of the audio file in seconds.', repr=False, json_schema_extra={'$id': 'http://rs.tdwg.org/ac/terms/mediaDuration'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The duration of the audio file in seconds. </p> <p>This duration is adjusted by the time expansion factor if the audio file is time expanded, providing the real duration of the recorded audio.</p>"},{"location":"reference/data/#soundevent.data.Recording.features","title":"<code>features = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of features associated with the recording.</p>"},{"location":"reference/data/#soundevent.data.Recording.hash","title":"<code>hash = Field(default=None, serialization_alias='ac:hashValue', title='Hash', description='The value computed by a hash function applied to the media that will be delivered at the access point.', repr=False, json_schema_extra={'$id': 'http://rs.tdwg.org/ac/terms/hashValue'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The md5 hash of the audio file. </p> <p>Default is None.</p>"},{"location":"reference/data/#soundevent.data.Recording.latitude","title":"<code>latitude = Field(default=None, serialization_alias='dwc:decimalLatitude', title='Decimal Latitude', description='The geographic latitude (in decimal degrees, using the spatial reference system given in dwc:geodeticDatum) of the geographic center of a dcterms:Location. Positive values are north of the Equator, negative values are south of it. Legal values lie between -90 and 90, inclusive.', repr=False, json_schema_extra={'$id': 'http://rs.tdwg.org/dwc/terms/decimalLatitude'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The latitude coordinate of the site of recording. </p> <p>Default is None.</p>"},{"location":"reference/data/#soundevent.data.Recording.license","title":"<code>license = Field(default=None, serialization_alias='dcterms:license', title='License', description='A legal document giving official permission to do something with the resource.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/terms/license'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Recording.longitude","title":"<code>longitude = Field(default=None, serialization_alias='dwc:decimalLongitude', title='Decimal Longitude', description='The geographic longitude (in decimal degrees, using the spatial reference system given in dwc:geodeticDatum) of the geographic center of a dcterms:Location. Positive values are east of the Greenwich Meridian, negative values are west of it. Legal values lie between -180 and 180, inclusive.', repr=False, json_schema_extra={'$id': 'http://rs.tdwg.org/dwc/terms/decimalLongitude'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Recording.model_config","title":"<code>model_config = ConfigDict(extra='allow')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Recording.notes","title":"<code>notes = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of notes associated with the recording.</p>"},{"location":"reference/data/#soundevent.data.Recording.owners","title":"<code>owners = Field(default_factory=list, serialization_alias='xmpRights:Owner', title='Copyright Owner', description='A list of legal owners of the resource.', repr=False, json_schema_extra={'$id': 'http://ns.adobe.com/xap/1.0/rights/Owner'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Recording.path","title":"<code>path = Field(title='Path', description='The path to the audio file.', repr=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The file path to the audio recording.</p>"},{"location":"reference/data/#soundevent.data.Recording.rights","title":"<code>rights = Field(default=None, serialization_alias='dcterms:rights', title='Copyright Statement', description='Information about rights held in and over the resource.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/terms/rights'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Recording.samplerate","title":"<code>samplerate = Field(title='Sample Rate', serialization_alias='mo:sample_rate', description='The sample rate of the audio file in Hz.', repr=False, json_schema_extra={'$id': 'http://purl.org/ontology/mo/sample_rate'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sample rate of the audio file in Hz. </p> <p>Similar to duration, the sample rate is adjusted by the time expansion factor if the audio file is time expanded, representing the real sample rate of the recorded audio.</p>"},{"location":"reference/data/#soundevent.data.Recording.tags","title":"<code>tags = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of tags associated with the recording.</p>"},{"location":"reference/data/#soundevent.data.Recording.time","title":"<code>time = Field(default=None, repr=False, deprecated=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The time at which the recording was made. </p> <p>Default is None.</p>"},{"location":"reference/data/#soundevent.data.Recording.time_expansion","title":"<code>time_expansion = Field(default=1.0, serialization_alias='ac:mediaSpeed', title='Media Speed', description='The decimal fraction representing the natural speed over the encoded speed.', repr=False, json_schema_extra={'$id': 'http://rs.tdwg.org/ac/terms/mediaSpeed'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The time expansion factor of the audio file.</p> <p>Default is 1.0, indicating no time expansion.</p>"},{"location":"reference/data/#soundevent.data.Recording.uuid","title":"<code>uuid = Field(default_factory=uuid4, serialization_alias='dcterms:identifier', title='Identifier', description='An unambiguous reference to the resource within a given context.', repr=False, json_schema_extra={'$id': 'http://purl.org/dc/terms/identifier'})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A unique identifier for the recording.</p>"},{"location":"reference/data/#soundevent.data.Recording-functions","title":"Functions","text":""},{"location":"reference/data/#soundevent.data.Recording.from_file","title":"<code>from_file(path, time_expansion=1, compute_hash=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a recording object from a file.</p> <p>This function does not load the audio data itself, but rather extracts the metadata from the file and creates a recording object with the appropriate metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the audio file.</p> required <code>time_expansion</code> <code>float</code> <p>The time expansion factor of the audio file, by default 1.</p> <code>1</code> <code>compute_hash</code> <code>bool</code> <p>Whether to compute the md5 hash of the audio file, by default True. If you are loading a large number of recordings, you might want to set this to False to speed up the loading process.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Recording</code> <p>The recording object.</p>"},{"location":"reference/data/#soundevent.data.RecordingSet","title":"<code>RecordingSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>created_on</code> <code>datetime</code> <code>recordings</code> <code>List[Recording]</code> <code>uuid</code> <code>UUID</code>"},{"location":"reference/data/#soundevent.data.RecordingSet-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.RecordingSet.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.RecordingSet.recordings","title":"<code>recordings = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.RecordingSet.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>RecordingSet</code></p> <p>Methods:</p> Name Description <code>from_directory</code> <p>Return a dataset from the directory.</p> <p>Attributes:</p> Name Type Description <code>description</code> <code>Optional[str]</code> <code>name</code> <code>str</code>"},{"location":"reference/data/#soundevent.data.Dataset-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Dataset.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Dataset.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Dataset-functions","title":"Functions","text":""},{"location":"reference/data/#soundevent.data.Dataset.from_directory","title":"<code>from_directory(path, name, description=None, recursive=True, compute_hash=True)</code>  <code>classmethod</code>","text":"<p>Return a dataset from the directory.</p> <p>Reads the audio files in the directory and returns a dataset containing the recordings.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the directory.</p> required <code>recursive</code> <code>bool</code> <p>Whether to search the directory recursively, by default True</p> <code>True</code> <code>compute_hash</code> <code>bool</code> <p>Whether to compute the hash of the audio files, by default</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path is not a directory.</p>"},{"location":"reference/data/#soundevent.data.SoundEvent","title":"<code>SoundEvent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Sound Event Class.</p> <p>Represents a specific sound event detected within a recording. Each sound event is characterized by a unique identifier, the recording in which it occurs, its spatial geometry (if available), associated tags, and features. Sound events are fundamental entities used for studying and categorizing various acoustic phenomena within audio recordings.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier (UUID) for the sound event.</p> <code>geometry</code> <code>Optional[Geometry]</code> <p>The spatial geometry locating the sound event within the recording. Can include information about the event's position, duration, and frequency range.</p> <code>features</code> <code>List[Feature]</code> <p>A list of features associated with the sound event, offering quantitative information about its acoustic properties.</p>"},{"location":"reference/data/#soundevent.data.SoundEvent-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.SoundEvent.features","title":"<code>features = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEvent.geometry","title":"<code>geometry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEvent.recording","title":"<code>recording</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEvent.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Sequence","title":"<code>Sequence</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Sequence Class.</p> <p>Represents a sequence of sound events in bioacoustic research. A sequence is a ordered collection of sound events, each characterized by its unique identifier, associated tags, features, and notes. Sequences can be hierarchical, allowing the organization of sub-sequences under parent sequences.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the sequence.</p> <code>sound_events</code> <code>List[SoundEvent]</code> <p>A list of sound events within the sequence.</p> <code>features</code> <code>List[Feature]</code> <p>A list of features associated with the sequence, offering quantitative information about the sequence's acoustic characteristics.</p> <code>parent</code> <code>Optional[Sequence]</code> <p>If the sequence is a subsequence, this attribute refers to the parent sequence under which the current sequence is organized.</p>"},{"location":"reference/data/#soundevent.data.Sequence-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Sequence.features","title":"<code>features = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Sequence.parent","title":"<code>parent = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Sequence.sound_events","title":"<code>sound_events = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Sequence.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Clip","title":"<code>Clip</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Clip Class.</p> <p>The <code>Clip</code> class represents a specific segment of an audio recording within the context of bioacoustic research. Clips provide isolated and manageable portions of audio data, essential for analysis, annotation, and machine learning tasks.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>(UUID, optional)</code> <p>The unique identifier of the clip, automatically generated upon creation. This identifier distinguishes the clip from others and is crucial for referencing and management purposes.</p> <code>recording</code> <code>Recording</code> <p>An instance of the <code>Recording</code> class representing the larger audio recording that the clip belongs to. Clips are extracted from recordings and serve as individual units for analysis. The recording provides essential context for understanding the origin and source of the audio data.</p> <code>start_time</code> <code>float</code> <p>The start time of the clip in seconds, indicating the beginning point of the segment within the recording's timeline. Start time is essential for accurate temporal positioning and alignment of the clip within the context of the original recording.</p> <code>end_time</code> <code>float</code> <p>The end time of the clip in seconds, representing the conclusion of the segment within the recording's timeline. End time provides clear boundaries for the duration of the clip, aiding in precise temporal delineation and analysis of the audio content.</p> <code>features</code> <code>(List[Feature], optional)</code> <p>A list of <code>Feature</code> instances representing computed features or descriptors associated with the clip. Features provide quantitative and qualitative insights into the audio content, allowing for advanced analysis and machine learning applications. These features serve as valuable inputs for algorithms and models, enhancing the depth of analysis and interpretation.</p>"},{"location":"reference/data/#soundevent.data.Clip-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Clip.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Return the duration of the clip.</p>"},{"location":"reference/data/#soundevent.data.Clip.end_time","title":"<code>end_time</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Clip.features","title":"<code>features = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Clip.recording","title":"<code>recording</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Clip.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Clip.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation","title":"<code>SoundEventAnnotation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Annotation Class.</p> <p>The <code>Annotation</code> class encapsulates essential information about a specific annotation created within a bioacoustic research project. Annotations provide detailed labeling for individual sound events, enhancing the interpretability and utility of audio data.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the annotation, automatically generated upon creation. This identifier distinguishes the annotation from others and is crucial for referencing and management purposes.</p> <code>created_by</code> <code>Optional[User]</code> <p>The user who created the annotation, providing insights into the annotator's identity. This information is valuable for tracking and accountability within annotation projects.</p> <code>sound_event</code> <code>SoundEvent</code> <p>An instance of the <code>SoundEvent</code> class representing the specific sound event being annotated. Sound events define distinct audio occurrences, such as bird calls or animal vocalizations, and are essential for categorizing the content of the audio data.</p> Notes <pre><code>A list of `Note` instances representing additional contextual information or\nremarks associated with the annotation. Notes can provide insights into specific\ncharacteristics of the sound event, aiding in the comprehensive understanding\nof the annotated data.\n</code></pre> <p>tags     A list of <code>Tag</code> instances representing user-provided tags associated with the     annotated sound event. These tags offer additional semantic context to the     annotation, enabling detailed classification and facilitating targeted analysis     of the acoustic content. created_on     The timestamp indicating the time at which the annotation was created. This     information is essential for tracking the progress of an annotation task and     understanding the chronological order of annotations within a project.</p>"},{"location":"reference/data/#soundevent.data.SoundEventAnnotation-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation.created_by","title":"<code>created_by = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation.notes","title":"<code>notes = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation.sound_event","title":"<code>sound_event</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation.tags","title":"<code>tags = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventAnnotation.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation","title":"<code>SequenceAnnotation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class representing the annotations of a sequence.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the annotation.</p> <code>sequence</code> <code>Sequence</code> <p>The sequence being annotated.</p> Notes <pre><code>A list of notes associated with the sequence.\n</code></pre> <p>tags     The tags attached to the sequence providing semantic information. created_on     The date and time the annotation was created. created_by     The user who created the annotation.</p>"},{"location":"reference/data/#soundevent.data.SequenceAnnotation-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation.created_by","title":"<code>created_by = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation.notes","title":"<code>notes = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation.sequence","title":"<code>sequence</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation.tags","title":"<code>tags = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequenceAnnotation.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation","title":"<code>ClipAnnotation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Clip Annotations Class.</p> <p>Represents annotations associated with a specific audio clip.</p> <p>ClipAnnotations encapsulates details about the audio clip, associated tags, annotations of sound events, status badges indicating the state of the annotation process, and any additional notes related to the annotations.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier automatically generated for the clip annotations.</p> <code>clip</code> <code>Clip</code> <p>The Clip instance representing the audio clip associated with these annotations.</p> <code>tags</code> <code>List[Tag]</code> <p>A list of Tag instances defining the set of tags associated with the clip.</p> <code>annotations</code> <p>A list of Annotation instances representing detailed annotations of sound events in the clip.</p> Notes <pre><code>A list of Note instances representing additional contextual\ninformation or remarks associated with the clip.\n</code></pre>"},{"location":"reference/data/#soundevent.data.ClipAnnotation-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.clip","title":"<code>clip</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.notes","title":"<code>notes = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.sequences","title":"<code>sequences = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.sound_events","title":"<code>sound_events = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.tags","title":"<code>tags = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipAnnotation.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationState","title":"<code>AnnotationState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Annotation State Enumeration.</p> <p>Enumeration representing the different states an audio clip can be in during the annotation process. The <code>AnnotationState</code> enum provides clear insights into the progress of the annotation process, helping users understand the current stage of the annotation of the clip.</p> <p>Attributes:</p> Name Type Description <code>assigned</code> <p>The clip has been assigned to an annotator and is awaiting completion.</p> <code>completed</code> <p>The clip has been fully annotated and is awaiting review.</p> <code>verified</code> <p>The clip has been reviewed and accepted by the reviewer, indicating that the annotation is complete and accurate.</p> <code>rejected</code> <p>The clip annotations have been reviewed and rejected by the reviewer, indicating that the annotation is incomplete or inaccurate.</p>"},{"location":"reference/data/#soundevent.data.AnnotationState-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.AnnotationState.assigned","title":"<code>assigned = 'assigned'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationState.completed","title":"<code>completed = 'completed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationState.rejected","title":"<code>rejected = 'rejected'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationState.verified","title":"<code>verified = 'verified'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.StatusBadge","title":"<code>StatusBadge</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Annotation Status Badge Class.</p> <p>Represents an indicator of the current state of annotation for a clip.</p> <p>The StatusBadge class includes information such as the task state, the responsible user, and the creation timestamp, providing insights into the clip's annotation progress.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>AnnotationState</code> <p>The AnntoationState enum indicating the current state of the annotation of the clip.</p> <code>owner</code> <code>Optional[User]</code> <p>Optional field specifying the user responsible for this status badge.</p> <code>created_on</code> <code>datetime</code> <p>The date and time when the status badge was created, providing a historical record of the badge's creation time.</p>"},{"location":"reference/data/#soundevent.data.StatusBadge-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.StatusBadge.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.StatusBadge.owner","title":"<code>owner = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.StatusBadge.state","title":"<code>state</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationTask","title":"<code>AnnotationTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>clip</code> <code>Clip</code> <code>created_on</code> <code>datetime</code> <code>status_badges</code> <code>List[StatusBadge]</code> <code>uuid</code> <code>UUID</code>"},{"location":"reference/data/#soundevent.data.AnnotationTask-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.AnnotationTask.clip","title":"<code>clip</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationTask.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationTask.status_badges","title":"<code>status_badges = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationTask.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationSet","title":"<code>AnnotationSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of annotations for multiple audio clips.</p> <p>The <code>AnnotationSet</code> class groups together <code>ClipAnnotation</code> objects, representing human-provided annotations related to a set of audio clips.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the annotation set, automatically generated.</p> <code>clip_annotations</code> <code>List[ClipAnnotation]</code> <p>A list of <code>ClipAnnotation</code> objects, where each object contains the annotations associated with a specific audio clip within this set.</p> <code>created_on</code> <code>datetime</code> <p>The date and time when the <code>AnnotationSet</code> object was created.</p> <code>name</code> <code>Optional[str]</code> <p>An optional name for the annotation set (e.g., \"Expert Annotator Set 1\").</p> <code>description</code> <code>Optional[str]</code> <p>An optional detailed description of the annotation set, providing context about its origin, purpose, or the annotation guidelines used.</p>"},{"location":"reference/data/#soundevent.data.AnnotationSet-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.AnnotationSet.clip_annotations","title":"<code>clip_annotations = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationSet.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationSet.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationSet.name","title":"<code>name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationSet.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationProject","title":"<code>AnnotationProject</code>","text":"<p>               Bases: <code>AnnotationSet</code></p> <p>Represents an annotation project.</p> <p>An <code>AnnotationProject</code> extends an <code>AnnotationSet</code> by adding project-specific details necessary for organizing and carrying out an annotation task. It includes the set of clips to be annotated (defined within <code>AnnotationTask</code> objects), instructions for the annotators, and the official list of tags that can be used within the project.</p> <p>It inherits from <code>AnnotationSet</code> and adds structure for tracking the annotation process itself.</p> <p>Attributes:</p> Name Type Description <code>instructions</code> <code>Optional[str]</code> <p>Optional textual instructions for annotators working on this project.</p> <code>annotation_tags</code> <code>List[Tag]</code> <p>A list of <code>Tag</code> objects that are permitted for use in annotations within this project.</p> <code>tasks</code> <code>List[AnnotationTask]</code> <p>A list of <code>AnnotationTask</code> objects, each defining a specific clip that requires annotation as part of this project.</p>"},{"location":"reference/data/#soundevent.data.AnnotationProject-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.AnnotationProject.annotation_tags","title":"<code>annotation_tags = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationProject.instructions","title":"<code>instructions = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.AnnotationProject.tasks","title":"<code>tasks = Field(default_factory=list, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictedTag","title":"<code>PredictedTag</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Predicted Tag Class.</p> <p>Predicted tags are categorical descriptions generated by machine learning-based methods to represent processed sound clips. These descriptions include probability scores, reflecting the confidence of the tag assignment. Predicted tags serve as essential elements in audio analysis, providing insights into the characteristics of sound events.</p> <p>Attributes:</p> Name Type Description <code>tag</code> <code>Tag</code> <p>The predicted tag representing the categorical description of the sound event.</p> <code>score</code> <code>float</code> <p>A probability score ranging from 0 to 1, indicating the confidence level of the tag assignment. A score of 1 signifies a high level of certainty in the assigned tag. When the audio analysis method does not provide a score, the default value is set to 1.</p>"},{"location":"reference/data/#soundevent.data.PredictedTag-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.PredictedTag.score","title":"<code>score = Field(default=1, ge=0, le=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictedTag.tag","title":"<code>tag</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventPrediction","title":"<code>SoundEventPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Predicted Sound Event Class.</p> <p>Predicted sound events represent the outcomes of automated methods and machine learning models employed to identify sound events within audio clips. These predictions provide valuable insights into the presence and characteristics of sound events, supporting comprehensive audio analysis.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the prediction, automatically generated upon creation. This identifier distinguishes each prediction, facilitating reference and management.</p> <code>sound_event</code> <code>SoundEvent</code> <p>The predicted sound event captured by the automated method or machine learning model. This encapsulates details such as the sound event's temporal properties, and other essential characteristics.</p> <code>score</code> <code>float</code> <p>A probability score indicating the confidence of the prediction. This score reflects the certainty of the event's identification within the clip. Researchers leverage these scores to assess the reliability and accuracy of the predictions, enabling further analysis and evaluation.</p> <code>tags</code> <code>List[PredictedTag]</code> <p>A list of predicted tags associated with the sound event. Predicted tags provide semantic labels that offer insights into the nature and characteristics of the event. Each tag is assigned its own probability score, indicating the model's confidence in the relevance of the tag to the event. These scores assist researchers in understanding the significance and reliability of the predicted tags.</p>"},{"location":"reference/data/#soundevent.data.SoundEventPrediction-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.SoundEventPrediction.score","title":"<code>score = Field(default=1, ge=0, le=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventPrediction.sound_event","title":"<code>sound_event</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventPrediction.tags","title":"<code>tags = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SoundEventPrediction.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequencePrediction","title":"<code>SequencePrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class representing a sequence prediction.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the prediction.</p> <code>sequence</code> <code>Sequence</code> <p>The sequence being predicted.</p> <code>score</code> <code>float</code> <p>A score between 0 and 1 indicating the confidence in the prediction.</p> <code>tags</code> <code>List[PredictedTag]</code> <p>List of tags attached to the sequence providing semantic information.</p>"},{"location":"reference/data/#soundevent.data.SequencePrediction-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.SequencePrediction.score","title":"<code>score = Field(default=1, ge=0, le=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequencePrediction.sequence","title":"<code>sequence</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequencePrediction.tags","title":"<code>tags = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.SequencePrediction.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction","title":"<code>ClipPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Clip Prediction.</p> <p>Clip prediction encapsulate the outcomes of various processing steps applied to recording clips in bioacoustic research. These processing steps can include sound event detection, tag generation, and acoustic feature extraction.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the processed clip.</p> <code>clip</code> <code>Clip</code> <p>The original clip that was processed, serving as the basis for the analysis.</p> <code>sound_events</code> <code>Sequence[SoundEventPrediction]</code> <p>A list of predicted sound events detected within the clip. Each predicted sound event contains information about its characteristics, including temporal and frequency properties.</p> <code>tags</code> <code>Sequence[PredictedTag]</code> <p>A list of predicted tags generated at the clip level. These tags provide high-level semantic information about the clip's content, aiding in organization and categorization.</p> <code>features</code> <code>Sequence[Feature]</code> <p>A list of acoustic features extracted from the clip. These features offer numerical representations describing properties such as signal-to-noise ratio, spectral centroid, or other acoustic characteristics. They enhance the understanding of the clip's acoustic content.</p>"},{"location":"reference/data/#soundevent.data.ClipPrediction-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction.clip","title":"<code>clip</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction.features","title":"<code>features = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction.sequences","title":"<code>sequences = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction.sound_events","title":"<code>sound_events = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction.tags","title":"<code>tags = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipPrediction.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictionSet","title":"<code>PredictionSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of predictions for multiple audio clips.</p> <p>The <code>PredictionSet</code> class groups together <code>ClipPrediction</code> objects, typically representing the output of an analysis process (like a model inference run) applied to a set of audio clips. This allows managing these predictions as a single unit.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>UUID</code> <p>A unique identifier for the prediction set, automatically generated.</p> <code>clip_predictions</code> <code>List[ClipPrediction]</code> <p>A list of <code>ClipPrediction</code> objects, where each object contains the predictions associated with a specific audio clip within this set.</p> <code>name</code> <code>Optional[str]</code> <p>An optional name for the prediction set (e.g., \"Model V2 - Test Set\").</p> <code>description</code> <code>Optional[str]</code> <p>An optional detailed description of the prediction set, providing context about its origin or purpose.</p> <code>created_on</code> <code>datetime</code> <p>The date and time when the <code>PredictionSet</code> object was created.</p>"},{"location":"reference/data/#soundevent.data.PredictionSet-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.PredictionSet.clip_predictions","title":"<code>clip_predictions = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictionSet.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictionSet.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictionSet.name","title":"<code>name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.PredictionSet.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ModelRun","title":"<code>ModelRun</code>","text":"<p>               Bases: <code>PredictionSet</code></p> <p>Represents the set of predictions generated by a model run.</p> <p>A model run encapsulates the output of applying a specific machine learning model to a collection of audio clips. It inherits prediction data management from <code>PredictionSet</code> and adds information about the model used.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The specific model version used to generate the predictions in this run.</p>"},{"location":"reference/data/#soundevent.data.ModelRun-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.ModelRun.model","title":"<code>model</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ModelRun.version","title":"<code>version</code>  <code>property</code> <code>writable</code>","text":"<p>Provides backward-compatible access to 'version'. Deprecated.</p>"},{"location":"reference/data/#soundevent.data.EvaluationSet","title":"<code>EvaluationSet</code>","text":"<p>               Bases: <code>AnnotationSet</code></p> <p>Evaluation Set Class.</p> <p>Attributes:</p> Name Type Description <code>evaluation_tags</code> <code>Sequence[Tag]</code>"},{"location":"reference/data/#soundevent.data.EvaluationSet-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.EvaluationSet.evaluation_tags","title":"<code>evaluation_tags = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Match","title":"<code>Match</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Match Class.</p> <p>The <code>Match</code> class represents the outcome of the matching process in bioacoustic research. During this process, sound events from a source and a target are compared based on a similarity metric. The matched sound events are paired, and the Match object captures these pairs along with their affinity scores, indicating the level of similarity between the matched events.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>Optional[SoundEventPrediction]</code> <p>The predicted sound event that was matched from the source model run.</p> <code>target</code> <code>Optional[SoundEventAnnotation]</code> <p>The annotation that was matched from the target evaluation set.</p> <code>affinity</code> <code>float</code> <p>The affinity score quantifying the degree of geometric similarity between the matched source and target sound events. Affinity scores range from 0.0 (no similarity) to 1.0 (perfect match). Researchers can use this score to evaluate the strength and quality of the match.</p> <code>score</code> <code>Optional[float]</code> <p>The score of the matched sound event. This score is typically used to evaluate the quality of the matched sound event. For example, the score could represent the probability of the sound event, or the confidence of the model in predicting the sound event.</p> <code>metrics</code> <code>Sequence[Feature]</code> <p>A list of metrics that were computed for the matched sound event. These metrics can be used to evaluate the quality of the matched sound event, in addition to the affinity and score.</p>"},{"location":"reference/data/#soundevent.data.Match-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Match.affinity","title":"<code>affinity = Field(default=0.0, ge=0.0, le=1.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Match.metrics","title":"<code>metrics = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Match.score","title":"<code>score = Field(default=None, ge=0.0, le=1.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Match.source","title":"<code>source = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Match.target","title":"<code>target = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Match.uuid","title":"<code>uuid = Field(default_factory=uuid4)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation","title":"<code>ClipEvaluation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluated example model.</p> <p>Attributes:</p> Name Type Description <code>annotations</code> <code>ClipAnnotation</code> <p>An instance of the <code>ClipAnnotations</code> class representing the original annotations for the evaluated example. This object contains the audio clip, ground truth annotations, and other relevant information necessary for evaluation.</p> <code>prediction</code> <p>An instance of the <code>ProcessedClip</code> class representing the model's predictions for the given example. This processed clip encapsulates the model's annotations and predictions, providing a standardized format for comparison and analysis.</p> <code>matches</code> <code>Sequence[Match]</code> <p>A list of <code>Match</code> instances representing the matches between the model's predictions and the ground truth annotations. Each <code>Match</code> object contains information about the predicted and actual annotations that align, allowing for detailed analysis of correct predictions and potential errors.</p> <code>metrics</code> <code>Sequence[Feature]</code> <p>A list of <code>Feature</code> instances representing computed evaluation metrics for the evaluated example. These metrics quantify the model's performance on this specific instance, offering numerical insights into accuracy, precision, recall, and other relevant evaluation criteria.</p> <code>score</code> <code>Optional[float]</code> <p>A float representing the overall score for the evaluated example. This score is usually some combination of the evaluation metrics, providing a single value that can be used to compare different models and evaluation examples. Can be used to rank predictions based on their overall score</p>"},{"location":"reference/data/#soundevent.data.ClipEvaluation-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation.annotations","title":"<code>annotations</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation.matches","title":"<code>matches = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation.metrics","title":"<code>metrics = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation.predictions","title":"<code>predictions</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation.score","title":"<code>score = Field(default=None, ge=0.0, le=1.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.ClipEvaluation.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Evaluation","title":"<code>Evaluation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluation Class.</p> <p>Attributes:</p> Name Type Description <code>clip_evaluations</code> <code>Sequence[ClipEvaluation]</code> <code>created_on</code> <code>datetime</code> <code>evaluation_task</code> <code>str</code> <code>metrics</code> <code>Sequence[Feature]</code> <code>score</code> <code>Optional[float]</code> <code>uuid</code> <code>UUID</code>"},{"location":"reference/data/#soundevent.data.Evaluation-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.Evaluation.clip_evaluations","title":"<code>clip_evaluations = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Evaluation.created_on","title":"<code>created_on = Field(default_factory=datetime.datetime.now)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Evaluation.evaluation_task","title":"<code>evaluation_task</code>  <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Evaluation.metrics","title":"<code>metrics = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Evaluation.score","title":"<code>score = Field(default=None, alias='score')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.Evaluation.uuid","title":"<code>uuid = Field(default_factory=uuid4, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data-functions","title":"Functions","text":""},{"location":"reference/data/#soundevent.data.find_tag","title":"<code>find_tag(tags, term=None, term_name=None, term_label=None, label=None, key=None, default=None, raises=False)</code>","text":"<p>Find the first matching tag based on provided criteria.</p> <p>Searches an iterable of Tag objects and returns the first one that matches the single specified search criterion. Users must provide exactly one search method: tag key, Term object, Term name, or Term label.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Iterable[Tag]</code> <p>An iterable of Tag objects to search within.</p> required <code>term</code> <code>Optional[Term]</code> <p>The Term object associated with the tag to search for.</p> <code>None</code> <code>term_name</code> <code>Optional[str]</code> <p>The name of the Term associated with the tag to search for.</p> <code>None</code> <code>term_label</code> <code>Optional[str]</code> <p>The label of the Term associated with the tag to search for.</p> <code>None</code> <code>key</code> <code>Optional[str]</code> <p>The key of the tag to search for.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>(Deprecated) The label of the term to search for. Use <code>term_label</code> instead.</p> <code>None</code> <code>default</code> <code>Optional[Tag]</code> <p>A default Tag object to return if no matching tag is found. Defaults to None.</p> <code>None</code> <code>raises</code> <code>bool</code> <p>If True, raises a ValueError if no matching tag is found and no default is provided. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Tag]</code> <p>The first matching Tag object found, or the <code>default</code> value if no match is found (and <code>raises</code> is False). Returns None if no match is found and no <code>default</code> is provided.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If none of <code>key</code>, <code>term</code>, <code>term_name</code>, or <code>term_label</code> are provided.</p> <code>ValueError</code> <p>If more than one of <code>key</code>, <code>term</code>, <code>term_name</code>, or <code>term_label</code> are provided.</p> <code>ValueError</code> <p>If <code>raises</code> is True and no matching tag is found.</p> Notes <ul> <li>Only one search criterion (<code>key</code>, <code>term</code>, <code>term_name</code>, or   <code>term_label</code>) can be provided per call.</li> <li>If multiple tags match the chosen criterion, the first one   encountered in the <code>tags</code> iterable is returned.</li> <li>The <code>label</code> parameter is deprecated and will be removed in a future   version. Please use <code>term_label</code>. Using <code>label</code> counts as using   <code>term_label</code> regarding the one-criterion rule.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t1 = Term(\n...     name=\"instrument\",\n...     label=\"Instrument Type\",\n...     definition=\"Type of musical instrument.\",\n... )\n&gt;&gt;&gt; t2 = Term(\n...     name=\"scene\",\n...     label=\"Acoustic Scene\",\n...     definition=\"Class of acoustic scene as defined by the AudioSet ontology\",\n... )\n&gt;&gt;&gt; tag1 = Tag(term=t1, value=\"guitar\")\n&gt;&gt;&gt; tag2 = Tag(term=t2, value=\"park\")\n&gt;&gt;&gt; tag_list = [tag1, tag2]\n&gt;&gt;&gt; # Find by term name\n&gt;&gt;&gt; find_tag(tag_list, term_name=\"scene\") is tag2\nTrue\n&gt;&gt;&gt; # Find by term label\n&gt;&gt;&gt; find_tag(tag_list, term_label=\"Instrument Type\") is tag1\nTrue\n&gt;&gt;&gt; # Find by key (uses the term name)\n&gt;&gt;&gt; find_tag(tag_list, key=\"instrument\") is tag1\nTrue\n&gt;&gt;&gt; # No match, return default\n&gt;&gt;&gt; find_tag(tag_list, term_name=\"weather\", default=tag1) is tag1\nTrue\n&gt;&gt;&gt; # No match, return None\n&gt;&gt;&gt; find_tag(tag_list, term_name=\"weather\") is None\nTrue\n</code></pre>"},{"location":"reference/data/#soundevent.data.find_tag_value","title":"<code>find_tag_value(tags, key=None, term=None, term_name=None, term_label=None, default=None, raises=False)</code>","text":"<p>Find the value of the first matching tag.</p> <p>Searches an iterable of Tag objects using <code>find_tag</code> and returns the <code>value</code> attribute of the first matching tag found.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Iterable[Tag]</code> <p>An iterable of Tag objects to search within.</p> required <code>key</code> <code>Optional[str]</code> <p>The key of the tag to search for.</p> <code>None</code> <code>term</code> <code>Optional[Term]</code> <p>The Term object associated with the tag to search for.</p> <code>None</code> <code>term_name</code> <code>Optional[str]</code> <p>The name of the Term associated with the tag to search for.</p> <code>None</code> <code>term_label</code> <code>Optional[str]</code> <p>The label of the Term associated with the tag to search for.</p> <code>None</code> <code>default</code> <code>Optional[str]</code> <p>A default string value to return if no matching tag is found. Defaults to None.</p> <code>None</code> <code>raises</code> <code>bool</code> <p>If True, raises a ValueError if no matching tag is found and no default is provided. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The string <code>value</code> of the first matching Tag object, or the <code>default</code> value if no match is found (and <code>raises</code> is False). Returns None if no match is found and no <code>default</code> is provided.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>raises</code> is True and no matching tag is found. Also raised if no search criteria are provided (via <code>find_tag</code>).</p> See Also <p>find_tag : The underlying function used to find     the Tag object itself.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t1 = Term(\n...     name=\"instrument\",\n...     label=\"Instrument Type\",\n...     definition=\"Type of musical instrument.\",\n... )\n&gt;&gt;&gt; t2 = Term(\n...     name=\"tau2019:scene\",\n...     label=\"Acoustic Scene\",\n...     definition=\"Type of acoustic scene according to TAU Urban Acoustic Scenes dataset.\",\n... )\n&gt;&gt;&gt; tag1 = Tag(term=t1, value=\"guitar\")\n&gt;&gt;&gt; tag2 = Tag(term=t2, value=\"park\")\n&gt;&gt;&gt; tag_list = [tag1, tag2]\n&gt;&gt;&gt; # Find value by term name\n&gt;&gt;&gt; find_tag_value(tag_list, term_name=\"tau2019:scene\")\n'park'\n&gt;&gt;&gt; # Find value by key\n&gt;&gt;&gt; find_tag_value(tag_list, key=\"instrument\")\n'guitar'\n&gt;&gt;&gt; # Find value by term label\n&gt;&gt;&gt; find_tag_value(tag_list, term_label=\"Acoustic Scene\")\n'park'\n&gt;&gt;&gt; # No match, return default\n&gt;&gt;&gt; find_tag_value(tag_list, term_name=\"weather\", default=\"unknown\")\n'unknown'\n</code></pre>"},{"location":"reference/data/#soundevent.data.find_feature","title":"<code>find_feature(features, term=None, term_name=None, term_label=None, name=None, label=None, default=None, raises=False)</code>","text":"<p>Find the first matching feature based on a single criterion.</p> <p>Searches an iterable of Feature objects and returns the first one that matches the single specified search criterion. Users must provide exactly one search method: feature name, Term object, Term name, or Term label.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Sequence[Feature]</code> <p>A sequence of Feature objects to search within.</p> required <code>term</code> <code>Optional[Term]</code> <p>The Term object associated with the feature to search for.</p> <code>None</code> <code>term_name</code> <code>Optional[str]</code> <p>The name of the Term associated with the feature to search for.</p> <code>None</code> <code>term_label</code> <code>Optional[str]</code> <p>The label of the Term associated with the feature to search for.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>The name of the feature to search for.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>(Deprecated) The label of the term to search for. Use <code>term_label</code> instead. If used, it counts as providing <code>term_label</code>.</p> <code>None</code> <code>default</code> <code>Optional[Feature]</code> <p>A default Feature object to return if no matching feature is found. Defaults to None.</p> <code>None</code> <code>raises</code> <code>bool</code> <p>If True, raises a ValueError if no matching feature is found and no default is provided. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Feature]</code> <p>The first matching Feature object found, or the <code>default</code> value if no match is found (and <code>raises</code> is False). Returns None if no match is found and no <code>default</code> is provided.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If none of <code>name</code>, <code>term</code>, <code>term_name</code>, or <code>term_label</code> are provided.</p> <code>ValueError</code> <p>If more than one of <code>name</code>, <code>term</code>, <code>term_name</code>, or <code>term_label</code> are provided.</p> <code>ValueError</code> <p>If <code>raises</code> is True and no matching feature is found.</p> Notes <ul> <li>Only one search criterion (<code>name</code>, <code>term</code>, <code>term_name</code>, or   <code>term_label</code>) can be provided per call.</li> <li>If multiple features match the chosen criterion, the first one   encountered in the <code>features</code> sequence is returned.</li> <li>The <code>label</code> parameter is deprecated. Use <code>term_label</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t_energy = Term(\n...     name=\"energy\",\n...     label=\"Signal Energy\",\n...     definition=\"Total energy (J) contained in the audio signal\",\n... )\n&gt;&gt;&gt; t_pitch = Term(\n...     name=\"pitch\",\n...     label=\"Fundamental Frequency\",\n...     definition=\"Frequency that contains the highest concentration of energy.\",\n... )\n&gt;&gt;&gt; feat1 = Feature(value=0.85, term=t_energy)\n&gt;&gt;&gt; feat2 = Feature(value=440.0, term=t_pitch)\n&gt;&gt;&gt; feat_list = [feat1, feat2]\n&gt;&gt;&gt; # Find by feature name (defaults to term label if does not exist)\n&gt;&gt;&gt; find_feature(feat_list, name=\"pitch\") is feat2\nTrue\n&gt;&gt;&gt; # Find by term name\n&gt;&gt;&gt; find_feature(feat_list, term_name=\"energy\") is feat1\nTrue\n&gt;&gt;&gt; # Find by term label\n&gt;&gt;&gt; find_feature(feat_list, term_label=\"Fundamental Frequency\") is feat2\nTrue\n&gt;&gt;&gt; # No match, return default\n&gt;&gt;&gt; find_feature(feat_list, name=\"zcr\", default=feat1) is feat1\nTrue\n&gt;&gt;&gt; # No match, raises error\n&gt;&gt;&gt; try:\n...     find_feature(feat_list, name=\"zcr\", raises=True)\n... except ValueError as e:\n...     print(e)\nNo feature found matching the criteria.\n</code></pre>"},{"location":"reference/data/#soundevent.data.find_feature_value","title":"<code>find_feature_value(features, term=None, term_name=None, term_label=None, name=None, default=None, raises=False)</code>","text":"<p>Find the value of the first matching feature.</p> <p>Searches a sequence of Feature objects using <code>find_feature</code> and returns the float <code>value</code> attribute of the first matching feature found.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Sequence[Feature]</code> <p>A sequence of Feature objects to search within.</p> required <code>term</code> <code>Optional[Term]</code> <p>The Term object to search for.</p> <code>None</code> <code>term_name</code> <code>Optional[str]</code> <p>The name of the Term to search for.</p> <code>None</code> <code>term_label</code> <code>Optional[str]</code> <p>The label of the Term to search for.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>The name of the feature to search for.</p> <code>None</code> <code>default</code> <code>Optional[float]</code> <p>A default float value to return if no matching feature is found. Defaults to None.</p> <code>None</code> <code>raises</code> <code>bool</code> <p>If True, raises a ValueError if no matching feature is found and no default is provided. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The float <code>value</code> of the matching Feature object, or the <code>default</code> value if no match is found (and <code>raises</code> is False). Returns None if no match is found and no <code>default</code> is provided.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>raises</code> is True and no feature is found, or if multiple search criteria are provided (via <code>find_feature</code>).</p> See Also <p>find_feature : The underlying function used     to find the Feature object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t_energy = Term(\n...     name=\"energy\",\n...     label=\"Signal Energy\",\n...     definition=\"Total energy (J) contained in the audio signal\",\n... )\n&gt;&gt;&gt; t_pitch = Term(\n...     name=\"pitch\",\n...     label=\"Fundamental Frequency\",\n...     definition=\"Frequency that contains the highest concentration of energy.\",\n... )\n&gt;&gt;&gt; feat1 = Feature(value=0.85, term=t_energy)\n&gt;&gt;&gt; feat2 = Feature(value=440.0, term=t_pitch)\n&gt;&gt;&gt; feat_list = [feat1, feat2]\n&gt;&gt;&gt; # Find value by name (uses the term name)\n&gt;&gt;&gt; find_feature_value(feat_list, name=\"pitch\")\n440.0\n&gt;&gt;&gt; # Find value by term name\n&gt;&gt;&gt; find_feature_value(feat_list, term_name=\"energy\")\n0.85\n&gt;&gt;&gt; # No match, return default\n&gt;&gt;&gt; find_feature_value(feat_list, name=\"zcr\", default=0.0)\n0.0\n</code></pre>"},{"location":"reference/data/#geometries","title":"Geometries","text":""},{"location":"reference/data/#soundevent.data.geometries","title":"<code>soundevent.data.geometries</code>","text":"<p>Geometry types.</p> <p>Sound event geometry plays a crucial role in locating and describing sound events within a recording. Different geometry types offer flexibility in representing the location of sound events, allowing for varying levels of detail and precision. The soundevent package follows the GeoJSON specification for geometry types and provides support for a range of geometry types.</p>"},{"location":"reference/data/#soundevent.data.geometries--units_and_time_reference","title":"Units and Time Reference","text":"<p>All geometries in the soundevent package utilize seconds as the unit for time and hertz as the unit for frequency. It is important to note that time values are always relative to the start of the recording. By consistently using these units, it becomes easier to develop functions and interact with geometry objects based on convenient assumptions.</p>"},{"location":"reference/data/#soundevent.data.geometries--supported_geometry_types","title":"Supported Geometry Types","text":"<p>The soundevent package supports the following geometry types, each serving a specific purpose in describing the location of sound events:</p> <ul> <li> <p>TimeStamp: Represents a single point in time.</p> </li> <li> <p>TimeInterval: Describes a time interval, indicating a range of time within which the sound event occurs.</p> </li> <li> <p>Point: Represents a specific point in time and frequency, pinpointing the exact location of the sound event.</p> </li> <li> <p>LineString: Represents a sequence of points in time and frequency, allowing for the description of continuous sound events.</p> </li> <li> <p>Polygon: Defines a closed shape in time and frequency, enabling the representation of complex sound event regions.</p> </li> <li> <p>Box: Represents a rectangle in time and frequency, useful for specifying rectangular sound event areas.</p> </li> <li> <p>Multi-Point: Describes a collection of points, allowing for the representation of multiple sound events occurring at different locations.</p> </li> <li> <p>Multi-Line String: Represents a collection of line strings, useful for capturing multiple continuous sound events.</p> </li> <li> <p>Multi-Polygon: Defines a collection of polygons, accommodating complex and overlapping sound event regions.</p> </li> </ul> <p>By offering these geometry types, the soundevent package provides a comprehensive framework for accurately and flexibly describing the location and extent of sound events within a recording.</p> <p>Classes:</p> Name Description <code>TimeStamp</code> <p>TimeStamp Geometry Class.</p> <code>TimeInterval</code> <p>TimeInterval Geometry Class.</p> <code>Point</code> <p>Point Geometry Class.</p> <code>LineString</code> <p>LineString Geometry Class.</p> <code>BoundingBox</code> <p>Bounding Box Geometry Class.</p> <code>Polygon</code> <p>Polygon Geometry Class.</p> <code>MultiPoint</code> <p>MultiPoint Geometry Class.</p> <code>MultiLineString</code> <p>MultiLineString Geometry Class.</p> <code>MultiPolygon</code> <p>MultiPolygon Geometry Class.</p> <p>Attributes:</p> Name Type Description <code>Geometry</code> <p>Geometry Type.</p>"},{"location":"reference/data/#soundevent.data.geometries-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.Geometry","title":"<code>Geometry = Union[TimeStamp, TimeInterval, Point, LineString, Polygon, BoundingBox, MultiPoint, MultiLineString, MultiPolygon]</code>  <code>module-attribute</code>","text":"<p>Geometry Type.</p> <p>The Geometry type is a versatile data structure designed to pinpoint sound events accurately within audio recordings. It encompasses a range of geometric representations, each tailored to specific use cases within bioacoustic research. These geometries play a fundamental role in describing the spatial and temporal characteristics of sound events, enabling precise analysis and interpretation.</p> <p>Available Geometries:</p> <ul> <li><code>TimeStamp</code>: Represents a sound event located at a specific point in time.</li> <li><code>TimeInterval</code>: Describes a sound event within a specified time interval.</li> <li><code>Point</code>: Locates a sound event at a precise time and frequency point.</li> <li><code>LineString</code>: Defines a sound event as a trajectory in time and frequency.</li> <li><code>Polygon</code>: Represents a complex shape in time and frequency, enclosing a     sound event.</li> <li><code>BoundingBox</code>: Describes a sound event within a defined time and frequency     range.</li> <li><code>MultiPoint</code>: Represents multiple discrete points in time and frequency as a     single sound event.</li> <li><code>MultiLineString</code>: Describes a sound event formed by multiple connected     trajectories.</li> <li><code>MultiPolygon</code>: Represents a sound event encompassed by multiple complex     shapes.</li> </ul>"},{"location":"reference/data/#soundevent.data.geometries-classes","title":"Classes","text":""},{"location":"reference/data/#soundevent.data.geometries.TimeStamp","title":"<code>TimeStamp</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>TimeStamp Geometry Class.</p> <p>The <code>TimeStamp</code> class represents a specific time point within an audio recording where a sound event occurs. This geometry type is particularly useful for very short sound events that are not well-represented by a time interval. <code>TimeStamp</code> provides precise temporal localization, indicating the exact moment when a sound event is detected.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>TimeStampName</code> <p>The type of geometry, always set to \"TimeStamp,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>Time</code> <p>The time stamp of the sound event, specifying the exact moment, in seconds, when the event occurred relative to the start of the recording. The time stamp is represented in a standard format, allowing for consistent interpretation and synchronization across different applications and systems.</p>"},{"location":"reference/data/#soundevent.data.geometries.TimeStamp-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.TimeStamp.coordinates","title":"<code>coordinates = Field(..., description='The time stamp of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.TimeStamp.type","title":"<code>type = 'TimeStamp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.TimeInterval","title":"<code>TimeInterval</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>TimeInterval Geometry Class.</p> <p>The <code>TimeInterval</code> class represents a specific time interval within an audio recording where a sound event occurs. This geometry type is particularly useful for events that have a clear start and end time but lack a well-defined frequency range. <code>TimeInterval</code> provides a structured way to define the duration of such sound events, allowing for accurate temporal localization.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>TimeIntervalName</code> <p>The type of geometry, always set to \"TimeInterval,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[Time]</code> <p>The time interval of the sound event, specifying the start and end times of the event relative to the start of the recording. The time interval is represented as a list of two <code>Time</code> objects, indicating the beginning and end of the event. This structured format enables precise definition of the event's duration.</p>"},{"location":"reference/data/#soundevent.data.geometries.TimeInterval-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.TimeInterval.coordinates","title":"<code>coordinates = Field(description='The time interval of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.TimeInterval.type","title":"<code>type = 'TimeInterval'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.Point","title":"<code>Point</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>Point Geometry Class.</p> <p>The <code>Point</code> class represents a specific point in time and frequency within an audio recording where a sound event occurs. This geometry type provides precise localization, indicating both the exact moment and frequency content of the event. <code>Point</code> is ideal for events with well-defined temporal and spectral characteristics, enabling accurate pinpointing of sound occurrences.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>PointName</code> <p>The type of geometry, always set to \"Point,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[float]</code> <p>The points of the sound event, specifying both the time and frequency components of the event relative to the start of the recording. The <code>coordinates</code> attribute is represented as a list of two float values, indicating the time and frequency of the event. This structured format enables accurate localization of the sound event within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.Point-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.Point.coordinates","title":"<code>coordinates = Field(..., description='The points of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.Point.type","title":"<code>type = 'Point'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.LineString","title":"<code>LineString</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>LineString Geometry Class.</p> <p>The <code>LineString</code> class represents a continuous trajectory of sound events within an audio recording, defined by a sequence of points in both time and frequency. This geometry type is particularly valuable for events with clear and defined frequency trajectories, allowing for detailed mapping of sound occurrences over time. <code>LineString</code> provides a comprehensive representation for events that exhibit specific frequency patterns or modulations.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>LineStringName</code> <p>The type of geometry, always set to \"LineString,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[List[float]]</code> <p>The line of the sound event, specifying the trajectory of the event as a list of points in both time and frequency. Each point is represented as a list of two float values, indicating the time and frequency coordinates of the event. The <code>coordinates</code> attribute is ordered by time, providing a clear sequence of the sound event's trajectory within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.LineString-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.LineString.coordinates","title":"<code>coordinates = Field(..., description='The line of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.LineString.type","title":"<code>type = 'LineString'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>Bounding Box Geometry Class.</p> <p>The <code>BoundingBox</code> class represents sound events within an audio recording, defined by a rectangular bounding box in both time and frequency. This geometry type is suitable for events with clear and well-defined frequency ranges, start and stop times. <code>BoundingBox</code> provides a simple yet effective way to localize sound events within a specific time interval and frequency band, enabling accurate mapping of events that exhibit distinct spectral content.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>BoundingBoxName</code> <p>The type of geometry, always set to \"BoundingBox,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[float]</code> <p>The bounding box of the sound event, specifying the start and end times as well as the start and end frequencies of the event. The <code>coordinates</code> attribute is represented as a list of four float values in the format (start time, start frequency, end time, end frequency). All times are relative to the start of the recording. This format allows precise localization of the event within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.BoundingBox-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.BoundingBox.coordinates","title":"<code>coordinates = Field(..., description='The bounding box of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.BoundingBox.type","title":"<code>type = 'BoundingBox'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.Polygon","title":"<code>Polygon</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>Polygon Geometry Class.</p> <p>The <code>Polygon</code> class represents complex-shaped sound events within an audio recording, defined by a polygonal boundary in both time and frequency. This geometry type is valuable for events that do not exhibit a concentrated frequency band and are contained within intricate shapes on the spectrogram. <code>Polygon</code> provides a detailed and accurate representation for sound events that require precise spatial localization and have diverse frequency distributions.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>PolygonName</code> <p>The type of geometry, always set to \"Polygon,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[List[List[float]]]</code> <p>The polygon of the sound event, specifying the boundary of the event as a list of points in both time and frequency. Each point is represented as a list of two float values, indicating the time and frequency coordinates of the event. The <code>coordinates</code> attribute represents the polygonal shape of the sound event, allowing for detailed mapping of its extent within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.Polygon-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.Polygon.coordinates","title":"<code>coordinates = Field(..., description='The polygon of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.Polygon.type","title":"<code>type = 'Polygon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiPoint","title":"<code>MultiPoint</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>MultiPoint Geometry Class.</p> <p>The <code>MultiPoint</code> class represents sound events within an audio recording, defined by multiple points in both time and frequency. This geometry type is suitable for events that consist of multiple interesting points distributed across the spectrogram. <code>MultiPoint</code> allows researchers to identify and map several distinct points within an event, providing a detailed representation for complex acoustic patterns.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>MultiPointName</code> <p>The type of geometry, always set to \"MultiPoint,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[List[float]]</code> <p>The points of the sound event, specifying the time and frequency coordinates of each interesting point within the event. The <code>coordinates</code> attribute is represented as a list of lists, where each inner list contains two float values representing the time and frequency of a point. All times are relative to the start of the recording. This format allows for the localization of multiple points within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.MultiPoint-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiPoint.coordinates","title":"<code>coordinates = Field(..., description='The points of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiPoint.type","title":"<code>type = 'MultiPoint'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiLineString","title":"<code>MultiLineString</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>MultiLineString Geometry Class.</p> <p>The <code>MultiLineString</code> class represents sound events within an audio recording, defined by multiple lines in both time and frequency. This geometry type is suitable for events that consist of multiple interesting lines distributed across the spectrogram. <code>MultiLineString</code> allows researchers to identify and map several distinct lines within an event, providing a detailed representation for complex acoustic patterns, such as events with multiple harmonics.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>MultiLineStringName</code> <p>The type of geometry, always set to \"MultiLineString,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[List[List[float]]]</code> <p>The lines of the sound event, specifying the time and frequency coordinates of each point in each line within the event. The <code>coordinates</code> attribute is represented as a list of lists of lists, where each innermost list contains two float values representing the time and frequency of a point. All times are relative to the start of the recording. This format allows for the localization of multiple lines within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.MultiLineString-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiLineString.coordinates","title":"<code>coordinates = Field(..., description='The lines of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiLineString.type","title":"<code>type = 'MultiLineString'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiPolygon","title":"<code>MultiPolygon</code>","text":"<p>               Bases: <code>BaseGeometry</code></p> <p>MultiPolygon Geometry Class.</p> <p>The <code>MultiPolygon</code> class represents sound events within an audio recording, defined by multiple polygons in both time and frequency. This geometry type is suitable for events that are split into multiple distinct polygons, often due to occlusion by other sound events. <code>MultiPolygon</code> enables the identification and mapping of multiple separate regions within an event, providing a detailed representation for complex acoustic patterns, such as events occluded by others.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>MultiPolygonName</code> <p>The type of geometry, always set to \"MultiPolygon,\" indicating the specific geometry type being utilized. This attribute ensures clear identification of the geometry representation and its intended purpose.</p> <code>coordinates</code> <code>List[List[List[List[float]]]]</code> <p>The polygons of the sound event, specifying the time and frequency coordinates for each point in each polygon within the event. The <code>coordinates</code> attribute is represented as a list of lists of lists of lists, where each innermost list contains two float values representing the time and frequency of a point. All times are relative to the start of the recording. This format allows for the localization of multiple distinct regions within the recording.</p>"},{"location":"reference/data/#soundevent.data.geometries.MultiPolygon-attributes","title":"Attributes","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiPolygon.coordinates","title":"<code>coordinates = Field(..., description='The polygons of the sound event.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#soundevent.data.geometries.MultiPolygon.type","title":"<code>type = 'MultiPolygon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/data/#other","title":"Other","text":""},{"location":"reference/data/#soundevent.data.PathLike","title":"<code>soundevent.data.PathLike = Union[os.PathLike, str]</code>  <code>module-attribute</code>","text":"<p>PathLike: A type alias for a path-like object.</p>"},{"location":"reference/evaluation/","title":"Evaluation Module","text":"Additional dependencies <p>To use the <code>soundevent.evaluation</code> module you need to install some additional dependencies. Make sure you have them installed by running the following command:</p> <pre><code>pip install soundevent[evaluation]\n</code></pre>"},{"location":"reference/evaluation/#soundevent.evaluation","title":"<code>soundevent.evaluation</code>","text":"<p>Evaluation functions.</p> <p>Modules:</p> Name Description <code>affinity</code> <p>Measures of affinity between sound events geometries.</p> <code>clip_classification</code> <code>clip_multilabel_classification</code> <code>encoding</code> <p>Tag Encoder Module.</p> <code>match</code> <p>Algorithms for matching geometries.</p> <code>metrics</code> <code>sound_event_classification</code> <p>Sound event classification evaluation.</p> <code>sound_event_detection</code> <p>Sound event detection evaluation.</p> <code>tasks</code> <p>Functions:</p> Name Description <code>classification_encoding</code> <p>Encode a list of tags into an integer value.</p> <code>compute_affinity</code> <p>Compute the geometric affinity between two geometries.</p> <code>create_tag_encoder</code> <p>Create an encoder object from a list of tags.</p> <code>match_geometries</code> <p>Match geometries between a source and a target sequence.</p> <code>multilabel_encoding</code> <p>Encode a list of tags into a binary multilabel array.</p> <code>prediction_encoding</code> <p>Encode a list of predicted tags into a floating-point array of scores.</p>"},{"location":"reference/evaluation/#soundevent.evaluation-functions","title":"Functions","text":""},{"location":"reference/evaluation/#soundevent.evaluation.classification_encoding","title":"<code>classification_encoding(tags, encoder)</code>","text":"<p>Encode a list of tags into an integer value.</p> <p>This function is commonly used for mapping a list of tags to a compact integer representation, typically representing classes associated with objects like clips or sound events.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Sequence[Tag]</code> <p>A list of tags to be encoded.</p> required <code>encoder</code> <code>Callable[[Tag], Optional[int]]</code> <p>A callable object that takes a data.Tag object as input and returns an optional integer encoding. If the encoder returns None for a tag, it will be skipped.</p> required <p>Returns:</p> Name Type Description <code>encoded</code> <code>Optional[int]</code> <p>The encoded integer value representing the tags, or None if no encoding is available.</p> <p>Examples:</p> <p>Consider the following set of tags:</p> <pre><code>&gt;&gt;&gt; dog = data.Tag(key=\"animal\", value=\"dog\")\n&gt;&gt;&gt; cat = data.Tag(key=\"animal\", value=\"cat\")\n&gt;&gt;&gt; brown = data.Tag(key=\"color\", value=\"brown\")\n&gt;&gt;&gt; blue = data.Tag(key=\"color\", value=\"blue\")\n</code></pre> <p>If we are interested in encoding only the 'dog' and 'brown' classes, the following examples demonstrate how the encoding works for tag list:</p> <pre><code>&gt;&gt;&gt; encoder = create_tag_encoder([dog, brown])\n&gt;&gt;&gt; classification_encoding([brown], encoder)\n1\n&gt;&gt;&gt; classification_encoding([dog, blue], encoder)\n0\n&gt;&gt;&gt; classification_encoding([dog, brown], encoder)\n0\n&gt;&gt;&gt; classification_encoding([cat], encoder)\n</code></pre>"},{"location":"reference/evaluation/#soundevent.evaluation.compute_affinity","title":"<code>compute_affinity(geometry1, geometry2, time_buffer=0.01, freq_buffer=100)</code>","text":"<p>Compute the geometric affinity between two geometries.</p> <p>This function calculates the geometric similarity between two geometries, which is a measure of how much they overlap. The affinity is computed as the Intersection over Union (IoU).</p> <p>Intersection over Union (IoU)</p> <p>IoU is a standard metric for comparing the similarity between two shapes. It is calculated as the ratio of the area of the overlap between the two geometries to the area of their combined shape.</p> <p>.. math::</p> <pre><code>\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n</code></pre> <p>An IoU of 1 means the geometries are identical, while an IoU of 0 means they do not overlap at all. This is particularly useful in bioacoustics for comparing annotations or predictions of sound events in a time-frequency representation (spectrogram).</p> <p>To account for small variations in annotations, a buffer can be added to each geometry before computing the IoU. This is controlled by the <code>time_buffer</code> and <code>freq_buffer</code> parameters.</p> <p>Parameters:</p> Name Type Description Default <code>geometry1</code> <code>Geometry</code> <p>The first geometry to be compared.</p> required <code>geometry2</code> <code>Geometry</code> <p>The second geometry to be compared.</p> required <code>time_buffer</code> <code>float</code> <p>Time buffer in seconds added to each geometry. Default is 0.01.</p> <code>0.01</code> <code>freq_buffer</code> <code>float</code> <p>Frequency buffer in Hertz added to each geometry. Default is 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>affinity</code> <code>float</code> <p>The Intersection over Union (IoU) score, a value between 0 and 1 indicating the degree of overlap.</p> Notes <ul> <li>If either input geometry is of a time-based type, a specialized time-based affinity calculation is performed.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; geometry1 = data.BoundingBox(coordinates=[0.4, 2000, 0.6, 8000])\n&gt;&gt;&gt; geometry2 = data.BoundingBox(coordinates=[0.5, 5000, 0.7, 6000])\n&gt;&gt;&gt; affinity = compute_affinity(\n...     geometry1,\n...     geometry2,\n...     time_buffer=0.02,\n...     freq_buffer=150,\n... )\n&gt;&gt;&gt; print(round(affinity, 3))\n0.111\n</code></pre>"},{"location":"reference/evaluation/#soundevent.evaluation.create_tag_encoder","title":"<code>create_tag_encoder(tags)</code>","text":"<p>Create an encoder object from a list of tags.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Sequence[Tag]</code> <p>A list of tags to be encoded.</p> required <p>Returns:</p> Type Description <code>SimpleEncoder</code> <p>An instance of SimpleEncoder initialized with the provided tags.</p>"},{"location":"reference/evaluation/#soundevent.evaluation.match_geometries","title":"<code>match_geometries(source, target, time_buffer=0.01, freq_buffer=100, affinity_threshold=0)</code>","text":"<p>Match geometries between a source and a target sequence.</p> <p>The matching is performed by first computing an affinity matrix between all pairs of source and target geometries. The affinity is a measure of similarity, calculated as the Intersection over Union (IoU). For more details on how affinity is computed, see <code>soundevent.evaluation.affinity.compute_affinity</code>.</p> <p>The affinity calculation is influenced by the <code>time_buffer</code> and <code>freq_buffer</code> parameters, which add a buffer to each geometry before comparison. This can help account for small variations in annotations.</p> <p>Once the affinity matrix is computed, the Hungarian algorithm (via <code>scipy.optimize.linear_sum_assignment</code>) is used to find an optimal assignment of source to target geometries that maximizes the total affinity.</p> <p>Finally, matches with an affinity below <code>affinity_threshold</code> are discarded and considered as unmatched.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Sequence[Geometry]</code> <p>The source geometries to match.</p> required <code>target</code> <code>Sequence[Geometry]</code> <p>The target geometries to match.</p> required <code>time_buffer</code> <code>float</code> <p>A buffer in seconds added to each geometry when computing affinity. See <code>soundevent.evaluation.affinity.compute_affinity</code> for more details. Defaults to 0.01.</p> <code>0.01</code> <code>freq_buffer</code> <code>float</code> <p>A buffer in Hertz added to each geometry when computing affinity. See <code>soundevent.evaluation.affinity.compute_affinity</code> for more details. Defaults to 100.</p> <code>100</code> <code>affinity_threshold</code> <code>float</code> <p>The minimum affinity (IoU) for a pair of geometries to be considered a match. Pairs with affinity below this value are considered unmatched, by default 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterable[Tuple[Optional[int], Optional[int], float]]</code> <p>An iterable of matching results. Each source and target geometry is accounted for exactly once in the output. Each tuple can be one of:</p> <ul> <li><code>(source_index, target_index, affinity)</code>: A successful match   between a source and a target geometry with an affinity score.</li> <li><code>(source_index, None, 0)</code>: An unmatched source geometry.</li> <li><code>(None, target_index, 0)</code>: An unmatched target geometry.</li> </ul>"},{"location":"reference/evaluation/#soundevent.evaluation.multilabel_encoding","title":"<code>multilabel_encoding(tags, encoder)</code>","text":"<p>Encode a list of tags into a binary multilabel array.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Sequence[Tag]</code> <p>A list of tags to be encoded.</p> required <code>encoder</code> <code>Encoder</code> <p>A callable object that takes a data.Tag object as input and returns an optional integer encoding. If the encoder returns None for a tag, it will be skipped.</p> required <p>Returns:</p> Name Type Description <code>encoded</code> <code>ndarray</code> <p>A binary numpy array of shape (num_classes,) representing the multilabel encoding for the input tags. Each index with a corresponding tag is set to 1, and the rest are 0.</p> <p>Examples:</p> <p>Consider the following set of tags:</p> <pre><code>&gt;&gt;&gt; dog = data.Tag(key=\"animal\", value=\"dog\")\n&gt;&gt;&gt; cat = data.Tag(key=\"animal\", value=\"cat\")\n&gt;&gt;&gt; brown = data.Tag(key=\"color\", value=\"brown\")\n&gt;&gt;&gt; blue = data.Tag(key=\"color\", value=\"blue\")\n</code></pre> <p>And we are only interested in encoding the following two classes:</p> <pre><code>&gt;&gt;&gt; encoder = create_tag_encoder([dog, brown])\n</code></pre> <p>Then the following examples show how the multilabel encoding works:</p> <pre><code>&gt;&gt;&gt; multilabel_encoding([brown], encoder)\narray([0, 1]...)\n&gt;&gt;&gt; multilabel_encoding([dog, blue], encoder)\narray([1, 0]...)\n&gt;&gt;&gt; multilabel_encoding([dog, brown], encoder)\narray([1, 1]...)\n&gt;&gt;&gt; multilabel_encoding([cat], encoder)\narray([0, 0]...)\n</code></pre>"},{"location":"reference/evaluation/#soundevent.evaluation.prediction_encoding","title":"<code>prediction_encoding(tags, encoder)</code>","text":"<p>Encode a list of predicted tags into a floating-point array of scores.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Sequence[PredictedTag]</code> <p>A list of predicted tags to be encoded.</p> required <code>encoder</code> <code>Encoder</code> <p>A callable object that takes a data.Tag object as input and returns an optional integer encoding. If the encoder returns None for a tag, it will be skipped.</p> required <p>Returns:</p> Name Type Description <code>encoded</code> <code>ndarray</code> <p>A numpy array of floats of shape (num_classes,) representing the predicted scores for each class. The array contains the scores for each class corresponding to the input predicted tags.</p> <p>Examples:</p> <p>Consider the following set of tags:</p> <pre><code>&gt;&gt;&gt; dog = data.Tag(key=\"animal\", value=\"dog\")\n&gt;&gt;&gt; cat = data.Tag(key=\"animal\", value=\"cat\")\n&gt;&gt;&gt; brown = data.Tag(key=\"color\", value=\"brown\")\n&gt;&gt;&gt; blue = data.Tag(key=\"color\", value=\"blue\")\n</code></pre> <p>And we are only interested in encoding the following two classes:</p> <pre><code>&gt;&gt;&gt; encoder = create_tag_encoder([dog, brown])\n</code></pre> <p>Then the following examples show how the encoding works for predicted tags:</p> <pre><code>&gt;&gt;&gt; prediction_encoding([data.PredictedTag(tag=brown, score=0.5)], encoder)\narray([0. , 0.5]...)\n&gt;&gt;&gt; prediction_encoding(\n...     [\n...         data.PredictedTag(tag=dog, score=0.2),\n...         data.PredictedTag(tag=blue, score=0.9),\n...     ],\n...     encoder,\n... )\narray([0.2, 0. ]...)\n&gt;&gt;&gt; prediction_encoding(\n...     [\n...         data.PredictedTag(tag=dog, score=0.2),\n...         data.PredictedTag(tag=brown, score=0.5),\n...     ],\n...     encoder,\n... )\narray([0.2, 0.5]...)\n&gt;&gt;&gt; prediction_encoding(\n...     [\n...         data.PredictedTag(tag=cat, score=0.7),\n...     ],\n...     encoder,\n... )\narray([0., 0.]...)\n</code></pre>"},{"location":"reference/geometry/","title":"Geometry Module","text":"Additional dependencies <p>To use the <code>soundevent.geometry</code> module you need to install some additional dependencies. Make sure you have them installed by running the following command:</p> <pre><code>pip install soundevent[geometry]\n</code></pre>"},{"location":"reference/geometry/#soundevent.geometry","title":"<code>soundevent.geometry</code>","text":"<p>Geometry Module.</p> <p>The geometry module in the <code>soundevent</code> package offers a set of essential functions to handle sound event geometry objects effectively. It provides tools to manage various aspects of sound event geometries, including operations like overlap detection, geometry shifting, and transformations. These functionalities are crucial in bioacoustic analysis, allowing users to comprehend the geometric relationships between different sound events.</p> <p>Understanding the spatial arrangement of sound events is pivotal in bioacoustic research, enabling tasks such as matching sound event predictions with ground truths for evaluation. The geometry module simplifies these tasks by providing a clear and efficient interface for handling sound event geometries.</p> <p>Modules:</p> Name Description <code>conversion</code> <p>Convert Geometry objects into shapely objects and vice versa.</p> <code>features</code> <p>Compute sound event features from geometries.</p> <code>html</code> <p>HTML representation of geometries.</p> <code>operations</code> <p>Functions that handle SoundEvent geometries.</p> <p>Functions:</p> Name Description <code>buffer_geometry</code> <p>Buffer a geometry.</p> <code>compute_bounds</code> <p>Compute the bounds of a geometry.</p> <code>compute_geometric_features</code> <p>Compute features from a geometry.</p> <code>compute_interval_overlap</code> <p>Compute the length of the overlap between two intervals.</p> <code>geometry_to_html</code> <p>Represent the geometry as HTML.</p> <code>geometry_to_shapely</code> <p>Convert a Geometry to a shapely geometry.</p> <code>get_geometry_point</code> <p>Calculate the coordinates of a specific point within a geometry.</p> <code>group_sound_events</code> <p>Group sound events into sequences based on a pairwise comparison.</p> <code>have_frequency_overlap</code> <p>Check if two geometries have frequency overlap.</p> <code>intervals_overlap</code> <p>Check if two intervals overlap.</p> <code>is_in_clip</code> <p>Check if a geometry lies within a clip, considering a minimum overlap.</p> <code>rasterize</code> <p>Rasterize geometric objects into an xarray DataArray.</p> <code>scale_geometry</code> <p>Scale a geometry by a given time and frequency factor.</p> <code>shapely_to_geometry</code> <p>Convert a shapely geometry to a soundevent geometry.</p> <code>shift_geometry</code>"},{"location":"reference/geometry/#soundevent.geometry-functions","title":"Functions","text":""},{"location":"reference/geometry/#soundevent.geometry.buffer_geometry","title":"<code>buffer_geometry(geometry, time_buffer=0, freq_buffer=0, **kwargs)</code>","text":"<p>Buffer a geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>The geometry to buffer.</p> required <code>time_buffer</code> <code>Time</code> <p>The time buffer to apply to the geometry, in seconds. Defaults to 0.</p> <code>0</code> <code>freq_buffer</code> <code>Frequency</code> <p>The frequency buffer to apply to the geometry, in Hz. Defaults to 0.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the Shapely buffer function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>geometry</code> <code>Geometry</code> <p>The buffered geometry.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the geometry type is not supported.</p> <code>ValueError</code> <p>If the time buffer or the frequency buffer is negative.</p>"},{"location":"reference/geometry/#soundevent.geometry.compute_bounds","title":"<code>compute_bounds(geometry)</code>","text":"<p>Compute the bounds of a geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>The geometry to compute the bounds of.</p> required <p>Returns:</p> Name Type Description <code>bounds</code> <code>tuple[float, float, float, float]</code> <p>The bounds of the geometry. The bounds are returned in the following order: start_time, low_freq, end_time, high_freq.</p>"},{"location":"reference/geometry/#soundevent.geometry.compute_geometric_features","title":"<code>compute_geometric_features(geometry)</code>","text":"<p>Compute features from a geometry.</p> <p>Some basic acoustic features can be computed from a geometry. This function computes these features and returns them as a list of features.</p> <p>The following features are computed when possible:</p> <ul> <li><code>duration</code>: The duration of the geometry.</li> <li><code>low_freq</code>: The lowest frequency of the geometry.</li> <li><code>high_freq</code>: The highest frequency of the geometry.</li> <li><code>bandwidth</code>: The bandwidth of the geometry.</li> <li><code>num_segments</code>: The number of segments in the geometry.</li> </ul> <p>Depending on the geometry type, some features may not be computed. For example, a <code>TimeStamp</code> geometry does not have a bandwidth.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>The geometry to compute features from.</p> required <p>Returns:</p> Type Description <code>List[Feature]</code> <p>The computed features.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the geometry type is not supported.</p>"},{"location":"reference/geometry/#soundevent.geometry.compute_interval_overlap","title":"<code>compute_interval_overlap(interval1, interval2)</code>","text":"<p>Compute the length of the overlap between two intervals.</p> <p>Parameters:</p> Name Type Description Default <code>interval1</code> <code>tuple[float, float]</code> <p>The first interval, as a tuple (start, stop).</p> required <code>interval2</code> <code>tuple[float, float]</code> <p>The second interval, as a tuple (start, stop).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The length of the overlap between the two intervals. If there is no overlap, the function returns 0.0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If intervals are not well-formed, i.e. start is greater than stop.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; compute_interval_overlap((0, 2), (1, 3))\n1.0\n&gt;&gt;&gt; compute_interval_overlap((0, 1), (1, 2))\n0.0\n&gt;&gt;&gt; compute_interval_overlap((0, 1), (2, 3))\n0.0\n</code></pre>"},{"location":"reference/geometry/#soundevent.geometry.geometry_to_html","title":"<code>geometry_to_html(geom)</code>","text":"<p>Represent the geometry as HTML.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Geometry</code> <p>The geometry to represent as HTML.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The HTML representation of the geometry.</p>"},{"location":"reference/geometry/#soundevent.geometry.geometry_to_shapely","title":"<code>geometry_to_shapely(geom)</code>","text":"<pre><code>geometry_to_shapely(\n    geom: data.TimeStamp,\n) -&gt; shapely.LineString\n</code></pre><pre><code>geometry_to_shapely(\n    geom: data.TimeInterval,\n) -&gt; shapely.Polygon\n</code></pre><pre><code>geometry_to_shapely(geom: data.Point) -&gt; shapely.Point\n</code></pre><pre><code>geometry_to_shapely(\n    geom: data.LineString,\n) -&gt; shapely.LineString\n</code></pre><pre><code>geometry_to_shapely(geom: data.Polygon) -&gt; shapely.Polygon\n</code></pre><pre><code>geometry_to_shapely(\n    geom: data.BoundingBox,\n) -&gt; shapely.Polygon\n</code></pre><pre><code>geometry_to_shapely(\n    geom: data.MultiPoint,\n) -&gt; shapely.MultiPoint\n</code></pre><pre><code>geometry_to_shapely(\n    geom: data.MultiLineString,\n) -&gt; shapely.MultiLineString\n</code></pre><pre><code>geometry_to_shapely(\n    geom: data.MultiPolygon,\n) -&gt; shapely.MultiPolygon\n</code></pre> <p>Convert a Geometry to a shapely geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Geometry</code> <p>The Geometry to convert.</p> required <p>Returns:</p> Type Description <code>Geometry</code> <p>The converted shapely geometry.</p>"},{"location":"reference/geometry/#soundevent.geometry.get_geometry_point","title":"<code>get_geometry_point(geometry, position='bottom-left')</code>","text":"<p>Calculate the coordinates of a specific point within a geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>The geometry object for which to calculate the point coordinates.</p> required <code>position</code> <code>Positions</code> <p>The specific point within the geometry to calculate coordinates for. Defaults to 'bottom-left'.</p> <code>'bottom-left'</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>The coordinates of the specified point within the geometry.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid point is specified.</p> Notes <p>The following positions are supported:</p> <ul> <li>'bottom-left': The point defined by the start time and lowest frequency     of the geometry.</li> <li>'bottom-right': The point defined by the end time and lowest frequency     of the geometry.</li> <li>'top-left': The point defined by the start time and highest frequency     of the geometry.</li> <li>'top-right': The point defined by the end time and highest frequency     of the geometry.</li> <li>'center-left': The point defined by the middle time and lowest frequency     of the geometry.</li> <li>'center-right': The point defined by the middle time and highest frequency     of the geometry.</li> <li>'top-center': The point defined by the end time and middle frequency     of the geometry.</li> <li>'bottom-center': The point defined by the start time and middle frequency     of the geometry.</li> <li>'center': The point defined by the middle time and middle frequency     of the geometry.</li> <li>'centroid': The centroid of the geometry. Computed using the shapely     library.</li> <li>'point_on_surface': A point on the surface of the geometry. Computed     using the shapely library.</li> </ul> <p>For all positions except 'centroid' and 'point_on_surface', the time and frequency values are calculated by first computing the bounds of the geometry and then determining the appropriate values based on the specified point type.</p>"},{"location":"reference/geometry/#soundevent.geometry.group_sound_events","title":"<code>group_sound_events(sound_events, comparison_fn)</code>","text":"<p>Group sound events into sequences based on a pairwise comparison.</p> <p>This function takes a sequence of <code>data.SoundEvent</code> objects and a comparison function. It applies the comparison function to all pairs of sound events to determine their similarity. Sound events that are deemed similar are grouped together into <code>data.Sequence</code> objects.</p> <p>The comparison function should take two <code>data.SoundEvent</code> objects as input and return True if they are considered similar, and False otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>sound_events</code> <code>Sequence[SoundEvent]</code> <p>A sequence of <code>data.SoundEvent</code> objects to be grouped.</p> required <code>comparison_fn</code> <code>Callable[[SoundEvent, SoundEvent], bool]</code> <p>A function that compares two <code>data.SoundEvent</code> objects and returns True if they should be grouped together, False otherwise.</p> required <p>Returns:</p> Type Description <code>list[Sequence]</code> <p>A list of <code>data.Sequence</code> objects, where each sequence contains a group of similar sound events.</p> Notes <p>This function groups sound events based on transitive similarity. While it uses pairwise comparisons, the final groups (sequences) can include sound events that aren't directly similar according to your <code>comparison_fn</code>. Think of it like a chain:  sound event A is similar to B, B is similar to C, but A might not be similar to C directly. They all end up in the same group because of their connections through B. Technically, this works by finding the connected components in a graph where the sound events are nodes, and the edges represent similarity based on your <code>comparison_fn</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from soundevent import data\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; recording = data.Recording(\n...     path=Path(\"example.wav\"),\n...     duration=60,\n...     samplerate=44100,\n...     channels=1,\n... )\n&gt;&gt;&gt; sound_events = [\n...     data.SoundEvent(\n...         geometry=data.BoundingBox(coordinates=[0, 1000, 1, 2000]),\n...         recording=recording,\n...     ),\n...     data.SoundEvent(\n...         geometry=data.BoundingBox(coordinates=[0.8, 800, 1.2, 1600]),\n...         recording=recording,\n...     ),\n...     data.SoundEvent(\n...         geometry=data.BoundingBox(coordinates=[8, 900, 9.3, 1500]),\n...         recording=recording,\n...     ),\n... ]\n&gt;&gt;&gt; # Define a comparison function based on temporal overlap\n&gt;&gt;&gt; def compare_sound_events(se1, se2):\n...     return have_temporal_overlap(\n...         se1.geometry, se2.geometry, min_absolute_overlap=0.5\n...     )\n&gt;&gt;&gt; # Group sound events with the comparison function\n&gt;&gt;&gt; sequences = group_sound_events(sound_events, compare_sound_events)\n</code></pre>"},{"location":"reference/geometry/#soundevent.geometry.have_frequency_overlap","title":"<code>have_frequency_overlap(geom1, geom2, min_absolute_overlap=0, min_relative_overlap=None)</code>","text":"<p>Check if two geometries have frequency overlap.</p> <p>This function determines whether two geometry objects have any frequency overlap, optionally considering a minimum required overlap, either in absolute terms (Hz) or relative to the smaller frequency range of the two geometries.</p> <p>Parameters:</p> Name Type Description Default <code>geom1</code> <code>Geometry</code> <p>The first geometry object.</p> required <code>geom2</code> <code>Geometry</code> <p>The second geometry object.</p> required <code>min_absolute_overlap</code> <code>float</code> <p>The minimum required absolute overlap in Hz. Defaults to 0, meaning any positive overlap is sufficient (touching intervals are not considered overlapping).</p> <code>0</code> <code>min_relative_overlap</code> <code>float</code> <p>The minimum required relative overlap (between 0 and 1). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the geometries overlap with the specified minimum overlap, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If both <code>min_absolute_overlap</code> (&gt; 0) and <code>min_relative_overlap</code> are provided.</li> <li>If <code>min_relative_overlap</code> is not in the range [0, 1].</li> </ul>"},{"location":"reference/geometry/#soundevent.geometry.intervals_overlap","title":"<code>intervals_overlap(interval1, interval2, min_relative_overlap=None, min_absolute_overlap=0)</code>","text":"<p>Check if two intervals overlap.</p> <p>This function determines whether two intervals, represented as tuples of (start, stop) values, overlap. An overlap is considered to occur if the length of the intersection of the two intervals is strictly greater than a specified threshold.</p> <p>By default, touching intervals are not considered overlapping.</p> <p>Parameters:</p> Name Type Description Default <code>interval1</code> <code>tuple[float, float]</code> <p>The first interval, as a tuple (start, stop).</p> required <code>interval2</code> <code>tuple[float, float]</code> <p>The second interval, as a tuple (start, stop).</p> required <code>min_absolute_overlap</code> <code>float</code> <p>The minimum required absolute overlap. The overlap must be strictly greater than this value. Defaults to 0.</p> <code>0</code> <code>min_relative_overlap</code> <code>float</code> <p>The minimum required relative overlap with respect to the shorter of the two intervals. The overlap must be strictly greater than the computed threshold. The value must be in the range [0, 1]. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the intervals overlap with the specified minimum overlap, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <ul> <li>If both <code>min_absolute_overlap &gt; 0</code> and <code>min_relative_overlap</code> are provided.</li> <li>If <code>min_relative_overlap</code> is not in the range [0, 1].</li> </ul>"},{"location":"reference/geometry/#soundevent.geometry.is_in_clip","title":"<code>is_in_clip(geometry, clip, minimum_overlap=0)</code>","text":"<p>Check if a geometry lies within a clip, considering a minimum overlap.</p> <p>This function determines whether a given geometry falls within the time boundaries of a clip. It takes into account a <code>minimum_overlap</code> parameter, which specifies the minimum required temporal overlap (in seconds) between the geometry and the clip for the geometry to be considered inside the clip.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>The geometry object to be checked.</p> required <code>clip</code> <code>Clip</code> <p>The clip object to check against.</p> required <code>minimum_overlap</code> <code>float</code> <p>The minimum required overlap between the geometry and the clip in seconds. Defaults to 0, meaning any overlap is sufficient.</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the geometry is within the clip with the specified minimum overlap, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>minimum_overlap</code> is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from soundevent import data\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; recording = data.Recording(\n...     path=Path(\"example.wav\"),\n...     samplerate=44100,\n...     duration=60,\n...     channels=1,\n... )\n&gt;&gt;&gt; geometry = data.BoundingBox(coordinates=[4, 600, 4.8, 1200])\n&gt;&gt;&gt; clip = data.Clip(start_time=0.0, end_time=5.0, recording=recording)\n&gt;&gt;&gt; is_in_clip(geometry, clip, minimum_overlap=0.5)\nTrue\n</code></pre>"},{"location":"reference/geometry/#soundevent.geometry.rasterize","title":"<code>rasterize(geometries, array, values=1, fill=0, dtype=np.float32, xdim=Dimensions.time.value, ydim=Dimensions.frequency.value, all_touched=False)</code>","text":"<p>Rasterize geometric objects into an xarray DataArray.</p> <p>This function takes a list of geometric objects (<code>geometries</code>) and rasterizes them into a specified <code>xr.DataArray</code>. Each geometry can be associated with a <code>value</code>, which is used to fill the corresponding pixels in the rasterized array.</p> <p>Parameters:</p> Name Type Description Default <code>geometries</code> <code>List[Geometry]</code> <p>A list of <code>Geometry</code> objects to rasterize.</p> required <code>array</code> <code>DataArray</code> <p>The xarray DataArray into which the geometries will be rasterized.</p> required <code>values</code> <code>Union[Value, List[Value], Tuple[Value]]</code> <p>The values to fill the rasterized pixels for each geometry. If a single value is provided, it will be used for all geometries. If a list or tuple of values is provided, it must have the same length as the <code>geometries</code> list. Defaults to 1.</p> <code>1</code> <code>fill</code> <code>float</code> <p>The value to fill pixels not covered by any geometry. Defaults to 0.</p> <code>0</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type of the output rasterized array. Defaults to np.float32.</p> <code>float32</code> <code>xdim</code> <code>str</code> <p>The name of the dimension representing the x-axis in the DataArray. Defaults to \"time\".</p> <code>time.value</code> <code>ydim</code> <code>str</code> <p>The name of the dimension representing the y-axis in the DataArray. Defaults to \"frequency\".</p> <code>frequency.value</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be filled, otherwise only pixels whose center point is within the geometry are filled. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>A new xarray DataArray containing the rasterized data, with the same coordinates and dimensions as the input <code>array</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of <code>values</code> does not match the number of <code>geometries</code>.</p>"},{"location":"reference/geometry/#soundevent.geometry.scale_geometry","title":"<code>scale_geometry(geom, time=1, freq=1, time_anchor=0, freq_anchor=0)</code>","text":"<p>Scale a geometry by a given time and frequency factor.</p> <p>The scaling is performed with respect to an anchor point. The formula for scaling a value <code>val</code> is <code>(val - anchor) * factor + anchor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Geometry</code> <p>The geometry to scale.</p> required <code>time</code> <code>float</code> <p>The time factor to apply to the geometry. Defaults to 1.</p> <code>1</code> <code>freq</code> <code>float</code> <p>The frequency factor to apply to the geometry. Defaults to 1.</p> <code>1</code> <code>time_anchor</code> <code>float</code> <p>The time anchor to use for scaling, in seconds. Defaults to 0.</p> <code>0</code> <code>freq_anchor</code> <code>float</code> <p>The frequency anchor to use for scaling, in Hz. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Geometry</code> <p>The scaled geometry.</p>"},{"location":"reference/geometry/#soundevent.geometry.shapely_to_geometry","title":"<code>shapely_to_geometry(geom)</code>","text":"<p>Convert a shapely geometry to a soundevent geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Geometry</code> <p>The shapely geometry to convert.</p> required <p>Returns:</p> Type Description <code>Geometry</code> <p>The converted soundevent geometry.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the geometry type is not supported.</p>"},{"location":"reference/geometry/#soundevent.geometry.shift_geometry","title":"<code>shift_geometry(geom, time=0, freq=0)</code>","text":""},{"location":"reference/io/","title":"IO Module","text":""},{"location":"reference/io/#soundevent.io","title":"<code>soundevent.io</code>","text":"<p>The IO module of the soundevent package.</p> <p>This module contains the classes and functions for reading and writing sound event data.</p> <p>Functions:</p> Name Description <code>save</code> <p>Save a data object to a file.</p> <code>load</code> <p>Load data from a file.</p> <p>Attributes:</p> Name Type Description <code>DataCollections</code> <p>Type alias for all data collection types.</p>"},{"location":"reference/io/#soundevent.io.DataCollections","title":"<code>DataCollections = Union[data.Dataset, data.AnnotationSet, data.AnnotationProject, data.PredictionSet, data.ModelRun, data.EvaluationSet, data.Evaluation, data.RecordingSet]</code>  <code>module-attribute</code>","text":"<p>Type alias for all data collection types.</p>"},{"location":"reference/io/#soundevent.io.save","title":"<code>save(obj, path, audio_dir=None, format='aoef', **kwargs)</code>","text":"<p>Save a data object to a file.</p> <p>This function saves a data object to a file in a format that can be loaded by the <code>load</code> function. The following object types are supported:</p> <ul> <li><code>RecordingSet</code></li> <li><code>Dataset</code></li> <li><code>AnnotationSet</code></li> <li><code>AnnotationProject</code></li> <li><code>EvaluationSet</code></li> <li><code>PredictionSet</code></li> <li><code>ModelRun</code></li> <li><code>Evaluation</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>DataCollections</code> <p>The data object to save.</p> required <code>path</code> <code>PathLike</code> <p>Path to the file to save to.</p> required <code>audio_dir</code> <code>Optional[PathLike]</code> <p>All path to audio files will be stored relative to this directory. This is useful to avoid storing absolute paths which are not portable. If <code>None</code>, audio paths will be stored as absolute paths.</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Format to save the data in. If <code>None</code>, the format will be inferred from the file extension.</p> <code>'aoef'</code>"},{"location":"reference/io/#soundevent.io.load","title":"<code>load(path, audio_dir=None, format='aoef', type=None)</code>","text":"<pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"recording_set\"] = \"recording_set\",\n) -&gt; data.RecordingSet\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"dataset\"] = \"dataset\",\n) -&gt; data.Dataset\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"annotation_set\"] = \"annotation_set\",\n) -&gt; data.AnnotationSet\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\n        \"annotation_project\"\n    ] = \"annotation_project\",\n) -&gt; data.AnnotationProject\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"prediction_set\"] = \"prediction_set\",\n) -&gt; data.PredictionSet\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"model_run\"] = \"model_run\",\n) -&gt; data.ModelRun\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"evaluation_set\"] = \"evaluation_set\",\n) -&gt; data.EvaluationSet\n</code></pre><pre><code>load(\n    path: data.PathLike,\n    audio_dir: Optional[data.PathLike] = None,\n    format: Optional[str] = \"aoef\",\n    type: Literal[\"evaluation\"] = \"evaluation\",\n) -&gt; data.Evaluation\n</code></pre> <p>Load data from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the file to load.</p> required <code>audio_dir</code> <code>Optional[PathLike]</code> <p>Path to the directory containing the audio files. Often, the audio path is stored relative to some directory to avoid storing absolute paths, which are not portable. In this case, the <code>audio_dir</code> argument is used to resolve the relative paths.</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Format of the file to load. If not specified, the format is inferred.</p> <code>'aoef'</code> <code>type</code> <code>Optional[DataType]</code> <p>Type of the data to load. If not specified, the type is inferred. This argument is mainly used for letting the type system know what type of object is returned by the function and can be omitted in most cases.</p> <code>None</code>"},{"location":"reference/io/#soundevent.io.crowsetta","title":"<code>soundevent.io.crowsetta</code>","text":"<p>Crowsetta Module.</p> <p>This module provides a set of functions to facilitate the export and import of data in the soundevent format to and from the Crowsetta format.</p> <p>Modules:</p> Name Description <code>annotation</code> <p>Module for converting between ClipAnnotation and Crowsetta annotation formats.</p> <code>bbox</code> <p>Module for converting between SoundEvent annotations and Crowsetta bounding boxes.</p> <code>labels</code> <p>Crowsetta label conversion functions.</p> <code>segment</code> <p>crowsetta.segment module.</p> <code>sequence</code> <p>Module for converting between SoundEvent annotations and Crowsetta sequences.</p> <p>Functions:</p> Name Description <code>annotation_from_clip_annotation</code> <p>Convert a ClipAnnotation to a Crowsetta annotation.</p> <code>annotation_to_clip_annotation</code> <p>Convert a Crowsetta annotation to a ClipAnnotation.</p> <code>bbox_from_annotation</code> <p>Convert a soundevent annotation to a Crowsetta bounding box.</p> <code>bbox_to_annotation</code> <p>Convert a crowsetta bounding box annotation to a soundevent annotation.</p> <code>label_from_tag</code> <p>Convert a soundevent tag to a crowsetta label.</p> <code>label_from_tags</code> <p>Convert a sequence of soundevent tags to a crowsetta label.</p> <code>label_to_tags</code> <p>Convert a crowsetta label to a list of soundevent tags.</p> <code>segment_from_annotation</code> <p>Convert a soundevent annotation to a crowsetta segment.</p> <code>segment_to_annotation</code> <p>Convert a crowsetta segment to a soundevent time interval annotation.</p> <code>sequence_from_annotations</code> <p>Convert a sequence of soundevent annotations to a Crowsetta sequence.</p> <code>sequence_to_annotations</code> <p>Convert a Crowsetta sequence to a list of soundevent annotations.</p>"},{"location":"reference/io/#soundevent.io.crowsetta-functions","title":"Functions","text":""},{"location":"reference/io/#soundevent.io.crowsetta.annotation_from_clip_annotation","title":"<code>annotation_from_clip_annotation(annot, annot_path, annotation_fmt, ignore_errors=True, cast_geometry=True, **kwargs)</code>","text":"<p>Convert a ClipAnnotation to a Crowsetta annotation.</p> <p>This function transforms a <code>ClipAnnotation</code> object into a Crowsetta annotation (<code>crowsetta.Annotation</code>). The choice of annotation format (<code>bbox</code> for bounding boxes or <code>seq</code> for sequences) determines the type of Crowsetta annotation created. Each sound event within the <code>ClipAnnotation</code> is individually converted into either a sequence or a bounding boxes using the corresponding conversion functions.</p> <p>Parameters:</p> Name Type Description Default <code>annot</code> <code>ClipAnnotation</code> <p>The ClipAnnotation object to convert.</p> required <code>annot_path</code> <code>PathLike</code> <p>The path to the annotation file.</p> required <code>annotation_fmt</code> <code>Union[Literal['bbox'], Literal['seq']]</code> <p>The desired Crowsetta annotation format: 'bbox' for bounding boxes or 'seq' for sequences.</p> required <code>ignore_errors</code> <code>bool</code> <p>If True, ignore errors during conversion and continue with the next sound event, otherwise this function will raise an error if a sound event cannot be converted, by default True.</p> <code>True</code> <code>cast_geometry</code> <code>bool</code> <p>If True, cast non-matching geometries to the expected type, otherwise raise an error if the geometry does not match the expected type, by default True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the corresponding conversion functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Annotation</code> <p>A Crowsetta annotation representing the converted ClipAnnotation.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.annotation_to_clip_annotation","title":"<code>annotation_to_clip_annotation(annot, recording=None, tags=None, notes=None, adjust_time_expansion=True, created_by=None, recording_kwargs=None, **kwargs)</code>","text":"<p>Convert a Crowsetta annotation to a ClipAnnotation.</p> <p>This function transforms a Crowsetta annotation (<code>crowsetta.Annotation</code>) into a <code>ClipAnnotation</code> object. Depending on the annotation format, the Crowsetta annotation is converted into a list of sound event annotations or sequence annotations, or both, and included in the resulting ClipAnnotation.</p> <p>Parameters:</p> Name Type Description Default <code>annot</code> <code>Annotation</code> <p>The Crowsetta annotation to convert.</p> required <code>recording</code> <code>Optional[Recording]</code> <p>The original recording associated with the annotations, if not provided, it is loaded based on the path of the notated recording, by default None.</p> <code>None</code> <code>tags</code> <code>Optional[List[Tag]]</code> <p>Tags associated with the clip annotation, by default None.</p> <code>None</code> <code>notes</code> <code>Optional[List[Note]]</code> <p>Notes associated with the clip annotation, by default None.</p> <code>None</code> <code>adjust_time_expansion</code> <code>bool</code> <p>If True, adjust the onset and offset times based on the recording's time expansion factor, by default True.</p> <code>True</code> <code>created_by</code> <code>Optional[User]</code> <p>User information representing the creator of the converted clip annotation, by default None.</p> <code>None</code> <code>recording_kwargs</code> <code>Optional[dict]</code> <p>Additional keyword arguments passed when loading the recording, by default None. You can use this to set the recording metadata such as <code>time_expansion</code>, <code>latitude</code>, <code>longitude</code>, etc.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the conversion functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClipAnnotation</code> <p>A ClipAnnotation representing the converted Crowsetta annotation.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.bbox_from_annotation","title":"<code>bbox_from_annotation(obj, cast_to_bbox=True, raise_on_time_geometries=True, **kwargs)</code>","text":"<p>Convert a soundevent annotation to a Crowsetta bounding box.</p> <p>This function transforms a SoundEventAnnotation object into a Crowsetta bounding box (<code>BBox</code>). The SoundEvent's geometry is used to determine the onset, offset, low frequency, and high frequency values of the bounding box, and the associated tags are converted into a Crowsetta label using the <code>convert_tags_to_label</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>SoundEventAnnotation</code> <p>The SoundEventAnnotation object to convert.</p> required <code>cast_to_bbox</code> <code>bool</code> <p>If True, cast the geometry to a bounding box, otherwise, keep it as is, by default True.</p> <code>True</code> <code>raise_on_time_geometries</code> <code>bool</code> <p>If True, raise an exception if the geometry is a TimeInterval or TimeStamp, otherwise, cast it to a bounding box with a lowest frequency of 0 and a highest frequency of the recording's Nyquist frequency. By default True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>convert_tags_to_label</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BBox</code> <p>A Crowsetta bounding box representing the converted soundevent annotation, with onset, offset, low frequency, high frequency, and associated label.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.bbox_to_annotation","title":"<code>bbox_to_annotation(bbox, recording, adjust_time_expansion=True, notes=None, created_by=None, **kwargs)</code>","text":"<p>Convert a crowsetta bounding box annotation to a soundevent annotation.</p> <p>This function transforms a crowsetta bounding box annotation (<code>BBox</code>) into a <code>SoundEventAnnotation</code> object. The onset, offset, low frequency, and high frequency values of the bounding box are used to create a <code>BoundingBox</code> geometry. The label associated with the bounding box is converted into a list of soundevent tags using the <code>convert_label_to_tags</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>BBox</code> <p>The bounding box annotation to convert.</p> required <code>recording</code> <code>Recording</code> <p>The original recording from which the bounding box annotation was derived.</p> required <code>adjust_time_expansion</code> <code>bool</code> <p>If True, adjust the onset, offset, and frequency values based on the recording's time expansion factor, by default True.</p> <code>True</code> <code>notes</code> <code>List[Note]</code> <p>Additional notes associated with the converted soundevent annotation, by default None.</p> <code>None</code> <code>created_by</code> <code>User</code> <p>User information representing the creator of the converted soundevent annotation, by default None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>convert_label_to_tags</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>SoundEventAnnotation</code> <p>A SoundEventAnnotation object representing the converted bounding box annotation, containing a SoundEvent with the bounding box geometry, associated tags, notes, and creator information.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.label_from_tag","title":"<code>label_from_tag(tag, label_fn=None, label_mapping=None, value_only=False, separator=':')</code>","text":"<p>Convert a soundevent tag to a crowsetta label.</p> <p>This function facilitates the conversion of a soundevent tag into a crowsetta label. Users can customize this conversion using the following options:</p> <ol> <li>If a custom mapping function (<code>label_fn</code> argument) is provided, it will be used to directly convert the tag to a label.</li> <li>If a mapping dictionary (<code>label_mapping</code> argument) is provided, the function will attempt to look up the label for the tag in the mapping. If found, it returns the label; otherwise, it proceeds to the next option.</li> <li>If the <code>value_only</code> argument is set to True, the function returns only the value of the tag.</li> <li>If none of the above conditions are met, the function constructs the label by combining the tag's key and value with the specified separator.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Tag</code> <p>The soundevent tag to convert to a label.</p> required <code>label_fn</code> <code>Optional[Callable[[Tag], str]]</code> <p>A function to convert tags to labels.</p> <code>None</code> <code>label_mapping</code> <code>Optional[Dict[Tag, str]]</code> <p>A dictionary mapping tags to labels.</p> <code>None</code> <code>value_only</code> <code>bool</code> <p>If True, return only the value of the tag, by default False.</p> <code>False</code> <code>separator</code> <code>str</code> <p>The separator to use between the key and value when constructing the label, by default \":\".</p> <code>':'</code> <p>Returns:</p> Type Description <code>str</code> <p>The crowsetta label corresponding to the soundevent tag.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.label_from_tags","title":"<code>label_from_tags(tags, seq_label_fn=None, select_by_key=None, index=None, separator=',', empty_label=EMPTY_LABEL, **kwargs)</code>","text":"<p>Convert a sequence of soundevent tags to a crowsetta label.</p> <p>This function facilitates the conversion of a sequence of soundevent tags into a Crowsetta label. Users can customize the conversion process using the following options:</p> <ol> <li>If a custom sequence label function (<code>seq_label_fn</code> argument) is provided, it will be used to directly convert the sequence of tags to a label.</li> <li>If the sequence of tags is empty, the function returns the specified <code>empty_label</code>.</li> <li>If the <code>select_by_key</code> argument is provided, the function will attempt to find the first tag in the sequence with a matching key. If found, it returns the label converted from that tag using <code>convert_tag_to_label</code>, otherwise it returns the <code>empty_label</code>.</li> <li>If the <code>index</code> argument is provided, it will be used to select a tag from the sequence based on the index. If the index is out of bounds, it wraps around to the valid range. The label converted from the selected tag is then returned.</li> <li>If none of the above conditions are met, the function constructs a label by joining the labels of all tags in the sequence with the specified separator.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Sequence[Tag]</code> <p>The sequence of soundevent tags to convert to a label.</p> required <code>seq_label_fn</code> <code>Optional[Callable[[Sequence[Tag]], str]]</code> <p>A function to convert sequences of tags to labels.</p> <code>None</code> <code>select_by_key</code> <code>Optional[str]</code> <p>If provided, select the first tag with a matching key for label conversion.</p> <code>None</code> <code>index</code> <code>Optional[int]</code> <p>If provided, use it as the index to select a tag from the sequence for label conversion. The index is wrapped around if it exceeds the bounds.</p> <code>None</code> <code>separator</code> <code>str</code> <p>The separator to use between the labels when constructing the final label.</p> <code>','</code> <code>empty_label</code> <code>str</code> <p>The label to return when the sequence of tags is empty. By default \"empty\".</p> <code>EMPTY_LABEL</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>convert_tag_to_label</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The Crowsetta label corresponding to the sequence of soundevent tags.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.label_to_tags","title":"<code>label_to_tags(label, tag_fn=None, tag_mapping=None, term_mapping=None, key_mapping=None, key=None, term=None, fallback='crowsetta', empty_labels=(EMPTY_LABEL,))</code>","text":"<p>Convert a crowsetta label to a list of soundevent tags.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The Crowsetta label to convert.</p> required <code>tag_fn</code> <code>Optional[LabelToTagFn]</code> <p>A function to directly convert labels to a list of tags.</p> <code>None</code> <code>tag_mapping</code> <code>Optional[LabelToTagMap]</code> <p>A dictionary mapping labels to lists of tags or a single tag.</p> <code>None</code> <code>term_mapping</code> <code>Optional[Dict[str, Term]]</code> <p>A dictionary mapping labels directly to <code>soundevent.data.Term</code> objects</p> <code>None</code> <code>key_mapping</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary mapping labels to keys (deprecated, use <code>term_mapping</code> instead).</p> <code>None</code> <code>key</code> <code>Optional[str]</code> <p>The key to use for the tag (deprecated, use <code>term</code> instead).</p> <code>None</code> <code>term</code> <code>Optional[Term]</code> <p>The <code>soundevent.data.Term</code> to use for the tag</p> <code>None</code> <code>fallback</code> <code>str</code> <p>The key to use if no other key is provided (deprecated, use <code>term</code> instead).</p> <code>'crowsetta'</code> <code>empty_labels</code> <code>Sequence[str]</code> <p>Labels considered empty, resulting in an empty list of tags.</p> <code>(EMPTY_LABEL,)</code> <p>Returns:</p> Type Description <code>List[Tag]</code> <p>The list of soundevent tags corresponding to the Crowsetta label.</p> Notes <p>This is the default conversion process:</p> <ol> <li>If <code>label</code> is in <code>empty_labels</code>, return an empty list.</li> <li>If <code>tag_fn</code> is provided, use it to convert the label.</li> <li>If <code>term_mapping</code> is provided and the label is found, use the corresponding term.</li> <li>If <code>tag_mapping</code> is provided and the label is found, return the corresponding tags.</li> <li>If <code>key_mapping</code> is provided and the label is found, use the corresponding key (deprecated).</li> <li>If <code>key</code> is provided, use it (deprecated). Otherwise, use <code>fallback</code> (deprecated).</li> <li>If <code>term</code> is not yet set, derive it from the <code>key</code> (deprecated).</li> <li>Return a list containing a single <code>Tag</code> with the determined <code>term</code> and the original <code>label</code> as the <code>value</code>.</li> </ol>"},{"location":"reference/io/#soundevent.io.crowsetta.segment_from_annotation","title":"<code>segment_from_annotation(obj, cast_to_segment=True, **kwargs)</code>","text":"<p>Convert a soundevent annotation to a crowsetta segment.</p> <p>This function transforms a SoundEventAnnotation object into a Crowsetta segment. The SoundEvent's geometry is used to determine the onset and offset times of the segment, and the associated tags are converted into a Crowsetta label using the <code>convert_tags_to_label</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>SoundEventAnnotation</code> <p>The SoundEventAnnotation object to convert.</p> required <code>cast_to_segment</code> <code>bool</code> <p>If True, any geometry that is not a TimeInterval will be cast to a TimeInterval, otherwise a ValueError will be raised. By default True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>convert_tags_to_label</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Segment</code> <p>A Crowsetta Segment representing the converted soundevent segment, with onset and offset times, sample indices, and associated label.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sound event has no geometry, or if the geometry is not a TimeInterval and <code>cast_to_segment</code> is False.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.segment_to_annotation","title":"<code>segment_to_annotation(segment, recording, adjust_time_expansion=True, notes=None, created_by=None, **kwargs)</code>","text":"<p>Convert a crowsetta segment to a soundevent time interval annotation.</p> <p>This function transforms a Crowsetta segment into a SoundEvent time interval annotation. The segment's onset and offset times are used to create a time interval, and the label is converted into a list of soundevent tags using the <code>convert_label_to_tags</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>Segment</code> <p>The Crowsetta segment to convert.</p> required <code>recording</code> <code>Recording</code> <p>The original recording associated with the segment.</p> required <code>adjust_time_expansion</code> <code>bool</code> <p>If True, adjust the segment's onset and offset times based on the recording's time expansion factor, by default True.</p> <code>True</code> <code>notes</code> <code>List[Note]</code> <p>Additional notes associated with the converted time interval, by default None.</p> <code>None</code> <code>created_by</code> <code>User</code> <p>User information representing the creator of annotation. By default None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>convert_label_to_tags</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>SoundEventAnnotation</code> <p>A SoundEventAnnotation object representing the converted time interval, containing a SoundEvent with the time interval, associated tags, notes, and creator information.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.sequence_from_annotations","title":"<code>sequence_from_annotations(annotations, cast_to_segment=True, ignore_errors=False, **kwargs)</code>","text":"<p>Convert a sequence of soundevent annotations to a Crowsetta sequence.</p> <p>This function transforms a sequence of <code>SoundEventAnnotation</code> objects into a Crowsetta sequence (<code>crowsetta.Sequence</code>). Each annotation is individually converted into a Crowsetta segment using the <code>to_crowsetta_segment</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>Sequence[SoundEventAnnotation]</code> <p>The sequence of SoundEventAnnotation objects to convert.</p> required <code>cast_to_segment</code> <code>bool</code> <p>If True, cast the annotations to Crowsetta segments, otherwise, keep them as is, by default True.</p> <code>True</code> <code>ignore_errors</code> <code>bool</code> <p>If True, ignore errors during conversion and continue with the next annotation, otherwise this function will raise an error if an annotation cannot be converted, by default False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>to_crowsetta_segment</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence</code> <p>A Crowsetta sequence representing the converted soundevent annotations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an annotation cannot be converted and <code>ignore_errors</code> is False.</p>"},{"location":"reference/io/#soundevent.io.crowsetta.sequence_to_annotations","title":"<code>sequence_to_annotations(sequence, recording, adjust_time_expansion=True, created_by=None, **kwargs)</code>","text":"<p>Convert a Crowsetta sequence to a list of soundevent annotations.</p> <p>This function transforms a Crowsetta sequence (<code>crowsetta.Sequence</code>) into a list of <code>SoundEventAnnotation</code> objects. Each segment in the Crowsetta sequence is individually converted into a soundevent annotation using the <code>to_time_interval_annotation</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Sequence</code> <p>The Crowsetta sequence to convert.</p> required <code>recording</code> <code>Recording</code> <p>The original recording associated from which the Crowsetta sequence was annotated.</p> required <code>adjust_time_expansion</code> <code>bool</code> <p>If True, adjust the onset and offset times based on the recording's time expansion factor, by default True.</p> <code>True</code> <code>created_by</code> <code>User</code> <p>User information representing the creator of the converted soundevent annotations, by default None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>to_time_interval_annotation</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[SoundEventAnnotation]</code> <p>A list of soundevent annotations representing the converted Crowsetta sequence.</p>"},{"location":"reference/operations/","title":"Operations Module","text":""},{"location":"reference/operations/#soundevent.operations","title":"<code>soundevent.operations</code>","text":"<p>Operations Module.</p> <p>This module provides generic operations for manipulating sound event data.</p> <p>Functions:</p> Name Description <code>segment_clip</code> <p>Segments a clip into smaller clips of a specified duration.</p>"},{"location":"reference/operations/#soundevent.operations-attributes","title":"Attributes","text":""},{"location":"reference/operations/#soundevent.operations-functions","title":"Functions","text":""},{"location":"reference/operations/#soundevent.operations.segment_clip","title":"<code>segment_clip(clip, duration, hop=None, include_incomplete=False)</code>","text":"<p>Segments a clip into smaller clips of a specified duration.</p> <p>This function iterates yields segments of a given duration, with a specified hop size between segments. It can optionally include the last segment even if it is shorter than the specified duration.</p> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>Clip</code> <p>The input audio clip to be segmented.</p> required <code>duration</code> <code>float</code> <p>The duration of each segment in seconds.</p> required <code>hop</code> <code>Optional[float]</code> <p>The hop size between segments in seconds. If None (default), the hop size is set equal to the duration, resulting in non-overlapping segments.</p> <code>None</code> <code>include_incomplete</code> <code>bool</code> <p>Whether to include the last segment if it is shorter than the specified duration. Defaults to False.</p> <code>False</code> <p>Yields:</p> Type Description <code>Clip</code> <p>A segmented clip with a unique UUID, start time, end time, and the same recording as the input clip.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the duration or hop size is negative or zero.</p> Notes <p>If <code>include_incomplete</code> is True, the last segment might be shorter than the specified duration, as its end time is clamped to the end time of the input clip.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from soundevent import data\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; recording = data.Recording(\n...     path=Path(\"example.wav\"),\n...     samplerate=44100,\n...     channels=1,\n...     duration=60,\n... )\n&gt;&gt;&gt; clip = data.Clip(\n...     start_time=0.0,\n...     end_time=10.0,\n...     recording=recording,\n... )\n&gt;&gt;&gt; segments = segment_clip(clip, duration=2.0, hop=1.0)\n&gt;&gt;&gt; for segment in segments:\n...     print(segment.start_time, segment.end_time)\n0.0 2.0\n1.0 3.0\n2.0 4.0\n3.0 5.0\n4.0 6.0\n5.0 7.0\n6.0 8.0\n7.0 9.0\n8.0 10.0\n</code></pre>"},{"location":"reference/plot/","title":"Plotting Module","text":""},{"location":"reference/plot/#soundevent.plot","title":"<code>soundevent.plot</code>","text":"<p>Plotting utilities.</p> <p>Modules:</p> Name Description <code>annotation</code> <p>Functions for plotting annotations.</p> <code>common</code> <p>Common utilities for plotting.</p> <code>geometries</code> <p>Functions for plotting sound event geometries.</p> <code>prediction</code> <code>tags</code> <p>Functions for plotting tags.</p> <p>Classes:</p> Name Description <code>TagColorMapper</code> <p>Maps tags to colors.</p> <p>Functions:</p> Name Description <code>add_tags_legend</code> <p>Add a legend for tags.</p> <code>create_axes</code> <p>Create a new figure and axes.</p> <code>plot_annotation</code> <p>Plot a sound event annotation on a spectrogram.</p> <code>plot_annotations</code> <p>Plot a collection of sound event annotations on a spectrogram.</p> <code>plot_geometry</code> <p>Plot a geometry in the given ax.</p> <code>plot_prediction</code> <p>Plot a sound event prediction on a spectrogram.</p> <code>plot_predictions</code> <p>Plot a collection of sound event predictions on a spectrogram.</p> <code>plot_tag</code> <p>Plot a tag as a dot on a spectrogram.</p>"},{"location":"reference/plot/#soundevent.plot-classes","title":"Classes","text":""},{"location":"reference/plot/#soundevent.plot.TagColorMapper","title":"<code>TagColorMapper(cmap='tab20', num_colors=20)</code>","text":"<p>Maps tags to colors.</p> <p>Methods:</p> Name Description <code>get_color</code> <p>Get color for tag.</p>"},{"location":"reference/plot/#soundevent.plot.TagColorMapper-functions","title":"Functions","text":""},{"location":"reference/plot/#soundevent.plot.TagColorMapper.get_color","title":"<code>get_color(tag)</code>","text":"<p>Get color for tag.</p>"},{"location":"reference/plot/#soundevent.plot-functions","title":"Functions","text":""},{"location":"reference/plot/#soundevent.plot.add_tags_legend","title":"<code>add_tags_legend(ax, color_mapper)</code>","text":"<p>Add a legend for tags.</p>"},{"location":"reference/plot/#soundevent.plot.create_axes","title":"<code>create_axes(figsize=None)</code>","text":"<p>Create a new figure and axes.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>Optional[Tuple[float, float]]</code> <p>The size of the figure. If None, use the default size.</p> <code>None</code> <p>Returns:</p> Type Description <code>ax</code> <p>The axes.</p>"},{"location":"reference/plot/#soundevent.plot.plot_annotation","title":"<code>plot_annotation(annotation, ax=None, position='top-right', color_mapper=None, time_offset=0.001, freq_offset=1000, color=None, **kwargs)</code>","text":"<p>Plot a sound event annotation on a spectrogram.</p> <p>This function plots the geometry of the sound event and its associated tags.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>SoundEventAnnotation</code> <p>The sound event annotation to plot.</p> required <code>ax</code> <code>Optional[Axes]</code> <p>The matplotlib axes to plot on. If None, a new one is created.</p> <code>None</code> <code>position</code> <code>Positions</code> <p>The position of the tags relative to the geometry.</p> <code>'top-right'</code> <code>color_mapper</code> <code>Optional[TagColorMapper]</code> <p>A <code>TagColorMapper</code> instance to map tags to colors. If None, a new one is created.</p> <code>None</code> <code>time_offset</code> <code>float</code> <p>The time offset for positioning the tags.</p> <code>0.001</code> <code>freq_offset</code> <code>float</code> <p>The frequency offset for positioning the tags.</p> <code>1000</code> <code>color</code> <code>Optional[str]</code> <p>The color of the geometry. If None, the color is determined by the color mapper.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>create_axes</code> and <code>plot_geometry</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The matplotlib axes with the annotation plotted.</p>"},{"location":"reference/plot/#soundevent.plot.plot_annotations","title":"<code>plot_annotations(annotations, ax=None, position='top-right', color_mapper=None, time_offset=0.001, freq_offset=1000, legend=True, color=None, **kwargs)</code>","text":"<p>Plot a collection of sound event annotations on a spectrogram.</p> <p>This function iterates through a collection of sound event annotations and plots each one on the provided matplotlib axes.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>Iterable[SoundEventAnnotation]</code> <p>An iterable of <code>SoundEventAnnotation</code> objects to plot.</p> required <code>ax</code> <code>Optional[Axes]</code> <p>The matplotlib axes to plot on. If None, a new one is created.</p> <code>None</code> <code>position</code> <code>Positions</code> <p>The position of the tags relative to the geometry.</p> <code>'top-right'</code> <code>color_mapper</code> <code>Optional[TagColorMapper]</code> <p>A <code>TagColorMapper</code> instance to map tags to colors. If None, a new one is created.</p> <code>None</code> <code>time_offset</code> <code>float</code> <p>The time offset for positioning the tags.</p> <code>0.001</code> <code>freq_offset</code> <code>float</code> <p>The frequency offset for positioning the tags.</p> <code>1000</code> <code>legend</code> <code>bool</code> <p>Whether to add a legend for the tags.</p> <code>True</code> <code>color</code> <code>Optional[str]</code> <p>The color of the geometries. If None, the color is determined by the color mapper.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>plot_annotation</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The matplotlib axes with the annotations plotted.</p>"},{"location":"reference/plot/#soundevent.plot.plot_geometry","title":"<code>plot_geometry(geometry, ax=None, figsize=None, **kwargs)</code>","text":"<p>Plot a geometry in the given ax.</p>"},{"location":"reference/plot/#soundevent.plot.plot_prediction","title":"<code>plot_prediction(prediction, ax=None, position='top-right', color_mapper=None, time_offset=0.001, freq_offset=1000, max_alpha=0.5, color=None, **kwargs)</code>","text":"<p>Plot a sound event prediction on a spectrogram.</p> <p>This function plots the geometry of the sound event and its associated tags. The transparency of the geometry is determined by the prediction score and the <code>max_alpha</code> parameter. The transparency of the tags is directly controlled by the prediction score of each tag.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>SoundEventPrediction</code> <p>The sound event prediction to plot.</p> required <code>ax</code> <code>Optional[Axes]</code> <p>The matplotlib axes to plot on. If None, a new one is created.</p> <code>None</code> <code>position</code> <code>Positions</code> <p>The position of the tags relative to the geometry.</p> <code>'top-right'</code> <code>color_mapper</code> <code>Optional[TagColorMapper]</code> <p>A <code>TagColorMapper</code> instance to map tags to colors. If None, a new one is created.</p> <code>None</code> <code>time_offset</code> <code>float</code> <p>The time offset for positioning the tags.</p> <code>0.001</code> <code>freq_offset</code> <code>float</code> <p>The frequency offset for positioning the tags.</p> <code>1000</code> <code>max_alpha</code> <code>float</code> <p>The maximum transparency of the plotted geometry, scaled by the prediction score.</p> <code>0.5</code> <code>color</code> <code>Optional[str]</code> <p>The color of the geometry. If None, the color is determined by the color mapper.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>create_axes</code> and <code>plot_geometry</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The matplotlib axes with the prediction plotted.</p>"},{"location":"reference/plot/#soundevent.plot.plot_predictions","title":"<code>plot_predictions(predictions, ax=None, position='top-right', color_mapper=None, time_offset=0.001, freq_offset=1000, legend=True, max_alpha=0.5, color=None, **kwargs)</code>","text":"<p>Plot a collection of sound event predictions on a spectrogram.</p> <p>This function iterates through a collection of sound event predictions and plots each one on the provided matplotlib axes.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Iterable[SoundEventPrediction]</code> <p>An iterable of <code>SoundEventPrediction</code> objects to plot.</p> required <code>ax</code> <code>Optional[Axes]</code> <p>The matplotlib axes to plot on. If None, a new one is created.</p> <code>None</code> <code>position</code> <code>Positions</code> <p>The position of the tags relative to the geometry.</p> <code>'top-right'</code> <code>color_mapper</code> <code>Optional[TagColorMapper]</code> <p>A <code>TagColorMapper</code> instance to map tags to colors. If None, a new one is created.</p> <code>None</code> <code>time_offset</code> <code>float</code> <p>The time offset for positioning the tags.</p> <code>0.001</code> <code>freq_offset</code> <code>float</code> <p>The frequency offset for positioning the tags.</p> <code>1000</code> <code>legend</code> <code>bool</code> <p>Whether to add a legend for the tags.</p> <code>True</code> <code>max_alpha</code> <code>float</code> <p>The maximum transparency of the plotted geometries, scaled by the prediction score.</p> <code>0.5</code> <code>color</code> <code>Optional[str]</code> <p>The color of the geometries. If None, the color is determined by the color mapper.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>plot_prediction</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The matplotlib axes with the predictions plotted.</p>"},{"location":"reference/plot/#soundevent.plot.plot_tag","title":"<code>plot_tag(time, frequency, color, ax=None, size=10, alpha=1, **kwargs)</code>","text":"<p>Plot a tag as a dot on a spectrogram.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>float</code> <p>Time coordinate of the tag.</p> required <code>frequency</code> <code>float</code> <p>Frequency coordinate of the tag.</p> required <code>color</code> <code>str</code> <p>Color of the tag.</p> required <code>ax</code> <code>Optional[Axes]</code> <p>Axes to plot on. If None, a new one is created.</p> <code>None</code> <code>size</code> <code>int</code> <p>Size of the tag marker.</p> <code>10</code> <code>alpha</code> <code>float</code> <p>Transparency of the tag marker.</p> <code>1</code> <code>**kwargs</code> <p>Keyword arguments passed to <code>create_axes</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axes with the tag plotted.</p>"},{"location":"reference/terms/","title":"Terms Module","text":""},{"location":"reference/terms/#soundevent.terms","title":"<code>soundevent.terms</code>","text":"<p>Terms module.</p> <p>This module provides tools for creating and managing standardized terms.</p> <p>In the soundevent ecosystem, metadata is stored in <code>Tag</code> objects, which are pairs of a <code>Term</code> and a <code>value</code>. The <code>Term</code> provides a standardized definition that gives context and meaning to the <code>value</code>. For example, the <code>Term</code> <code>scientific_name</code> gives meaning to the <code>value</code> <code>\"Turdus migratorius\"</code>. Using standardized terms makes data understandable, shareable, and interoperable.</p> <p>This module provides three main features:</p> <ol> <li>Pre-defined Terms: A collection of standard terms for common concepts     in bioacoustics (e.g.,     <code>scientific_name</code>,     <code>f1_score</code>).</li> <li>Global API: A set of functions     (<code>find_term</code>,     <code>add_term</code>,     <code>add_term</code>, etc.     ) for     managing terms in a global registry.     <code>add_terms_from_file</code> is a     convenience function that loads terms from a file and then registers them.</li> <li><code>TermRegistry</code> Class: The underlying     class for creating and managing custom term collections.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from soundevent.data import Tag, Term\n&gt;&gt;&gt; from soundevent.terms import scientific_name, add_term, find_term\n&gt;&gt;&gt;\n&gt;&gt;&gt; # A list of tags that might be attached to a sound event\n&gt;&gt;&gt; tags = []\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use a pre-defined term to create a Tag\n&gt;&gt;&gt; species_tag = Tag(term=scientific_name, value=\"Turdus migratorius\")\n&gt;&gt;&gt; tags.append(species_tag)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"{tags[0].term.label}: {tags[0].value}\")\nScientific Taxon Name: Turdus migratorius\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create and use a custom term for a new Tag\n&gt;&gt;&gt; add_term(\n...     Term(\n...         name=\"custom:quality\",\n...         label=\"Quality\",\n...         definition=\"The quality of the recording, from 1 (poor) to 5 (excellent).\",\n...     )\n... )\n&gt;&gt;&gt; quality_term = find_term(q=\"quality\")[0]\n&gt;&gt;&gt; quality_tag = Tag(term=quality_term, value=\"4\")\n&gt;&gt;&gt; tags.append(quality_tag)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"{tags[1].term.label}: {tags[1].value}\")\nQuality: 4\n</code></pre> <p>Classes:</p> Name Description <code>TermRegistry</code> <p>A mutable mapping for managing, storing, and retrieving <code>Term</code> objects.</p> <code>TermSet</code> <p>A collection of terms and their optional mappings.</p> <p>Functions:</p> Name Description <code>add_term</code> <p>Register a term.</p> <code>find_term</code> <p>Find terms by substring match.</p> <code>get_term</code> <p>Retrieve a term by its key.</p> <code>get_term_by</code> <p>Retrieve one term by an exact match on a single attribute.</p> <code>has_term</code> <p>Check if a term exists in the registry.</p> <code>remove_term</code> <p>Remove a term from the registry by its key.</p> <code>add_terms_from_file</code> <p>Load terms from a file and add them to a registry.</p> <code>get_global_term_registry</code> <p>Return the current global term registry.</p> <code>set_global_term_registry</code> <p>Set a new global term registry.</p>"},{"location":"reference/terms/#soundevent.terms-classes","title":"Classes","text":""},{"location":"reference/terms/#soundevent.terms.TermRegistry","title":"<code>TermRegistry(terms=None)</code>","text":"<p>               Bases: <code>MutableMapping[str, Term]</code></p> <p>A mutable mapping for managing, storing, and retrieving <code>Term</code> objects.</p> <p>Provides dictionary-like access (getting, setting, deleting by key) along with specialized methods for finding terms based on their attributes. It serves as a central point to manage and access standardized <code>Term</code> objects within a project.</p> <p>Attributes:</p> Name Type Description <code>_terms</code> <code>Dict[str, Term]</code> <p>The internal dictionary holding the registered terms.</p> <p>Parameters:</p> Name Type Description Default <code>terms</code> <code>Optional[Dict[str, Term]]</code> <p>A dictionary of initial terms {key: term} to populate the registry. Defaults to an empty registry.</p> <code>None</code> <p>Methods:</p> Name Description <code>add_term</code> <p>Register a term, optionally defaulting the key to <code>term.name</code>.</p> <code>find</code> <p>Find terms by substring match; returns multiple terms.</p> <code>get</code> <p>Retrieve a term by key, returning a default if not found.</p> <code>get_by</code> <p>Retrieve one term by an exact match on a single attribute.</p> <code>remove</code> <p>Remove a term from the registry by its key.</p>"},{"location":"reference/terms/#soundevent.terms.TermRegistry-functions","title":"Functions","text":""},{"location":"reference/terms/#soundevent.terms.TermRegistry.add_term","title":"<code>add_term(term, key=None, force=False)</code>","text":"<p>Register a term, optionally defaulting the key to <code>term.name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>Term</code> <p>The <code>Term</code> object to add.</p> required <code>key</code> <code>Optional[str]</code> <p>The key to use for registration. If None, <code>term.name</code> is used. Defaults to None.</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, allows overriding an existing term with the same key. If False, raises <code>TermOverrideError</code> if the key already exists.</p> <code>False</code> <p>Raises:</p> Type Description <code>TermOverrideError</code> <p>If <code>force</code> is False and a term with the same key already exists.</p>"},{"location":"reference/terms/#soundevent.terms.TermRegistry.find","title":"<code>find(label=None, name=None, uri=None, definition=None, q=None, ignore_case=True)</code>","text":"<p>Find terms by substring match; returns multiple terms.</p> <p>If <code>q</code> is provided, it searches <code>label</code>, <code>name</code>, <code>uri</code>, and <code>definition</code> for a match (OR logic). If <code>q</code> is not provided, it searches using the specific fields, requiring all provided fields to match (AND logic). If no arguments are given, all terms are returned.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Optional[str]</code> <p>Substring to search for in labels.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Substring to search for in names.</p> <code>None</code> <code>uri</code> <code>Optional[str]</code> <p>Substring to search for in URIs.</p> <code>None</code> <code>definition</code> <code>Optional[str]</code> <p>Substring to search for in definitions.</p> <code>None</code> <code>q</code> <code>Optional[str]</code> <p>General query string (searches all fields, OR logic).</p> <code>None</code> <code>ignore_case</code> <code>bool</code> <p>Perform case-insensitive search. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Term]</code> <p>A list of matching <code>Term</code> objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>q</code> is used with other specific criteria.</p>"},{"location":"reference/terms/#soundevent.terms.TermRegistry.get","title":"<code>get(key, default=None)</code>","text":"<p>Retrieve a term by key, returning a default if not found.</p> <p>Mimics <code>dict.get()</code>. Returns <code>None</code> by default if the key is not found, or a specified default value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the term to retrieve.</p> required <code>default</code> <code>Any</code> <p>The value to return if the key is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Term]</code> <p>The <code>Term</code> object or the <code>default</code> value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>default</code> is provided and is not <code>None</code> or a <code>Term</code>.</p>"},{"location":"reference/terms/#soundevent.terms.TermRegistry.get_by","title":"<code>get_by(label=None, name=None, uri=None)</code>","text":"<p>Retrieve one term by an exact match on a single attribute.</p> <p>Requires exactly one search criterion and expects exactly one match.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Optional[str]</code> <p>The exact label to match.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>The exact name to match.</p> <code>None</code> <code>uri</code> <code>Optional[str]</code> <p>The exact URI to match.</p> <code>None</code> <p>Returns:</p> Type Description <code>Term</code> <p>The single <code>Term</code> object that matches.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If zero or more than one criterion is provided.</p> <code>TermNotFoundError</code> <p>If no term matches the criterion.</p> <code>MultipleTermsFoundError</code> <p>If more than one term matches the criterion.</p>"},{"location":"reference/terms/#soundevent.terms.TermRegistry.remove","title":"<code>remove(key)</code>","text":"<p>Remove a term from the registry by its key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the term to remove.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If no term is found with the given key.</p>"},{"location":"reference/terms/#soundevent.terms.TermSet","title":"<code>TermSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of terms and their optional mappings.</p> <p>Attributes:</p> Name Type Description <code>aliases</code> <code>Dict[str, str]</code> <p>A mapping from a custom key to a term name.</p> <code>terms</code> <code>List[Term]</code> <p>A list of term objects.</p>"},{"location":"reference/terms/#soundevent.terms.TermSet-attributes","title":"Attributes","text":""},{"location":"reference/terms/#soundevent.terms.TermSet.aliases","title":"<code>aliases = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A mapping from a custom key to a term name.</p>"},{"location":"reference/terms/#soundevent.terms.TermSet.terms","title":"<code>terms</code>  <code>instance-attribute</code>","text":"<p>A list of term objects.</p>"},{"location":"reference/terms/#soundevent.terms-functions","title":"Functions","text":""},{"location":"reference/terms/#soundevent.terms.add_term","title":"<code>add_term(term, key=None, term_registry=None, force=False)</code>","text":"<p>Register a term.</p> <p>By default, the key is derived from <code>term.name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>Term</code> <p>The <code>Term</code> object to add.</p> required <code>key</code> <code>Optional[str]</code> <p>The key to use for registration. If None, <code>term.name</code> is used.</p> <code>None</code> <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>If provided, the term is added to this registry instead of the global one.</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, overwrite any existing term with the same key.</p> <code>False</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>force</code> is False and the key already exists.</p>"},{"location":"reference/terms/#soundevent.terms.find_term","title":"<code>find_term(label=None, name=None, uri=None, definition=None, q=None, ignore_case=True, term_registry=None)</code>","text":"<p>Find terms by substring match.</p> <p>If <code>q</code> is provided, it searches <code>label</code>, <code>name</code>, <code>uri</code>, and <code>definition</code> for a match (OR logic). If <code>q</code> is not provided, it searches using the specific fields, requiring all provided fields to match (AND logic). If no arguments are given, all terms are returned.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Optional[str]</code> <p>Substring to search for in labels.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Substring to search for in names.</p> <code>None</code> <code>uri</code> <code>Optional[str]</code> <p>Substring to search for in URIs.</p> <code>None</code> <code>definition</code> <code>Optional[str]</code> <p>Substring to search for in definitions.</p> <code>None</code> <code>q</code> <code>Optional[str]</code> <p>General query string (searches all fields, OR logic).</p> <code>None</code> <code>ignore_case</code> <code>bool</code> <p>Perform case-insensitive search.</p> <code>True</code> <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>If provided, the search is performed on this registry instead of the global one.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Term]</code> <p>A list of matching <code>Term</code> objects.</p>"},{"location":"reference/terms/#soundevent.terms.get_term","title":"<code>get_term(key, default=None, term_registry=None)</code>","text":"<p>Retrieve a term by its key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the term to retrieve.</p> required <code>default</code> <code>Optional[Term]</code> <p>The value to return if the key is not found.</p> <code>None</code> <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>If provided, the term is retrieved from this registry instead of the global one.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Term]</code> <p>The <code>Term</code> object or the <code>default</code> value if not found.</p>"},{"location":"reference/terms/#soundevent.terms.get_term_by","title":"<code>get_term_by(label=None, name=None, uri=None, term_registry=None)</code>","text":"<p>Retrieve one term by an exact match on a single attribute.</p> <p>Requires exactly one search criterion and expects exactly one match.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Optional[str]</code> <p>The exact label to match.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>The exact name to match.</p> <code>None</code> <code>uri</code> <code>Optional[str]</code> <p>The exact URI to match.</p> <code>None</code> <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>If provided, the search is performed on this registry instead of the global one.</p> <code>None</code> <p>Returns:</p> Type Description <code>Term</code> <p>The single <code>Term</code> object that matches.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If zero or more than one criterion is provided.</p> <code>TermNotFoundError</code> <p>If no term matches the criterion.</p> <code>MultipleTermsFoundError</code> <p>If more than one term matches the criterion.</p>"},{"location":"reference/terms/#soundevent.terms.has_term","title":"<code>has_term(key, term_registry=None)</code>","text":"<p>Check if a term exists in the registry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the term to check.</p> required <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>If provided, the check is performed on this registry instead of the global one.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the term exists, False otherwise.</p>"},{"location":"reference/terms/#soundevent.terms.remove_term","title":"<code>remove_term(key, term_registry=None)</code>","text":"<p>Remove a term from the registry by its key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the term to remove.</p> required <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>If provided, the term is removed from this registry instead of the global one.</p> <code>None</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no term is found with the given key.</p>"},{"location":"reference/terms/#soundevent.terms.add_terms_from_file","title":"<code>add_terms_from_file(path, term_registry=None, format=None, override_existing=False, ignore_overrides=True, ignore_missing_key=True)</code>","text":"<p>Load terms from a file and add them to a registry.</p> <p>This function provides options to handle cases where a term being loaded already exists in the registry, or when a mapping refers to a non-existent term.</p> <p>The format can be specified explicitly. If not, it will be inferred from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the file.</p> required <code>term_registry</code> <code>Optional[TermRegistry]</code> <p>The registry to add the terms to. If None, the global registry is used.</p> <code>None</code> <code>format</code> <code>Optional[TermFormat]</code> <p>The format of the file. If None, it will be inferred from the file extension.</p> <code>None</code> <code>override_existing</code> <code>bool</code> <p>If True, existing terms with the same name will be overwritten. Defaults to False.</p> <code>False</code> <code>ignore_overrides</code> <code>bool</code> <p>If True, and <code>override_existing</code> is False, any term that already exists in the registry will be skipped without raising an error. If False, a <code>TermOverrideError</code> will be raised. Defaults to True.</p> <code>True</code> <code>ignore_missing_key</code> <code>bool</code> <p>If True, any alias in the mapping that refers to a non-existent term will be skipped. If False, a <code>TermNotFoundError</code> will be raised. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>TermOverrideError</code> <p>If <code>override_existing</code> is False, <code>ignore_overrides</code> is False, and a term or alias being loaded already exists in the registry.</p> <code>TermNotFoundError</code> <p>If <code>ignore_missing_key</code> is False and an alias in the mapping refers to a term name that is not found in the registry.</p> Notes <p>See [soundevent.terms.io][] for detailed information on the supported JSON and CSV file structures.</p>"},{"location":"reference/terms/#soundevent.terms.get_global_term_registry","title":"<code>get_global_term_registry()</code>","text":"<p>Return the current global term registry.</p> <p>Returns:</p> Type Description <code>TermRegistry</code> <p>The active global <code>TermRegistry</code> instance.</p>"},{"location":"reference/terms/#soundevent.terms.set_global_term_registry","title":"<code>set_global_term_registry(term_registry)</code>","text":"<p>Set a new global term registry.</p> <p>This function replaces the existing global registry with a new one. All subsequent calls to functions in this module will operate on the new registry.</p> <p>Parameters:</p> Name Type Description Default <code>term_registry</code> <code>TermRegistry</code> <p>The new <code>TermRegistry</code> instance to set as the global registry.</p> required"},{"location":"reference/terms/#term_library","title":"Term Library","text":""},{"location":"reference/terms/#soundevent.terms.library","title":"<code>soundevent.terms.library</code>","text":"<p>Modules:</p> Name Description <code>devices</code> <code>geography</code> <code>metrics</code> <code>roi</code> <code>taxonomy</code> <p>Attributes:</p> Name Type Description <code>accuracy</code> <code>alternative</code> <code>average_precision</code> <code>balanced_accuracy</code> <code>bandwidth</code> <code>capture_device</code> <code>common_name</code> <code>country</code> <code>county</code> <code>duration</code> <code>elevation</code> <code>f1_score</code> <code>family</code> <code>genus</code> <code>high_freq</code> <code>jaccard_index</code> <code>location_id</code> <code>low_freq</code> <code>mean_average_precision</code> <code>num_segments</code> <code>order</code> <code>scientific_name</code> <code>state_province</code> <code>taxonomic_class</code> <code>top_3_accuracy</code> <code>true_class_probability</code>"},{"location":"reference/terms/#soundevent.terms.library-attributes","title":"Attributes","text":""},{"location":"reference/terms/#soundevent.terms.library.accuracy","title":"<code>accuracy = Term(uri='http://purl.obolibrary.org/obo/STATO_0000415', name='stato:accuracy', label='Accuracy', definition='In the context of binary classification, accuracy is defined as the proportion of true results (both true positives and true negatives) to the total number of cases examined (the sum of true positive, true negative, false positive and false negative). It can be understood as a measure of the proximity of measurement results to the true value. Accuracy is a metric used in the context of classification tasks to evaluate the proportion of correctly predicted instances among the total instances. Key Points: Use Case: Classification performance evaluation. Metric: Measures the proportion of correct predictions. Interpretation: Higher values indicate better classification performance.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.alternative","title":"<code>alternative = Term(uri='http://purl.org/dc/terms/alternative', name='dcterms:alternative', label='Alternative', definition='An alternative name for the resource.', scope_note='Can be used to reference an identifier from an external source for a resource within a new collection, acting as a cross-reference.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.average_precision","title":"<code>average_precision = Term(name='soundevent_metrics:averagePrecision', label='Average Precision', definition='The average precision (AP) is a metric that quantifies the quality of a binary detection task. The AP is defined as the area under the precision-recall curve.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.balanced_accuracy","title":"<code>balanced_accuracy = Term(name='soundevent_metrics:balancedAccuracy', label='Balanced Accuracy', definition='The macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.bandwidth","title":"<code>bandwidth = Term(name='soundevent:bandwidth', label='Bandwidth', definition='The difference between the highest and lowest frequency of the sound event.', scope_note='Numeric value in hertz (Hz)')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.capture_device","title":"<code>capture_device = Term(uri='http://rs.tdwg.org/ac/terms/captureDevice', name='ac:captureDevice', label='Capture Device', definition='Free form text describing the device or devices used to create the resource.', scope_note='It is best practice to record the device; this may include a combination such as camera plus lens, or camera plus microscope. Examples: \"Canon Supershot 2000\", \"Makroscan Scanner 2000\", \"Zeiss Axioscope with Camera IIIu\", \"SEM (Scanning Electron Microscope)\".')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.common_name","title":"<code>common_name = Term(uri='http://rs.tdwg.org/dwc/terms/vernacularName', name='dwc:vernacularName', label='Common Name', definition='A common or vernacular name.', scope_note='Common (= vernacular) names of the subject in one or several languages. The ISO 639-1 language code SHOULD be given in parentheses after the name if not all names are given by values of the Metadata Language term.', description=\"The ISO language code after the name should be formatted as in the following example: 'abete bianco (it); Tanne (de); White Fir (en)'. If names are known to be male- or female-specific, this may be specified as in: 'ewe (en-female); ram (en-male);'.\")</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.country","title":"<code>country = Term(uri='http://rs.tdwg.org/dwc/terms/country', name='dwc:country', label='Country', definition='The name of the country or major administrative unit in which the Location occurs.', scope_note='Recommended best practice is to use a controlled vocabulary such as the Getty Thesaurus of Geographic Names.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.county","title":"<code>county = Term(uri='http://rs.tdwg.org/dwc/terms/county', name='dwc:county', label='Second Order Division', definition='The full, unabbreviated name of the next smaller administrative region than stateProvince (county, shire, department, etc.) in which the dcterms:Location occurs.', scope_note='Recommended best practice is to use a controlled vocabulary such as the Getty Thesaurus of Geographic Names. Recommended best practice is to leave this field blank if the dcterms:Location spans multiple entities at this administrative level or if the dcterms:Location might be in one or another of multiple possible entities at this level. Multiplicity and uncertainty of the geographic entity can be captured either in the term dwc:higherGeography or in the term dwc:locality, or both.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.duration","title":"<code>duration = Term(uri='http://rs.tdwg.org/ac/terms/mediaDuration', label='Media Duration', name='ac:mediaDuration', definition='The playback duration of an audio or video file in seconds.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.elevation","title":"<code>elevation = Term(uri='http://rs.tdwg.org/dwc/terms/verbatimElevation', name='dwc:verbatimElevation', label='Verbatim Elevation', definition='The original description of the elevation (altitude, usually above sea level) of the Location.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.f1_score","title":"<code>f1_score = Term(name='soundevent_metrics:f1_score', label='F1 Score', definition=\"The F1 score is the harmonic mean of precision and recall. It is a measure of a test's accuracy that considers both the precision and recall of the test to compute the score. The F1 score is the weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\")</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.family","title":"<code>family = Term(uri='http://rs.tdwg.org/dwc/terms/family', name='dwc:family', label='Family', definition='The full scientific name of the family in which the dwc:Taxon is classified.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.genus","title":"<code>genus = Term(uri='http://rs.tdwg.org/dwc/terms/genus', name='dwc:genus', label='Genus', definition='The full scientific name of the genus in which the dwc:Taxon is classified.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.high_freq","title":"<code>high_freq = Term(uri='http://rs.tdwg.org/ac/terms/freqHigh', label='Upper frequency bound', name='ac:freqHigh', definition='The highest frequency of the phenomena reflected in the multimedia item or Region of Interest.', scope_note='Numeric value in hertz (Hz)', description='This term refers to the sound events depicted and not to the constraints of the recording medium, so are in principle independent from sampleRate. If dwc:scientificName is specified and if applied to the entire multimedia item, these frequency bounds refer to the sounds of the species given in the dwc:scientificName throughout the whole recording. Although many users will specify both freqLow and freqHigh, it is permitted to specify just one or the other, for example if only one of the bounds is discernible.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.jaccard_index","title":"<code>jaccard_index = Term(name='soundevent_metrics:jaccard', label='Jaccard Index', definition='The Jaccard index, also known as the Jaccard similarity coefficient, is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard index is defined as the size of the intersection divided by the size of the union of two sample sets.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.location_id","title":"<code>location_id = Term(uri='http://rs.tdwg.org/dwc/terms/locationID', name='dwc:locationID', label='Location ID', definition='An identifier for the set of location information (data associated with dcterms:Location). May be a global unique identifier or an identifier specific to the data set.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.low_freq","title":"<code>low_freq = Term(uri='http://rs.tdwg.org/ac/terms/freqLow', label='Lower frequency bound', name='ac:freqLow', definition='The lowest frequency of the phenomena reflected in the multimedia item or Region of Interest.', scope_note='Numeric value in hertz (Hz)', description='This term refers to the sound events depicted and not to the constraints of the recording medium, so are in principle independent from sampleRate. If dwc:scientificName is specified and if applied to the entire multimedia item, these frequency bounds refer to the sounds of the species given in the dwc:scientificName throughout the whole recording. Although many users will specify both freqLow and freqHigh, it is permitted to specify just one or the other, for example if only one of the bounds is discernible.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.mean_average_precision","title":"<code>mean_average_precision = Term(name='soundevent_metrics:meanAveragePrecision', label='Mean Average Precision', definition='The mean of the average precision scores per class.', description='The average precision (AP) is a metric that quantifies the quality of a binary detection task. The AP is defined as the area under the precision-recall curve. The mean average precision (mAP) is the mean of the average precision scores per class.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.num_segments","title":"<code>num_segments = Term(name='soundevent:numSegments', label='Number of Segments', definition='Number of segments that compose the ROI of a sound event.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.order","title":"<code>order = Term(uri='http://rs.tdwg.org/dwc/terms/order', name='dwc:order', label='Order', definition='The full scientific name of the order in which the dwc:Taxon is classified.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.scientific_name","title":"<code>scientific_name = Term(uri='http://rs.tdwg.org/dwc/terms/scientificName', name='dwc:scientificName', label='Scientific Taxon Name', definition='The full scientific name, with authorship and date information if known. When forming part of an Identification, this should be the name in lowest level taxonomic rank that can be determined. This term should not contain identification qualifications, which should instead be supplied in the IdentificationQualifier term.', scope_note='Scientific names of taxa represented in the media resource (with date and name authorship information if available) of the lowest level taxonomic rank that can be applied.', description='The Scientific Taxon Name may possibly be of a higher rank, e.g., a genus or family name, if this is the most specific identification available. Where multiple taxa are the subject of the media item, multiple names may be given. If possible, add this information here even if the title or caption of the resource already contains scientific taxon names. Where the list of scientific taxon names is impractically large (e.g., media collections or identification tools), the number of taxa should be given in Taxon Count (see below). If possible, avoid repeating the Taxonomic Coverage here. Do not use abbreviated Genus names (\"P. vulgaris\"). It is recommended to provide author citation to scientific names, to avoid ambiguities in the presence of homonyms (the same name created by different authors for different taxa). Identifier qualifications should be supplied in the Identification Qualifier term rather than here (i. e. \"Abies cf. alba\" is deprecated, to be replaced with Scientific Taxon Name = \"Abies alba\" and Identification Qualifier = \"cf. species\")')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.soundevent_term_set","title":"<code>soundevent_term_set = TermSet(terms=[accuracy, alternative, average_precision, balanced_accuracy, bandwidth, capture_device, common_name, country, county, duration, elevation, f1_score, family, genus, high_freq, jaccard_index, location_id, low_freq, mean_average_precision, num_segments, order, scientific_name, state_province, taxonomic_class, top_3_accuracy, true_class_probability], aliases={'species': scientific_name.name, 'genus': genus.name, 'family': family.name, 'order': order.name, 'common_name': common_name.name, 'class': taxonomic_class.name, 'duration': duration.name, 'low_freq': low_freq.name, 'high_freq': high_freq.name, 'location_id': location_id.name, 'site_id': location_id.name, 'country': country.name, 'state': state_province.name})</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.state_province","title":"<code>state_province = Term(uri='http://rs.tdwg.org/dwc/terms/stateProvince', name='dwc:stateProvince', label='First Order Administrative Division', definition='The name of the next smaller administrative region than country (state, province, canton, department, region, etc.) in which the dcterms:Location occurs.', scope_note='Recommended best practice is to use a controlled vocabulary such as the Getty Thesaurus of Geographic Names. Recommended best practice is to leave this field blank if the dcterms:Location spans multiple entities at this administrative level or if the dcterms:Location might be in one or another of multiple possible entities at this level. Multiplicity and uncertainty of the geographic entity can be captured either in the term dwc:higherGeography or in the term dwc:locality, or both.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.taxonomic_class","title":"<code>taxonomic_class = Term(uri='http://rs.tdwg.org/dwc/terms/class', name='dwc:class', label='Class', definition='The full scientific name of the class in which the dwc:Taxon is classified.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.top_3_accuracy","title":"<code>top_3_accuracy = Term(name='soundevent_metrics:top3Accuracy', label='Top 3 Accuracy', definition='The proportion of samples where the true class is in the top 3 predicted classes.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library.true_class_probability","title":"<code>true_class_probability = Term(name='soundevent_metrics:trueClassProbability', label='True Class Probability', definition='The model probability assigned to the true class.')</code>  <code>module-attribute</code>","text":""},{"location":"reference/terms/#soundevent.terms.library-classes","title":"Classes","text":""},{"location":"reference/terms/#soundevent.terms.library-functions","title":"Functions","text":""},{"location":"reference/transforms/","title":"Transforms Module","text":""},{"location":"reference/transforms/#soundevent.transforms","title":"<code>soundevent.transforms</code>","text":"<p>Data transformations for soundevent objects.</p> <p>This module provides a framework for applying transformations to soundevent data objects. The core of the framework is the <code>TransformBase</code> class, which defines a visitor pattern for traversing the complex hierarchy of soundevent data models.</p> <p>The module also includes concrete transform classes for common data manipulation tasks, such as modifying recording paths (<code>PathTransform</code>) or transforming tags (<code>TagsTransform</code>).</p> <p>These tools are designed to help users clean, modify, and standardize their bioacoustic datasets in a structured and reliable way.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base classes for data transformations.</p> <code>path</code> <p>Transformations for recording paths.</p> <code>tags</code> <p>Transformations for tags.</p> <p>Classes:</p> Name Description <code>PathTransform</code> <p>A transform for modifying the path of recordings.</p> <code>TagsTransform</code> <p>A transform for modifying sequences of tags.</p> <code>TransformBase</code> <p>Base class for creating data transformations.</p>"},{"location":"reference/transforms/#soundevent.transforms-classes","title":"Classes","text":""},{"location":"reference/transforms/#soundevent.transforms.PathTransform","title":"<code>PathTransform(transform)</code>","text":"<p>               Bases: <code>TransformBase</code></p> <p>A transform for modifying the path of recordings.</p> <p>This class provides a convenient way to apply a path transformation to all <code>Recording</code> objects within a larger data structure (like a <code>Dataset</code> or <code>AnnotationProject</code>). It works by overriding the <code>transform_path</code> method of the <code>TransformBase</code>.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Path], Path]</code> <p>A function that takes a <code>pathlib.Path</code> object and returns a transformed <code>pathlib.Path</code> object.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from soundevent import data\n&gt;&gt;&gt; from soundevent.transforms import PathTransform\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a sample dataset to work with\n&gt;&gt;&gt; recording = data.Recording(\n...     path=Path(\"../relative/path/rec.wav\"),\n...     duration=1,\n...     channels=1,\n...     samplerate=16000,\n... )\n&gt;&gt;&gt; dataset = data.Dataset(name=\"test-dataset\", recordings=[recording])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define a function to make all paths absolute\n&gt;&gt;&gt; def make_absolute(path: Path) -&gt; Path:\n...     # This is a simplistic example, in reality you might need a base directory\n...     return path.resolve()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create and apply the transform\n&gt;&gt;&gt; path_transformer = PathTransform(transform=make_absolute)\n&gt;&gt;&gt; transformed_dataset = path_transformer.transform_dataset(dataset)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check that the path in the transformed dataset is absolute\n&gt;&gt;&gt; transformed_dataset.recordings[0].path.is_absolute()\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Path], Path]</code> <p>A function that takes a <code>pathlib.Path</code> object and returns a transformed <code>pathlib.Path</code> object.</p> required <p>Methods:</p> Name Description <code>transform_path</code> <p>Apply the transformation to a path.</p> <p>Attributes:</p> Name Type Description <code>transform</code>"},{"location":"reference/transforms/#soundevent.transforms.PathTransform-attributes","title":"Attributes","text":""},{"location":"reference/transforms/#soundevent.transforms.PathTransform.transform","title":"<code>transform = transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.PathTransform-functions","title":"Functions","text":""},{"location":"reference/transforms/#soundevent.transforms.PathTransform.transform_path","title":"<code>transform_path(path)</code>","text":"<p>Apply the transformation to a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to transform.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The transformed path.</p>"},{"location":"reference/transforms/#soundevent.transforms.TagsTransform","title":"<code>TagsTransform(transform)</code>","text":"<p>               Bases: <code>TransformBase</code></p> <p>A transform for modifying sequences of tags.</p> <p>This class provides a way to apply a transformation to all <code>Tag</code> sequences within a soundevent data structure. It is useful for filtering, renaming, or otherwise modifying tags across an entire dataset.</p> <p>It can be initialized directly with a function that transforms a whole sequence of tags, or it can be constructed from a function that transforms a single tag using the <code>from_tag_transform</code> class method.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Sequence[Tag]], Sequence[Tag]]</code> <p>A function that takes a sequence of <code>Tag</code> objects and returns a transformed sequence of <code>Tag</code> objects.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from soundevent import data\n&gt;&gt;&gt; from soundevent.transforms import TagsTransform\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a sample recording with a misspelled species tag\n&gt;&gt;&gt; recording = data.Recording(\n...     path=Path(\"rec.wav\"),\n...     duration=1,\n...     channels=1,\n...     samplerate=16000,\n...     tags=[\n...         data.Tag(key=\"species\", value=\"Myotis mytis\"),\n...         data.Tag(key=\"quality\", value=\"good\"),\n...     ],\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a transform to correct the spelling of \"Myotis myotis\"\n&gt;&gt;&gt; def correct_species_name(tag: data.Tag) -&gt; data.Tag:\n...     if tag.key == \"species\" and tag.value == \"Myotis mytis\":\n...         return tag.model_copy(update={\"value\": \"Myotis myotis\"})\n...     return tag\n&gt;&gt;&gt; corrector = TagsTransform.from_tag_transform(\n...     transform=correct_species_name\n... )\n&gt;&gt;&gt; transformed_recording = corrector.transform_recording(recording)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify that the tag value has been corrected\n&gt;&gt;&gt; species_tag = next(\n...     t for t in transformed_recording.tags if t.key == \"species\"\n... )\n&gt;&gt;&gt; species_tag.value\n'Myotis myotis'\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Verify that other tags are untouched\n&gt;&gt;&gt; quality_tag = next(\n...     t for t in transformed_recording.tags if t.key == \"quality\"\n... )\n&gt;&gt;&gt; quality_tag.value\n'good'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Sequence[Tag]], Sequence[Tag]]</code> <p>A function that takes a sequence of <code>Tag</code> objects and returns a transformed sequence of <code>Tag</code> objects.</p> required <p>Methods:</p> Name Description <code>from_tag_transform</code> <p>Create a TagsTransform from a function that transforms a single tag.</p> <code>transform_tags</code> <p>Apply the transformation to a sequence of tags.</p> <p>Attributes:</p> Name Type Description <code>transform</code>"},{"location":"reference/transforms/#soundevent.transforms.TagsTransform-attributes","title":"Attributes","text":""},{"location":"reference/transforms/#soundevent.transforms.TagsTransform.transform","title":"<code>transform = transform</code>  <code>instance-attribute</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TagsTransform-functions","title":"Functions","text":""},{"location":"reference/transforms/#soundevent.transforms.TagsTransform.from_tag_transform","title":"<code>from_tag_transform(transform)</code>  <code>classmethod</code>","text":"<p>Create a TagsTransform from a function that transforms a single tag.</p> <p>This factory method is a convenient way to create a <code>TagsTransform</code> when your logic applies to each tag individually.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Tag], Optional[Tag]]</code> <p>A function that takes a single <code>Tag</code> object and returns either a transformed <code>Tag</code> object or <code>None</code>. If <code>None</code> is returned, the tag is removed from the sequence.</p> required <p>Returns:</p> Type Description <code>TagsTransform</code> <p>A new <code>TagsTransform</code> instance.</p>"},{"location":"reference/transforms/#soundevent.transforms.TagsTransform.transform_tags","title":"<code>transform_tags(tags)</code>","text":"<p>Apply the transformation to a sequence of tags.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Sequence[Tag]</code> <p>The sequence of tags to transform.</p> required <p>Returns:</p> Type Description <code>Sequence[Tag]</code> <p>The transformed sequence of tags.</p>"},{"location":"reference/transforms/#soundevent.transforms.TransformBase","title":"<code>TransformBase</code>","text":"<p>Base class for creating data transformations.</p> <p>This class implements the visitor pattern to traverse the complex hierarchy of soundevent data objects. It provides <code>transform_*</code> methods for each type of data object in the soundevent ecosystem.</p> <p>The default implementation of each <code>transform_*</code> method returns the object unchanged or, for container-like objects, recursively calls the appropriate transform methods on their children and returns a new container with the transformed children.</p> <p>To create a custom transformation, inherit from this class and override the <code>transform_*</code> method for the specific object or attribute you want to modify.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from soundevent import data\n&gt;&gt;&gt; from soundevent.transforms.base import TransformBase\n&gt;&gt;&gt;\n&gt;&gt;&gt; class UserAnonymizer(TransformBase):\n...     def transform_user(self, user: data.User) -&gt; data.User:\n...         return user.model_copy(update={\"name\": \"anonymous\"})\n</code></pre> <p>Methods:</p> Name Description <code>transform_annotation_project</code> <code>transform_annotation_set</code> <code>transform_annotation_task</code> <code>transform_clip</code> <code>transform_clip_annotation</code> <code>transform_clip_prediction</code> <code>transform_dataset</code> <code>transform_evaluation_set</code> <code>transform_features</code> <code>transform_geometry</code> <code>transform_model_run</code> <code>transform_notes</code> <code>transform_path</code> <code>transform_predicted_tags</code> <code>transform_prediction_set</code> <code>transform_recording</code> <code>transform_recording_set</code> <code>transform_sequence</code> <code>transform_sequence_annotation</code> <code>transform_sequence_prediction</code> <code>transform_sound_event</code> <code>transform_sound_event_annotation</code> <code>transform_sound_event_prediction</code> <code>transform_status_badge</code> <code>transform_tags</code> <code>transform_user</code>"},{"location":"reference/transforms/#soundevent.transforms.TransformBase-functions","title":"Functions","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_annotation_project","title":"<code>transform_annotation_project(annotation_project)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_annotation_set","title":"<code>transform_annotation_set(annotation_set)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_annotation_task","title":"<code>transform_annotation_task(annotation_task)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_clip","title":"<code>transform_clip(clip)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_clip_annotation","title":"<code>transform_clip_annotation(clip_annotation)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_clip_prediction","title":"<code>transform_clip_prediction(clip_prediction)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_dataset","title":"<code>transform_dataset(dataset)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_evaluation_set","title":"<code>transform_evaluation_set(evaluation_set)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_features","title":"<code>transform_features(features)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_geometry","title":"<code>transform_geometry(geometry)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_model_run","title":"<code>transform_model_run(model_run)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_notes","title":"<code>transform_notes(notes)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_path","title":"<code>transform_path(path)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_predicted_tags","title":"<code>transform_predicted_tags(predicted_tags)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_prediction_set","title":"<code>transform_prediction_set(prediction_set)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_recording","title":"<code>transform_recording(recording)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_recording_set","title":"<code>transform_recording_set(recording_set)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_sequence","title":"<code>transform_sequence(sequence)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_sequence_annotation","title":"<code>transform_sequence_annotation(sequence_annotation)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_sequence_prediction","title":"<code>transform_sequence_prediction(sequence_prediction)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_sound_event","title":"<code>transform_sound_event(sound_event)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_sound_event_annotation","title":"<code>transform_sound_event_annotation(sound_event_annotation)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_sound_event_prediction","title":"<code>transform_sound_event_prediction(sound_event_prediction)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_status_badge","title":"<code>transform_status_badge(status_badge)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_tags","title":"<code>transform_tags(tags)</code>","text":""},{"location":"reference/transforms/#soundevent.transforms.TransformBase.transform_user","title":"<code>transform_user(user)</code>","text":""}]}