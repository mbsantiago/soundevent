{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saving and Loading data.\n\nIn `soundevent`, we use the **Acoustic Objects Exchange Format** (**AOEF**) for\nstoring and exchanging audio objects. **AOEF** is a JSON-based format\nspecifically designed to standardize the representation of computational\nbioacoustic data, enabling effective sharing and collaboration among\nresearchers.\n\n!!! note \"Why JSON?\"\n\n    [JSON](https://www.json.org/) or JavaScript Object Notation, is a\n    lightweight data-interchange format that is widely supported across various\n    platforms and programming languages. It provides human-readable syntax and\n    is commonly used in web applications, making it an ideal choice for data\n    exchange.\n\nWe use AOEF to share common collections of audio objects, such as datasets,\nannotation projects, evaluation sets, model runs and performance evaluations.\n\nTo demonstrate how to save and load data in **AOEF** format, we provide\nexamples below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\nSuppose we have an example dataset stored in the **AOEF** format. The dataset\nis stored as a text file following the [JSON](https://www.json.org/)\nstructure. To view the contents of the file, you can use the following code.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\nfrom pathlib import Path\n\ndataset_path = Path(\"example_dataset.json\")\nwith open(dataset_path) as file:\n    dataset_contents = json.load(file)\n\nprint(json.dumps(dataset_contents, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Datasets\nBy using the loading functions provided by the `soundevent` package, you can\ndirectly load the data into Python and obtain a\n[`Dataset`](../../data_schemas/audio_content.md#datasets) object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from soundevent import io\n\ndataset = io.load(dataset_path)\nprint(repr(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [`load`][soundevent.io.load] function allows you to\naccess and analyze the dataset, which contains recordings and related\nobjects, all structured in a standardized and manageable way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recording = dataset.recordings[0]\nprint(f\"First recording: {recording!r}\")\nprint(f\"Recording tags: {recording.tags}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving Datasets\nIf you have your own dataset, you can save it to a file using the\n[`save`][soundevent.io.save] function. This function stores\nthe dataset in **AOEF** format, ensuring compatibility and easy sharing with\nother researchers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "io.save(dataset, dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Annotation Projects\n\nSimilar to loading datasets, you can also use the\n[`load`][soundevent.io.load] function\nto load annotations stored in the **AOEF** format.\n\nHere we have transformed 10 random annotated recordings from the\n[NIPS4BPlus](https://doi.org/10.7717%2Fpeerj-cs.223) dataset into the\n**AOEF** format and stored it in the `nips4b_plus_aoef.json` file. You can\nuse the provided code to view the annotations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "annotation_path = Path(\"nips4b_plus_sample.json\")\nwith open(annotation_path) as file:\n    annotation_contents = json.load(file)\n\nprint(json.dumps(annotation_contents, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Annotation Projects\nThe [`load`][soundevent.io.load]\nfunction can be used to load the annotations into Python and obtain an\n[`AnnotationProject`](../../data_schemas/annotation.md#annotation_project)\nobject directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nips4b_sample = io.load(annotation_path, type=\"annotation_set\")\nprint(repr(nips4b_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object allows you to access and analyze the annotations, along with\ntheir associated objects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for clip_annotation in nips4b_sample.clip_annotations:\n    clip = clip_annotation.clip\n    recording = clip.recording\n    print(\n        f\"* Recording {recording.path} [from \"\n        f\"{clip.start_time:.3f}s to {clip.end_time:.3f}s]\"\n    )\n    print(\n        f\"\\t{len(clip_annotation.sound_events)} sound event annotations found\"\n    )\n    for annotation in clip_annotation.sound_events:\n        sound_event = annotation.sound_event\n        start_time, end_time = sound_event.geometry.coordinates\n        print(f\"\\t+ Sound event from {start_time:.3f}s to {end_time:.3f}s\")\n        for tag in annotation.tags:\n            print(f\"\\t\\t- {tag}\")\n    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving Annotation Projects\nSaving the annotation project is just as straightforward using the\n[`save`][soundevent.io.save] function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "io.save(nips4b_sample, \"nips4b_plus_sample.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Runs\nFinally, the outputs of a model run can also be stored in the **AOEF**\nformat. You can save and load model runs using the\n[`save`][soundevent.io.save] and\n[`load`][soundevent.io.load] functions, respectively. The\nloading function reads the **AOEF** file and returns a\n[`ModelRun`](../../data_schemas/prediction.md#model_runs) object that can be\nused for further analysis.\n\nBy utilizing the saving and loading functions provided by soundevent, you can\neasily manage and exchange acoustic data objects in AOEF format, promoting\ncollaboration and advancing your bioacoustic research endeavors.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}